{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import nest_asyncio\n",
    "from llama_index.readers import PDFReader\n",
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")\n",
    "\n",
    "from llama_index.extractors import (\n",
    "    KeywordExtractor,\n",
    "    EntityExtractor,\n",
    "    BaseExtractor,\n",
    ")\n",
    "\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.storage.index_store import SimpleIndexStore\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.ingestion import IngestionPipeline\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "\n",
    "INPUT_PATH = \"data/.papers\"\n",
    "PERSIST_PATH = \"data/.storage\"\n",
    "COLLECTION_NAME = \"quickstart\"\n",
    "\n",
    "QUANT_VERSION = \"mistral-7b-instruct-v0.2.Q3_K_S.gguf\"\n",
    "LANGUAGE_MODEL = f\"./models/{QUANT_VERSION}\"\n",
    "EMBEDDING_MODEL = \"BAAI/bge-small-en\"\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 2060, compute capability 7.5\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./models/mistral-7b-instruct-v0.2.Q3_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 11\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q3_K:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q3_K - Small\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 2.95 GiB (3.50 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: system memory used  =  335.19 MiB\n",
      "llm_load_tensors: VRAM used           = 2682.19 MiB\n",
      "llm_load_tensors: offloading 30 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 30/33 layers to GPU\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3000\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  375.00 MiB, K (f16):  187.50 MiB, V (f16):  187.50 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 220.55 MiB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 217.36 MiB\n",
      "llama_new_context_with_model: total VRAM used: 2899.55 MiB (model: 2682.19 MiB, context: 217.36 MiB)\n",
      "/media/starscream/wheeljack1/projects/casper/src/language/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    model_path=LANGUAGE_MODEL,\n",
    "    temperature=0.0,\n",
    "    max_new_tokens=512,\n",
    "    context_window=3000,\n",
    "    generate_kwargs={},\n",
    "    model_kwargs={\"n_gpu_layers\": 30},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=False,\n",
    ")\n",
    "embed_model = HuggingFaceEmbedding(model_name=EMBEDDING_MODEL)\n",
    "\n",
    "# initialize client, setting path to save data\n",
    "chroma_client = chromadb.PersistentClient(path=PERSIST_PATH)\n",
    "chroma_collection = chroma_client.get_or_create_collection(COLLECTION_NAME)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection, llm=None)\n",
    "\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 documents\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(INPUT_PATH)\n",
    "loader = PDFReader()\n",
    "\n",
    "documents = []\n",
    "for i in files:\n",
    "    documents.append(loader.load_data(f\"{INPUT_PATH}/{i}\"))\n",
    "\n",
    "documents = [c for d in documents for c in d][:3]\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \"Challenging the Concept of Emergent Abilities in Language Models: A Mathematical Alternative\"'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "### [INST] Context: {context_str}. Give a highly concise title that summarizes \\\n",
    "the unique themes found in the context, in no more than 20 words. \\\n",
    "Dont include descriptions of what you are doing, such as this document summarizes. Be as concise as possible. \\\n",
    "\n",
    "Title: [/INST]\"\"\"\n",
    "\n",
    "\n",
    "llm.predict(PromptTemplate(template=prompt), context_str=documents[-2].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llm_predictor.base import LLMPredictorType\n",
    "from llama_index.bridge.pydantic import Field\n",
    "from llama_index.async_utils import run_jobs\n",
    "\n",
    "\n",
    "class CustomLLMExtractor(BaseExtractor):\n",
    "    llm: LLMPredictorType = Field(description=\"The LLM to use for generation.\")\n",
    "    prompt: PromptTemplate = Field(\n",
    "        default=\"\"\"[INST] [/INST]\"\"\",\n",
    "        description=\"The prompt to extract titles with.\",\n",
    "    )\n",
    "\n",
    "    def __init__(self, llm, prompt):\n",
    "        super().__init__(llm=llm, prompt=PromptTemplate(template=prompt))\n",
    "\n",
    "    async def aextract(self, nodes):\n",
    "        jobs = [self.llm.apredict(self.prompt, context_str=node.text) for node in nodes]\n",
    "        candidates = await run_jobs(\n",
    "            jobs, show_progress=self.show_progress, workers=self.num_workers\n",
    "        )\n",
    "\n",
    "        return [{\"node_title\": c.strip(' \\t\\n\\r\"')} for c in candidates]\n",
    "\n",
    "\n",
    "class EntityFlattener(BaseExtractor):\n",
    "    async def aextract(self, nodes):\n",
    "        return [\n",
    "            {\"entities\": \", \".join(node.metadata.get(\"entities\", []))} for node in nodes\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting entities:   0%|          | 0/7 [00:00<?, ?it/s]SpanMarker model predictions are being computed on the CPU while CUDA is available. Moving the model to CUDA using `model.cuda()` before performing predictions is heavily recommended to significantly boost prediction speeds.\n",
      "Extracting entities:  14%|█▍        | 1/7 [00:04<00:29,  4.83s/it]SpanMarker model predictions are being computed on the CPU while CUDA is available. Moving the model to CUDA using `model.cuda()` before performing predictions is heavily recommended to significantly boost prediction speeds.\n",
      "Extracting entities:  29%|██▊       | 2/7 [00:08<00:21,  4.40s/it]SpanMarker model predictions are being computed on the CPU while CUDA is available. Moving the model to CUDA using `model.cuda()` before performing predictions is heavily recommended to significantly boost prediction speeds.\n",
      "Extracting entities:  43%|████▎     | 3/7 [00:12<00:16,  4.19s/it]SpanMarker model predictions are being computed on the CPU while CUDA is available. Moving the model to CUDA using `model.cuda()` before performing predictions is heavily recommended to significantly boost prediction speeds.\n",
      "Extracting entities:  57%|█████▋    | 4/7 [00:14<00:09,  3.10s/it]SpanMarker model predictions are being computed on the CPU while CUDA is available. Moving the model to CUDA using `model.cuda()` before performing predictions is heavily recommended to significantly boost prediction speeds.\n",
      "Extracting entities:  71%|███████▏  | 5/7 [00:16<00:05,  2.77s/it]SpanMarker model predictions are being computed on the CPU while CUDA is available. Moving the model to CUDA using `model.cuda()` before performing predictions is heavily recommended to significantly boost prediction speeds.\n",
      "Extracting entities:  86%|████████▌ | 6/7 [00:20<00:03,  3.20s/it]SpanMarker model predictions are being computed on the CPU while CUDA is available. Moving the model to CUDA using `model.cuda()` before performing predictions is heavily recommended to significantly boost prediction speeds.\n",
      "Extracting entities: 100%|██████████| 7/7 [00:21<00:00,  3.07s/it]\n",
      "100%|██████████| 7/7 [00:27<00:00,  3.94s/it]\n",
      "100%|██████████| 7/7 [00:21<00:00,  3.07s/it]\n"
     ]
    }
   ],
   "source": [
    "transformations = [\n",
    "    SentenceSplitter(chunk_size=512, chunk_overlap=16),\n",
    "    EntityExtractor(prediction_threshold=0.5),\n",
    "    EntityFlattener(),\n",
    "    KeywordExtractor(keywords=10, llm=llm),\n",
    "    CustomLLMExtractor(llm=llm, prompt=prompt),\n",
    "]\n",
    "\n",
    "pipeline = IngestionPipeline(transformations=transformations)\n",
    "nodes = pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '1', 'file_name': 'data/.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'entities': 'Rylan Schaeffer, Stanford University, Brando Miranda', 'excerpt_keywords': 'Emergent abilities, large language models, complex systems, Nobel Prize, P.W. Anderson, \"More Is Different\", complexity, new properties, microscopic details, emergence (in physics), system behavior, model scaling, metrics, nonlinear metrics, discontinuous metrics, linear metrics, continuous metrics, predictability, AI models, BIG-Bench, InstructGPT, GPT-3.', 'node_title': 'Challenging the Reality of Emergent Abilities in Large Language Models: A Metric-Dependent Perspective'}\n",
      "{'page_label': '1', 'file_name': 'data/.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'entities': 'LaMDA', 'excerpt_keywords': 'emergence, machine learning, large language models, GPT-3, PaLM, LaMDA, performance improvements, scale, unpredictability, sharp transition.', 'node_title': 'Emergence of Unpredictable Abilities in Large Language Models: Sudden Appearance and Sharp Transition'}\n",
      "{'page_label': '2', 'file_name': 'data/.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'entities': '', 'excerpt_keywords': 'Emergent abilities, large language models, model families, sharp increases, unpredictable changes, control of abilities, desirable abilities, undesirable abilities, AI safety, alignment, per-token error rate, metrics, multiple choice graded, exact string match.', 'node_title': 'Challenging the Concept of Emergent Abilities in Language Models: A Metric-Dependent Illusion?'}\n",
      "{'page_label': '2', 'file_name': 'data/.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'entities': '', 'excerpt_keywords': 'Mathematical model, alternative explanation, LLMs, InstructGPT, GPT-3, quantitatively reproduce, evidence, emergent abilities, predictions, keywords (for this document: mathematical model, alternative explanation, LLMs, InstructGPT, GPT-3, quantitatively reproduce, evidence, emergent abilities, predictions, keywords)', 'node_title': 'Quantitative Modeling Alternative Explanations for LLM\\'s Emergent Abilities: Predictions and Titles\"\\n\\nor\\n\\n\"Alternative Hypotheses for LLMs: Predictions and Concise Title'}\n",
      "{'page_label': '3', 'file_name': 'data/.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'entities': '', 'excerpt_keywords': 'Large language models, Model parameters, Multiple choice grade, Published emergent ability, Accuracy, Per-token Probability of correct answer, Brier Score, Cross-entropy loss, Monotonic decrease, Power law.', 'node_title': 'Model Comparison: Emergent Abilities and Metrics in Large Language Models'}\n",
      "{'page_label': '3', 'file_name': 'data/.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'entities': 'Brier Score, Token Edit Distance', 'excerpt_keywords': 'emergent abilities, model performance, metrics, nonlinear metric, discontinuous metric, Accuracy, Multiple Choice Grade, Brier Score, neural scaling laws, test loss.', 'node_title': 'Emergent Abilities in AI: Metric Choice and Scaling Laws\"\\n\\n[Explanation of the title]: This title highlights the role of metric choice and scaling laws in shaping the apparent emergence of abilities in artificial intelligence.'}\n",
      "{'page_label': '3', 'file_name': 'data/.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'entities': '', 'excerpt_keywords': 'Model family, parameters N, per-token cross entropy, power law, constants c, α, Figure 2A, LCE (Likelihood per Token Cross Entropy), falls, number of parameters.', 'node_title': 'Power Law Relationship Between Model Complexity and Per-Token Cross Entropy (Fig. 2A)'}\n"
     ]
    }
   ],
   "source": [
    "for i, n in enumerate(nodes):\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/starscream/wheeljack1/projects/casper/src/language/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "llama_index.embeddings.huggingface.HuggingFaceEmbedding"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.persist(PERSIST_PATH)\n",
    "for node in nodes:\n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    node.embedding = node_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the provided context, the text suggests that there have been observations of \"emergent abilities\" in large language models (LLMs) such as GPT-3, PaLM, and LaMDA. These emergent abilities are defined as capabilities that are not present in smaller-scale models but appear in larger ones. The text also mentions that these abilities can transition seemingly instantaneously from not present to present. However, it's important to note that the existence of emergence in LLMs is still a topic of ongoing research and debate. Some researchers argue that these abilities are not truly emergent but rather an artifact of scale or complexity. The text also mentions an alternative explanation for these abilities being presented as a mathematical model, which aims to quantitatively reproduce the evidence offered in support of emergent abilities. Therefore, while there is evidence suggesting the existence of emergent abilities in LLMs, it's not definitively established and further research is needed to fully understand their nature and origin.\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex(\n",
    "    nodes, storage_context=storage_context, service_context=service_context\n",
    ")\n",
    "query_engine = index.as_query_engine()\n",
    "query_str = \"Does emergence in LLMs really happen and when?\"\n",
    "response = query_engine.query(query_str)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the provided context, the text suggests that there have been observations of \"emergent abilities\" in large language models (LLMs) such as GPT-3, PaLM, and LaMDA. These emergent abilities are defined as capabilities that are not present in smaller-scale models but appear in larger ones. The text also mentions that these abilities can transition seemingly instantaneously from not present to present. However, it's important to note that the existence of emergence in LLMs is still a topic of ongoing research and debate. The text also mentions that there are alternative explanations for the observed behaviors of LLMs that should be considered. Therefore, while the text suggests that emergence may occur in LLMs, it does not definitively prove or disprove its existence.\n"
     ]
    }
   ],
   "source": [
    "# load from disk\n",
    "db_2 = chromadb.PersistentClient(path=PERSIST_PATH)\n",
    "chroma_collection_2 = db_2.get_or_create_collection(\"quickstart\")\n",
    "vector_store_2 = ChromaVectorStore(chroma_collection=chroma_collection_2)\n",
    "index_2 = VectorStoreIndex.from_vector_store(\n",
    "    vector_store_2,\n",
    "    service_context=service_context,\n",
    ")\n",
    "\n",
    "query_engine = index_2.as_query_engine()\n",
    "query_str = \"Does emergence in LLMs really happen and when?\"\n",
    "response = query_engine.query(query_str)\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral_u",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
