{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct Preference Optimization: Your Language Model is Secretly a Reward Model\n",
      "Are Emergent Abilities of Large Language Models a Mirage?\n",
      "Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import arxiv\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "client = arxiv.Client(page_size=10, delay_seconds=10.0, num_retries=20)\n",
    "search = arxiv.Search(id_list=[\"2305.18290\", \"2304.15004\", \"2312.08782\"])\n",
    "results = client.results(search)\n",
    "\n",
    "# Prints 1000 titles before needing to make another request.\n",
    "for r in results:\n",
    "    print(r.title)\n",
    "    r.download_pdf(dirpath=\".papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACS: Mass Conditioned 3D Hand and Object Motion Synthesis\n",
      "Family Puzzle, Framing Topology, $c_-=24$ and 3(E8)$_1$ Conformal Field Theories: 48/16 = 45/15 = 24/8 =3\n",
      "Model Reduction to Spectral Submanifolds in Non-Smooth Dynamical Systems\n",
      "Topological Green's function zeros in an exactly solved model and beyond\n",
      "A Survey of Reinforcement Learning from Human Feedback\n"
     ]
    }
   ],
   "source": [
    "search = arxiv.Search(\n",
    "    query=\"emergence in large language models\",\n",
    "    max_results=5,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    ")\n",
    "results = client.results(search)\n",
    "\n",
    "# Prints 1000 titles before needing to make another request.\n",
    "for r in results:\n",
    "    print(r.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIRECTORY = \".pipeline\"\n",
    "files = os.listdir(DIRECTORY)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id_='71ae2f60-940b-43f0-84d7-0c03235ddf8e', embedding=None, metadata={'page_label': '1', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='2f6c3dd94d8e6a1f97f07f2ce1df2512ca573ac0385a0ea64be4bf13b33a33e8', text='Are Emergent Abilities of Large Language Models a\\nMirage?\\nRylan Schaeffer, Brando Miranda, and Sanmi Koyejo\\nComputer Science, Stanford University\\nAbstract\\nRecent work claims that large language models display emergent abilities , abil-\\nities not present in smaller-scale models that are present in larger-scale models.\\nWhat makes emergent abilities intriguing is two-fold: their sharpness , transition-\\ning seemingly instantaneously from not present to present, and their unpredictabil-\\nity, appearing at seemingly unforeseeable model scales. Here, we present an al-\\nternative explanation for emergent abilities: that for a particular task and model\\nfamily, when analyzing ﬁxed model outputs, emergent abilities appear due the\\nresearcher’s choice of metric rather than due to fundamental changes in model\\nbehavior with scale. Speciﬁcally, nonlinear or discontinuous metrics produce ap-\\nparent emergent abilities, whereas linear or continuous metrics produce smooth,\\ncontinuous, predictable changes in model performance. We present our alternative\\nexplanation in a simple mathematical model, then test it in three complementary\\nways: we (1) make, test and conﬁrm three predictions on the effect of metric\\nchoice using the InstructGPT/GPT-3 family on tasks with claimed emergent abil-\\nities, (2) make, test and conﬁrm two predictions about metric choices in a meta-\\nanalysis of emergent abilities on BIG-Bench; and (3) show how to choose metrics\\nto produce never-before-seen seemingly emergent abilities in multiple vision tasks\\nacross diverse deep networks. Via all three analyses, we provide evidence that al-\\nleged emergent abilities evaporate with different metrics or with better statistics,\\nand may not be a fundamental property of scaling AI models.\\n1 Introduction\\nEmergent properties of complex systems have long been studied across disciplines, from physics to\\nbiology to mathematics. The idea of emergence was popularized by Nobel Prize-winning physicist\\nP.W. Anderson’s “More Is Different” [1], which argues that as the complexity of a system increases,\\nnew properties may materialize that cannot be predicted even from a precise quantitative understand-\\ning of the system’s microscopic details. Recently, the idea of emergence gained signiﬁcant attention\\nin machine learning due to observations that large language models (LLMs) such as GPT [3], PaLM\\n[6] and LaMDA [30] exhibit so-called “emergent abilities” [33, 8, 28, 3] (Fig. 1).\\nThe term “emergent abilities of LLMs” was recently and crisply deﬁned as “abilities that are not\\npresent in smaller-scale models but are present in large-scale models; thus they cannot be predicted\\nby simply extrapolating the performance improvements on smaller-scale models” [33]. Such emer-\\ngent abilities were ﬁrst discovered in the GPT-3 family [3]. Subsequent work emphasized the discov-\\nery, writing that “[although model] performance is predictable at a general level, performance on a\\nspeciﬁc task can sometimes emerge quite unpredictably and abruptly at scale” [8]. These quotations\\ncollectively identify the two deﬁning properties of emergent abilities in LLMs:\\n1.Sharpness , transitioning seemingly instantaneously from not present to present\\nPreprint. Under review.arXiv:2304.15004v2  [cs.AI]  22 May 2023', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8081ff76-c3bd-4376-b128-deae874dba2d', embedding=None, metadata={'page_label': '2', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='d74b72a185048718e5a7cf97d0bda92a2a76375c8145ef77665959b2e4f91db7', text='Figure 1: Emergent abilities of large language models . Model families display sharp andunpre-\\ndictable increases in performance at speciﬁc tasks as scale increases. Source: Fig. 2 from [33].\\n2.Unpredictability , transitioning at seemingly unforeseeable model scales\\nThese emergent abilities have garnered signiﬁcant interest, raising questions such as: What controls\\nwhich abilities will emerge? What controls when abilities will emerge? How can we make desirable\\nabilities emerge faster, and ensure undesirable abilities never emerge? These questions are especially\\npertinent to AI safety and alignment, as emergent abilities forewarn that larger models might one\\nday, without warning, acquire undesired mastery over dangerous capabilities [29, 10, 17, 18].\\nIn this paper, we call into question the claim that LLMs possess emergent abilities, by which we\\nspeciﬁcally mean sharp andunpredictable changes in model outputs as a function of model scale\\non speciﬁc tasks. Our doubt stems from the observation that emergent abilities seem to appear\\nonly under metrics that nonlinearly or discontinuously scale any model’s per-token error rate. For\\ninstance, as we later show, >92% of emergent abilities on BIG-Bench tasks [28] (hand-annotated\\nby [32]) appear under either of these two metrics:\\nMultiple Choice Gradedef={1if highest probability mass on correct option\\n0otherwise\\nExact String Matchdef={1if output string exactly matches target string\\n0otherwise\\nThis raises the possibility of an alternative explanation for the origin of LLMs’ emergent abilities:\\nsharp and unpredictable changes might be induced by the researcher’s choice of measurement, even\\nthough the model family’s per-token error rate changes smoothly, continuously and predictably with\\nincreasing scale. Speciﬁcally, our alternative posits that emergent abilities are a mirage caused\\nprimarily by the researcher choosing a metric that nonlinearly or discontinuously deforms per-token\\nerror rates, and secondarily by possessing too few test data to accurately estimate the performance\\nof smaller models, thereby causing smaller models to appear wholly unable to perform the task.\\nTo communicate our alternative explanation, we present it as a simple mathematical model and\\ndemonstrate how it quantitatively reproduces the evidence offered in support of emergent abilities\\nof LLMs. We then test our alternative explanation in three complementary ways:\\n1. We make, test and conﬁrm three predictions based on our alternative hypotheses using the\\nInstructGPT [24] / GPT-3 [3] model family.\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5bdb772f-8219-4e7e-8d60-28daa5462d82', embedding=None, metadata={'page_label': '3', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='64aefe95c6f85237c360fd9cac7f03b7e48850420fa4bff16d3cf661a7b95ad5', text='109\\n1010\\n1011\\nModel Parameters\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0Multiple Choice Grade\\nNum. Choices\\n2D\\n109\\n1010\\n1011\\nModel Parameters\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7Multiple Choice Grade\\nPublished Emergent Ability\\n109\\n1010\\n1011\\nModel Parameters\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0Accuracy\\nT arget Str Len\\n5\\n109\\n1010\\n1011\\nModel Parameters\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0Accuracy\\nPublished Emergent Ability C\\n109\\n1010\\n1011\\nModel Parameters\\n101\\n100CE(Per-Token)\\nA\\n109\\n1010\\n1011\\nModel Parameters\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0Prob Correct (Per-T oken)\\nB\\nEmergent Abilities No Emergent Abilities\\nDiscontinuously\\nscore\\nLLM outputs\\n109\\n1010\\n1011\\nModel Parameters\\n4\\n3\\n2\\n1\\n0- Number of Incorrect T okens\\nT arget Str Len\\n5E\\nNonlinearly\\nscore\\nLLM outputs\\nContinuously\\nscore\\nLLM outputsLinearly\\nscore\\nLLM outputs\\n109\\n1010\\n1011\\nModel Parameters\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0- Brier Score\\nNum. Choices\\n2FFigure 2: Emergent abilities of large language models are created by the researcher’s chosen\\nmetrics, not unpredictable changes in model behavior with scale. (A) Suppose the per-token\\ncross-entropy loss decreases monotonically with model scale, e.g., LCEscales as a power law. (B)\\nThe per-token probability of selecting the correct token asymptotes towards 1. (C) If the researcher\\nscores models’ outputs using a nonlinear metric such as Accuracy (which requires a sequence of\\ntokens to allbe correct), the metric choice nonlinearly scales performance, causing performance\\nto change sharply and unpredictably in a manner that qualitatively matches published emergent\\nabilities (inset). (D) If the researcher instead scores models’ outputs using a discontinuous metric\\nsuch as Multiple Choice Grade (akin to a step function), the metric choice discontinuously scales\\nperformance, again causing performance to change sharply and unpredictably. (E) Changing from a\\nnonlinear metric to a linear metric such as Token Edit Distance, scaling shows smooth, continuous\\nand predictable improvements, ablating the emergent ability. (F) Changing from a discontinuous\\nmetric to a continuous metric such as Brier Score again reveals smooth, continuous and predictable\\nimprovements in task performance. Consequently, emergent abilities are created by the researcher’s\\nchoice of metrics, not fundamental changes in model family behavior on speciﬁc tasks with scale.\\n2. We meta-analyze published benchmarks [28, 33] to reveal that emergent abilities only ap-\\npear for speciﬁc metrics, not for model families on particular tasks, and that changing the\\nmetric causes the emergence phenomenon to evaporate.\\n3. We induce never-before-seen, seemingly emergent abilities in multiple architectures across\\nvarious vision tasks by intentionally changing the metrics used for evaluation.\\n2 Alternative Explanation for Emergent Abilities\\nHow might smooth, continuous, predictable changes in model family performance appear sharp and\\nunpredictable? The answer is that the researcher’s choice of a nonlinear or discontinuous metric can\\ndistort the model family’s performance to appear sharp and unpredictable.\\nTo expound, suppose that within a model family, the test loss falls smoothly, continuously and\\npredictably with the number of model parameters. One reason to believe this is the phenomenon\\nknown as neural scaling laws: empirical observations that deep networks exhibit power law scaling\\nin the test loss as a function of training dataset size, number of parameters or compute [13, 27, 11,\\n16, 9, 12, 15, 34, 14, 7, 26]. For concreteness, suppose we have a model family of different numbers\\nof parameters N > 0and assume that each model’s per-token cross entropy falls as a power law\\nwith the number of parameters Nfor constants c>0,α< 0(Fig. 2A):\\nLCE(N) =(N\\nc)α\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='b3aed4fb-ed5c-4e1a-bd97-fe39dbbb1a10', embedding=None, metadata={'page_label': '4', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='7454a1f7746355b3416d24820439c6ae72e51c6465e552436e63a1c61b537f93', text='To be clear, we do not require this particular functional form to hold; rather, we use it for illustra-\\ntive purposes. Let Vdenote the set of possible tokens, p∈∆|V|−1denote the true but unknown\\nprobability distribution, and ˆpN∈∆|V|−1denote theN-parameter model’s predicted probability\\ndistribution. The per-token cross entropy as a function of number of parameters Nis:\\nLCE(N)def=−∑\\nv∈Vp(v) log ˆpN(v)\\nIn practice,pis unknown, so we substitute a one-hot distribution of the observed token v∗:\\nLCE(N) =−log ˆpN(v∗)\\nA model with Nparameters then has a per-token probability of selecting the correct token (Fig. 2B):\\np(single token correct ) = exp(\\n−LCE(N))\\n= exp(\\n−(N/c)α)\\nSuppose the researcher then chooses a metric that requires selecting Ltokens correctly. For example,\\nour task might be L-digit integer addition, and a model’s output is scored 1if allLoutput digits\\nexactly match all target digits with no additions, deletions or substitutions, 0otherwise. If the\\nprobability each token is correct is independent1, the probability of scoring 1is:\\nAccuracy (N)≈pN(single token correct )num. of tokens= exp(\\n−(N/c)α)L\\nThis choice of metric nonlinearly scales performance with increasing token sequence length. When\\nplotting performance on a linear-log plot, one sees a sharp, unpredictable emergent ability on longer\\nsequences (Fig. 2C) that closely matches claimed emergent abilities (inset). What happens if the\\nresearcher switches from a nonlinear metric like Accuracy, under which the per-token error rate\\nscales geometrically in target length (App. A.3), to an approximately linear metric like Token Edit\\nDistance, under which the per-token error rate scales quasi-linearly in target length (App. A.2)?\\nToken Edit Distance (N)≈L(\\n1−pN(single token correct ))\\n=L(\\n1−exp(\\n−(N/c)α))\\nThe linear metric reveals smooth, continuous, predictable changes in model performance (Fig. 2E).\\nSimilarly, if the researcher uses a discontinuous metric like Multiple Choice Grade, the researcher\\ncan ﬁnd emergent abilities (Fig. 2D), but switching to a continuous metric like Brier Score removes\\nthe emergent ability (Fig. 2F). In summary, sharp and unpredictable changes with increasing scale\\ncan be fully explained by three interpretable factors: (1) the researcher choosing a metric that nonlin-\\nearly or discontinuously scales the per-token error rate, (2) having insufﬁcient resolution to estimate\\nmodel performance in the smaller parameter regime, with resolution2set by 1/test dataset size, and\\n(3) insufﬁciently sampling the larger parameter regime.\\n3 Analyzing InstructGPT/GPT-3’s Emergent Arithmetic Abilities\\nPrevious papers prominently claimed the GPT [3, 24] family3displays emergent abilities at integer\\narithmetic tasks [8, 28, 33] (Fig. 2E). We chose these tasks as they were prominently presented\\n[3, 8, 28, 33], and we focused on the GPT family due to it being publicly queryable. As explained\\nmathematically and visually in Sec. 2, our alternative explanation makes three predictions:\\n1. Changing the metric from a nonlinear or discontinuous metric (Fig. 2CD) to a linear or\\ncontinuous metric (Fig. 2 EF) should reveal smooth, continuous, predictable performance\\nimprovement with model scale.\\n1While the independence assumption is not true, the approximation yields results qualitatively matching the\\nobserved emergence claims.\\n2Resolution is deﬁned as “The smallest interval measurable by a scientiﬁc instrument; the resolving power.”\\n3As of 2023-03-15, 4 models with 350M, 1.3B, 6.7B, 175B parameters are available via the OpenAI API.\\n4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='13fdf59a-c381-47a9-a17a-74fe25e860b0', embedding=None, metadata={'page_label': '5', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='bb2783b4a3dcc00f798ac88ff7d51d624b23f3627f08aa2aae9c9dd9f4669b54', text='10910101011\\nModel Parameters0.00.20.40.60.81.0Accuracy\\nT arget Str Len\\n1\\n2\\n3\\n4\\n5\\n10910101011\\nGPT-3 Model Parameters0.00.20.40.60.81.0AccuracyT arget Str Len\\n1\\n2\\n3\\n4\\n5\\nT emp\\n0.0\\n1.0\\n10910101011\\nGPT-3 Model Parameters0.00.20.40.60.81.0AccuracyT arget Str Len\\n1\\n2\\n3\\n4\\n5\\nT emp\\n0.0\\n1.0\\n10910101011\\nModel Parameters4\\n3\\n2\\n1\\n0- T oken Edit Distance\\nT arget Str Len\\n1\\n2\\n3\\n4\\n5\\n10910101011\\nGPT-3 Model Parameters6\\n5\\n4\\n3\\n2\\n1\\n0- T oken Edit DistanceT arget Str Len\\n1\\n2\\n3\\n4\\nT emp\\n0.0\\n1.0\\n10910101011\\nGPT-3 Model Parameters6\\n5\\n4\\n3\\n2\\n1\\n0- T oken Edit DistanceT arget Str Len\\n1\\n2\\n3\\n4\\n5\\nT emp\\n0.0\\n1.0Figure 3: Claimed emergent abilities evaporate upon changing the metric. Left to Right: Math-\\nematical Model, 2-Integer 2-Digit Multiplication Task, 2-Integer 4-Digit Addition Task. Top: When\\nperformance is measured by a nonlinear metric (e.g., Accuracy), the InstructGPT/GPT-3 [3, 24]\\nfamily’s performance appears sharp and unpredictable on longer target lengths. Bottom: When per-\\nformance is instead measured by a linear metric (e.g., Token Edit Distance), the family exhibits\\nsmooth, predictable performance improvements.\\n10910101011\\nModel Parameters106\\n105\\n104\\n103\\n102\\n101\\n100Accuracy\\nT arget Str Len\\n1\\n2\\n3\\n4\\n5\\nFigure 4: Claimed emergent abilities evaporate upon using better statistics. Left to Right: Math-\\nematical Model, 2-Integer 2-Digit Multiplication Task, 2-Integer 4-Digit Addition Task. Based on\\nthe predictable effect Accuracy has on performance, measuring performance requires high resolu-\\ntion. Generating additional test data increases the resolution and reveals that even on Accuracy,\\nthe InstructGPT/GPT-3 family’s [3, 24] performance is above chance and improves in a smooth,\\ncontinuous, predictable manner that qualitatively matches the mathematical model.\\n2. For nonlinear metrics, increasing the resolution of measured model performance by in-\\ncreasing the test dataset size should reveal smooth, continuous, predictable model improve-\\nments commensurate with the predictable nonlinear effect of the chosen metric .\\n3. Regardless of metric, increasing the target string length should predictably affect the\\nmodel’s performance as a function of the length-1 target performance: approximately geo-\\nmetrically for accuracy and approximately quasilinearly for token edit distance.\\nTo test these predictions, we collected outputs from the InstructGPT/GPT-3 family on two tasks:\\n2-shot multiplication between two 2-digit integers and 2-shot addition between two 4-digit integers.\\nPrediction: Emergent Abilities Disappear With Different Metrics On both arithmetic tasks, the\\nGPT family displays emergent abilities if the target has 4 or 5 digits and if the metric is Accuracy\\n(Fig. 3, top) [3, 8, 33]. However, if one changes from nonlinear Accuracy to linear Token Edit\\nDistance while keeping the models’ outputs ﬁxed , the family’s performance smoothly, continuously\\n5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='0ed8b3a1-9fdd-4b52-9eb1-17a72abec082', embedding=None, metadata={'page_label': '6', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='fd0c71b401a1bc46b50a826752a940aed20b75aa2d98f6ae96f76e2a596a26f8', text='and predictably improves with increasing scale (Fig. 3, bottom). This conﬁrms our ﬁrst prediction\\nand supports our alternative explanation that the source of emergent abilities is the researcher’s\\nchoice of metric, not changes in the model family’s outputs . We also observe that under Token Edit\\nDistance, increasing the length of the target string from 1 to 5 predictably decreases the family’s\\nperformance in an approximately quasilinear manner, conﬁrming the ﬁrst half of our third prediction.\\nPrediction: Emergent Abilities Disappear With Better Statistics We next tested our second\\nprediction: that even on nonlinear metrics such as accuracy, smaller models do not have zero ac-\\ncuracy, but rather have non-zero above-chance accuracy commensurate with choosing to use ac-\\ncuracy as the metric . In order to accurately measure models’ accuracy, we increased the resolu-\\ntion by generating additional test data, and found that on both arithmetic tasks, all models in the\\nInstructGPT/GPT-3 family achieve above-chance accuracy (Fig. 4). This conﬁrms our second pre-\\ndiction. We also observe that as the target string length increases, the accuracy falls approximately\\ngeometrically with the length of the target string, conﬁrming the second half of our third prediction.\\nThese results additionally demonstrate that the researcher’s choice of metric has the effect that one\\nshould predict accuracy to have, i.e., geometric decay with the target length.\\n4 Meta-Analysis of Claimed Emergent Abilities\\nAnalyzing the GPT family is possible because the models are publicly queryable. However, other\\nmodel families claimed to exhibit emergent abilities are not publicly queryable, nor are their gener-\\nated outputs publicly available, meaning we are limited to analyzing the published results themselves\\n[8, 33, 32]. Our alternative explanation makes two predictions.\\n1. At the “population level” of Task-Metric-Model Family triplets, emergent abilities should\\nappear predominantly on speciﬁc metrics , not task-model family pairs, and speciﬁcally with\\nnonlinear and/or discontinuous metrics.\\n2. On individual Task-Metric-Model Family triplets that display an emergent ability, changing\\nthe metric to a linear and/or continuous metric should remove the emergent ability.\\nTo test these predictions, we used to claimed emergent abilities on BIG-Bench [28, 33] due to the\\nbenchmark being pertinent and publicly available.\\nPrediction: Emergent Abilities Should Appear with Metrics, not Task-Model Families If\\nemergent abilities are real, one should expect task-model family pairs to show emergence for all\\nreasonable metrics. However, if our alternative explanation is correct, we should expect emergent\\nabilities to appear only under certain metrics. To test this, we analyzed on which metrics emergent\\nabilities appear. To determine whether a task-metric-model family triplet exhibits a possible emer-\\ngent ability, we used a metric from previous work [28]. Letting yi∈Rdenote model performance\\nat model scales xi∈R, sorted such that xi<xi+1, the emergence score is:\\nEmergence Score({\\n(xn,yn)}N\\nn=1)def=sign(arg maxiyi−arg miniyi)(maxiyi−miniyi)√\\nMedian ({(yi−yi−1)2}i)\\n(1)\\nWe found that most metrics used in BIG-Bench have zero task-model family pairs that exhibit emer-\\ngent abilities: of the 39 preferred metrics in BIG-Bench, at most 5 display emergence (Fig. 5A).\\nMany of the 5 are nonlinear and/or discontinuous, e.g., Exact String Match, Multiple Choice Grade,\\nROUGE-L-Sum (App. A.4). Notably, because BIG-Bench often scores models on tasks using mul-\\ntiple metrics, the lack of emergent abilities under other metrics suggests that emergent abilities do\\nnot appear when model outputs are scored using other metrics.\\nBecause emergence score only suggests emergence, we also analyzed hand-annotated task-metric-\\nmodel family triplets [32], which revealed emergent abilities appear with 4/39metrics (Fig. 5B), and\\n2 metrics account for >92% of claimed emergent abilities (Fig. 5C): Multiple Choice Grade and\\nExact String Match. Multiple Choice Grade is discontinuous, and Exact String Match is nonlinear.\\nPrediction: Changing Metric Removes Emergent Abilities To test our second prediction, we\\nfocused on the LaMDA family [30] because its outputs are available through BIG-Bench. For our\\n6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c4933c01-0917-469b-a4dd-78521506cc2b', embedding=None, metadata={'page_label': '7', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='a3fcbde387cd515ef75ebeaf34e5de39c01d42ddda79669abeae29bb087d382c', text='0 100 200 300 400 500 600\\nEmergence Score (Defined in Srivastava et al. 2022) Over All BIG-Bench T asksaccuracy\\nalignment_score\\naverage\\naverage_log_probability\\navg_acc\\nbias_level\\nbleu\\nbleurt\\nbleurt_diff\\ncombined_bias\\ncorrect\\ncorrect_prob_mass\\ncustom_score\\ndifference_score\\nexact_str_match\\nf1\\nfairness\\nfull\\ngender_bias_score\\ngender_minority_bias_score\\ngender_minority_stereotype_score\\ngender_stereotype_score\\nlog10_p_dev\\nlog_likelihood\\nmacro_f1\\nmain_words_match\\nmean_accuracy\\nmultiple_choice_grade\\nnormalized_aggregate_score\\nnumeric_match_with_0_1_relative_error\\noverall\\noverall gender bias\\noverall_alpha_avg\\noverall_difference\\npair-wise-accuracy\\nrelative_score\\nrougeLsum\\nsequence_f1\\ntargets_reachedMetric\\nFigure 5: Emergent abilities appear only for speciﬁc metrics, not task-model families. (A)Pos-\\nsible emergent abilities appear with at most 5 out of 39 BIG-Bench metrics. (B) Hand-annotated data\\nby [32] reveals emergent abilities appear only under 4 preferred metrics. (C) >92% of emergent\\nabilities appear under one of two metrics: Multiple Choice Grade and Exact String Match.\\nFigure 6: Changing the metric when evaluating task-model family pairs causes emergent abil-\\nities to disappear. Left: The LaMDA model family displays emergent abilities when measured\\nunder the discontinuous Multiple Choice Grade. Right: The LaMDA model family’s emergent abil-\\nities disappear when measured under a continuous BIG-Bench metric: Brier Score.\\nanalysis, we identiﬁed tasks on which LaMDA displays emergent abilities with Multiple Choice\\nGrade, then asked whether LaMDA still displays emergent abilities on the same tasks with a different\\nBIG-Bench metric: Brier Score [2]. Brier Score is a strictly proper scoring rule for predictions of\\nmutually exclusive outcomes; for a binary outcome, the Brier Score simpliﬁes to the mean squared\\nerror between the outcome and its predicted probability mass. LaMDA’s emergent abilities on the\\ndiscontinuous Multiple Choice Grade disappeared when we changed the metric to the continuous\\nBrier Score (Fig. 6). These results support our alternative explanation that emergent abilities are\\ninduced by the chosen metric.\\n5 Inducing Emergent Abilities in Networks on Vision Tasks\\nTo demonstrate how emergent abilities can be induced by the researcher’s choice of metric, we\\nshow how to produce emergent abilities in deep networks of various architectures: fully connected,\\nconvolutional, self-attentional. We focus on vision tasks because abrupt transitions in vision models’\\ncapabilities have not been observed to the best of our knowledge; this is one reason why emergence\\nin large language models is considered so interesting. For the convolutional example, see App. B.\\nEmergent Reconstruction of CIFAR100 Natural Images by Nonlinear Autoencoders We ﬁrst\\ninduce an emergent ability to reconstruct images in shallow (i.e., single hidden layer) nonlinear au-\\ntoencoders trained on CIFAR100 natural images [19]. To emphasize that the sharpness of the metric\\nis responsible for emergent abilities, and to show that sharpness extends to metrics beyond Accu-\\nracy, we intentionally deﬁne a discontinuous metric that measures a network’s ability to reconstruct\\n7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f220426a-b268-4c43-a017-5d90e176a2a3', embedding=None, metadata={'page_label': '8', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='19ee088ab55c708410e83166509ad1004d2fb5a33954036a0aa065810475c9f9', text='10710810910101011\\nLanguage Model Effective Parameters05101520Normalized Score\\nPublished Emergent Ability\\nPublished Emergent Ability\\n104105106\\nShallow Autoencoder Model Parameters102\\n101\\nTest Mean Squared Error   =   1\\nNN\\nn=1||xn xn||2\\nNo Emergent Ability\\n104105106\\nShallow Autoencoder Model Parameters0.00.10.20.30.4Test Reconstruction Ability   =   1\\nNN\\nn=1[||xn xn||2<c]\\nMetric-Induced Emergent Ability\\nc\\n0.003\\n0.004Figure 7: Induced emergent reconstruction ability in shallow nonlinear autoencoders. (A) A\\npublished emergent ability at the BIG-Bench Periodic Elements task [28]. (B) Shallow nonlinear\\nautoencoders trained on CIFAR100 [19] display smoothly decreasing mean squared reconstruction\\nerror. (C) Using a newly deﬁned Reconstruction cmetric (Eqn. 2) induces an unpredictable change.\\n10101011\\nGPT-3 Model Parameters0.200.250.300.350.400.450.500.550.60T est Accuracy\\nPublished Emergent Ability\\nPublished Emergent Ability\\n106\\nCausal Transformer Parameters0.00.20.40.60.81.0T est Accuracy\\nNo Emergent Ability\\nNum. All Correct\\n1\\n106\\nCausal Transformer Parameters0.00.20.40.60.81.0T est Accuracy\\nMetric-Induced Emergent Ability\\nNum. All Correct\\n3\\nFigure 8: Induced emergent classiﬁcation ability in autoregressive Transformers. (A) A pub-\\nlished emergent ability on the MMLU benchmark [8]. (B) Autoregressive transformers trained to\\nclassify Omniglot images display increasing accuracy with increasing scale. (C) When accuracy is\\nredeﬁned as classifying allimages correctly, a seemingly emergent ability appears.\\na dataset as the average number of test data with squared reconstruction error below threshold c:\\nReconstruction c(\\n{xn}N\\nn=1)def=1\\nN∑\\nnI[\\n||xn−ˆxn||2<c]\\n(2)\\nwhere I(·)denotes an indicator variable and ˆxnis the autoencoder’s reconstruction of xn. The\\nautoencoder family displays smoothly decreasing squared reconstruction error as the number of bot-\\ntleneck units increases (Fig. 7B). Under our newly deﬁned Reconstruction cmetric and for particular\\nchoices ofc, the autoencoder family exhibits a sharp and seemingly unpredictable image reconstruc-\\ntion ability (Fig. 7C) that qualitatively matches published emergent abilities (Fig. 7A).\\nEmergent Classiﬁcation of Omniglot Characters by Autoregressive Transformers We next\\ninduce emergent abilities in Transformers [31] trained to autoregressively classify Omniglot hand-\\nwritten characters [20], in a setup inspired by recent work [5]: Omniglot images are embedded\\nby convolutional layers, then sequences of embedded image-image class label pairs are fed into\\ndecoder-only transformers. We measure image classiﬁcation performance on sequences of length\\nL∈[1,5], again via subset accuracy :1if allLimages are classiﬁed correctly (Fig. 8B), 0 oth-\\nerwise. Causal transformers display a seemingly emergent ability to correctly classify Omniglot\\nhandwritten characters (Fig. 8C) that qualitatively matches published emergent abilities (Fig. 8A).\\n6 Related Work\\nSrivastava et al. [28] observed that while accuracy at a particular task can empirically appear sharp\\nand unpredictable, cross entropy does not; the authors then hypothesized that emergent abilities may\\nbe partially attributed to the metric. Our paper converts their discussion into precise predictions,\\n8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ab07752f-2a4b-4270-b65c-906b14ddc52f', embedding=None, metadata={'page_label': '9', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='24647244cbc74e13929cedb4591b6136db69d5fa73b9be9cda966bb521fd493d', text='then quantitatively tests the predictions to reveal that: metric choice is likely wholly responsible\\nfor emergent abilities; well-known and widely-used metrics (including ones already used by [28])\\ncapture graded improvements; emergent abilities do not appear only for tasks involving multiple\\nsteps, and indeed appear most commonly on the discontinuous Multiple Choice Grade; metric choice\\ncan be used to induce emergent abilities in a novel domain (vision) in diverse architectures and tasks.\\nCaballero et al. [4] explain emergence by assuming a piece-wise power law functional form; under\\nthis view, emergent abilities are real, caused by a change in the governing power law. In contrast,\\nour work suggests that emergent abilities are induced by the researcher, even under a single power\\nlaw. Michaud et al. [25] posit that emergent abilities may be real under strong data assumptions.\\n7 Discussion\\nOur paper presents an alternative explanation for claimed emergent abilities of large language mod-\\nels. For a ﬁxed task and a ﬁxed model family, the researcher can choose a metric to create an\\nemergent ability or choose a metric to ablate an emergent ability. Ergo, emergent abilities may be\\ncreations of the researcher’s choices, not a fundamental property of the model family on the speciﬁc\\ntask. We emphasize that nothing in this paper should be interpreted as claiming that large language\\nmodels cannot display emergent abilities; rather, our message is that previously claimed emergent\\nabilities in [3, 8, 28, 33] might likely be a mirage induced by researcher analyses.\\nOur paper has several implications. Firstly, a task and a metric are distinct and meaningful choices\\nwhen constructing a benchmark. Secondly, when choosing metric(s), one should consider the met-\\nric’s effect on the per-token error rate and adapt their measuring process accordingly, e.g., if one\\nchooses accuracy, one should make sure to have sufﬁcient data to accurately measure accuracy to\\navoid the risk of drawing invalid scientiﬁc conclusions. Thirdly, when making claims about capa-\\nbilities of large models, including proper controls is critical. In this particular setting, emergent\\nabilities claims are possibly infected by a failure to control for multiple comparisons. In BIG-Bench\\nalone, there are≥220 tasks,∼40metrics per task,∼10model families, for a total of ∼106task-\\nmetric-model family triplets, meaning probability that notask-metric-model family triplet exhibits\\nan emergent ability by random chance might be small. Fourthly, scientiﬁc progress can be hampered\\nwhen models and their outputs are not made public for independent scientiﬁc investigation.\\n9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ce71cde5-b579-48ed-a462-5101fd29fd6e', embedding=None, metadata={'page_label': '10', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='fc730ca812c3cda3c7b58947625abd8ad4e61b6623b1fd73010c923db4fdda1b', text='References\\n[1] Philip W Anderson. More is different: broken symmetry and the nature of the hierarchical\\nstructure of science. Science , 177(4047):393–396, 1972.\\n[2] Glenn W Brier et al. Veriﬁcation of forecasts expressed in terms of probability. Monthly\\nweather review , 78(1):1–3, 1950.\\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language mod-\\nels are few-shot learners. Advances in neural information processing systems , 33:1877–1901,\\n2020.\\n[4] Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws.\\narXiv preprint arXiv:2210.14891 , 2022.\\n[5] Stephanie CY Chan, Adam Santoro, Andrew Kyle Lampinen, Jane X Wang, Aaditya K Singh,\\nPierre Harvey Richemond, James McClelland, and Felix Hill. Data distributional properties\\ndrive emergent in-context learning in transformers. In Advances in Neural Information Pro-\\ncessing Systems , 2022.\\n[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\\n[7] Aidan Clark, Diego De Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan\\nHoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Uniﬁed\\nscaling laws for routed language models. In International Conference on Machine Learning ,\\npages 4057–4086. PMLR, 2022.\\n[8] Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom\\nConerly, Nova Dassarma, Dawn Drain, Nelson Elhage, et al. Predictability and surprise in large\\ngenerative models. In 2022 ACM Conference on Fairness, Accountability, and Transparency ,\\npages 1747–1764, 2022.\\n[9] Mitchell A Gordon, Kevin Duh, and Jared Kaplan. Data and parameter scaling laws for neural\\nmachine translation. In Proceedings of the 2021 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 5915–5922, 2021.\\n[10] Dan Hendrycks. Detecting emergent behavior. 2022.\\n[11] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Hee-\\nwoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive\\ngenerative modeling. arXiv preprint arXiv:2010.14701 , 2020.\\n[12] Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for\\ntransfer. arXiv preprint arXiv:2102.01293 , 2021.\\n[13] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kia-\\nninejad, Md Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou. Deep learning scaling is\\npredictable, empirically. arXiv preprint arXiv:1712.00409 , 2017.\\n[14] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\\nTraining compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022.\\n[15] Andy L Jones. Scaling scaling laws with board games. arXiv preprint arXiv:2104.03113 ,\\n2021.\\n[16] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon\\nChild, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural\\nlanguage models. arXiv preprint arXiv:2001.08361 , 2020.\\n[17] Victoria Krakovna, Vikrant Varma, Ramana Kumar, and Mary Phuong. Reﬁning the sharp left\\nturn threat model, part 1: claims and mechanisms. 2022.\\n[18] Victoria Krakovna, Vikrant Varma, Ramana Kumar, and Mary Phuong. Reﬁning the sharp left\\nturn threat model, part 2: applying alignment techniques. 2022.\\n[19] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report,\\n2009.\\n10', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='fb6f9b8e-4e63-4c2d-83cb-d65d33685d99', embedding=None, metadata={'page_label': '11', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='ef15ad7fe78b636a22909de59d0891e717d9bd864827c27120bafcb4b0a2aded', text='[20] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept\\nlearning through probabilistic program induction. Science , 350(6266):1332–1338, 2015.\\n[21] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/ ,\\n1998.\\n[22] Yann LeCun, L ´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning\\napplied to document recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998.\\n[23] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summariza-\\ntion branches out , pages 74–81, 2004.\\n[24] Ryan Lowe and Jan Leike. Aligning language models to follow instructions. 2022.\\n[25] Eric J. Michaud, Ziming Liu, Uzay Girit, and Max Tegmark. The quantization model of neural\\nscaling, 2023.\\n[26] Oren Neumann and Claudius Gros. Scaling laws for a multi-agent reinforcement learning\\nmodel. arXiv preprint arXiv:2210.00849 , 2022.\\n[27] Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive\\nprediction of the generalization error across scales. arXiv preprint arXiv:1909.12673 , 2019.\\n[28] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid,\\nAdam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri `a Garriga-Alonso, et al.\\nBeyond the imitation game: Quantifying and extrapolating the capabilities of language models.\\narXiv preprint arXiv:2206.04615 , 2022.\\n[29] Jacob Steinhardt. Future ml systems will be qualitatively different. 2022.\\n[30] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-\\nTze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for\\ndialog applications. arXiv preprint arXiv:2201.08239 , 2022.\\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\\nprocessing systems , 30, 2017.\\n[32] Jason Wei. 137 emergent abilities of large language models. 2022.\\n[33] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large\\nlanguage models. arXiv preprint arXiv:2206.07682 , 2022.\\n[34] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision trans-\\nformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-\\nnition , pages 12104–12113, 2022.\\n11', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='4b3999b6-0b09-40f8-be23-73a037d90456', embedding=None, metadata={'page_label': '12', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='534817ecc51b8c4e70ec77fa7281ab169eb7273958d4471ff14504058f4db69d', text='A Approximate Behavior of Metrics on Sequential Data\\nHow do different metrics behave when used to measure autoregressive model outputs? Precisely\\nanswering this question is tricky and possibly analytically unsolvable, so we provide an approximate\\nanswer here.\\nNotationally, we consider Ntest data of length L(here, length is measured in tokens) with tar-\\ngets denoted tndef=(tn1,tn2,...tnL), the autoregressive model has a true-but-unknown per-token er-\\nror probability of ϵ∈[0,1]and the model outputs prediction ˆtndef=(ˆtn1,ˆtn2,...ˆtnL). This assumes\\nthat the model’s per-token error probability is constant, which is empirically false, but modeling the\\ncomplex dependencies of errors is beyond our scope.\\nA.1 Per-Token Error Probability is Resolution-Limited\\nNote that because we have Ntest data, each of length L, our resolution for viewing the per-token\\nerror probability ϵis limited by 1/NL . Here, resolution refers to “the smallest interval measurable\\nby a scientiﬁc instrument; the resolving power.” To explain what resolution means via an example,\\nsuppose one wants to measure a coin’s probability of yielding heads. After a single coin ﬂip, only\\ntwo outcomes are possible (H, T), so the resolution-limited probability of heads is either 0or1. After\\ntwo coin ﬂips, four outcomes are possible (HH, HT, TH, TT), so the resolution-limited probability\\nof heads is now one of 0,0.5,1. AfterFcoin ﬂips, we can only resolve the coin’s probability of\\nyielding heads up to 1/F. Consequently, we introduce a resolution-limited notation:\\n⌊a⌉bdef=arounded to the nearest integer multiple of 1/b (3)\\nA.2 Token Edit Distance\\nWe ﬁrst consider an adaptation of the Levenshtein (string edit) distance for models that function\\non tokens rather than characters, an adaptation we term the token edit distance . The token edit\\ndistance between two token sequences tn,ˆtnis deﬁned as the integer number of additions, deletions\\nor substitutions necessary to transform tnintoˆtn(or vice versa).\\nToken Edit Distance (tn,ˆtn)def=Num Substitutions +Num. Additions +Num. Deletions (4)\\n=L∑\\nℓ=1I[tnℓ̸=ˆtnℓ] +Num. Additions +Num. Deletions (5)\\n≥L∑\\nℓ=1I[tnℓ̸=ˆtnℓ] (6)\\nThe expected token edit distance is therefore:\\nE[Token Edit Distance (tn,ˆtn)]≥E[L∑\\nℓ=1I[tnℓ̸=ˆtnℓ]] (7)\\n=L∑\\nℓ=1p(tnℓ̸=ˆtnℓ) (8)\\n≈L(1−ϵ) (9)\\nThe resolution-limited expected token edit distance is therefore:\\n⌊E[Token Edit Distance (tn,ˆtn)]⌉NL≥L(\\n1−⌊ϵ⌉NL)\\n(10)\\nFrom this, we see that the expected token edit distance scales approximately linearly with the\\nresolution-limited per-token probability. The real rate is slightly higher than linear because addi-\\ntions and deletions contribute an additional non-negative cost, but modeling this requires a model\\n12', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='dfe9f768-d863-4eae-b633-610cc40f8f37', embedding=None, metadata={'page_label': '13', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='2cb479bc22ed63e42da05391bf860116c64d2bafdcf1b1e094cf8b9e4114049f', text='of how likely the model is to overproduce or underproduce tokens, which is something we do not\\ncurrently possess.\\nA.3 Accuracy\\nAccuracy (tn,ˆtn)def=I[No additions ]I[No deletions ]L∏\\nl=1I[tnl=ˆtnl] (11)\\n≈L∏\\nl=1I[tnl=ˆtnl] (12)\\nAs with the Token Edit Distance (App. A.3), we ignore how likely the language model is to over-\\nproduce or underproduce tokens because we do not have a good model of this process. Continuing\\nalong,\\nE[logAccuracy ] =∑\\nlE[logI[tnl=ˆtnl]] (13)\\n≤∑\\nllogE[I[tnl=ˆtnl]] (14)\\n≈Llog(1−ϵ) (15)\\nTaking an approximation that would make most mathematicians cry:\\nE[Accuracy ]≈exp(E[logAccuracy ]) (16)\\n= (1−ϵ)L(17)\\n(18)\\nThis reveals that accuracy approximately falls geometrically with target token length. The\\nresolution-limited expected accuracy is therefore:\\n⌊E[Accuracy ]⌉NL=⌊(1−ϵ)L⌉NL (19)\\nFrom this we can see that choosing a nonlinear metric like Accuracy is affected signiﬁcantly more\\nby limited resolution because Accuracy forces one to distinguish quantities that decay rapidly.\\nA.4 ROUGE-L-Sum\\nAnother BIG-Bench metric [28] is ROUGE-L-Sum [23], a metric based on the longest common\\nsubsequence (LCS) between two sequences. Section 3.2 of [23] gives the exact deﬁnition, but the\\nkey property is that ROUGE-L-Sum measures the “union” LCS, which means “stitching” together\\nLCSs across the candidate and multiple references. As explained in the original paper: if the candi-\\ndate sequence is c=w1w2w3w4w5, and if there are two reference sequences r1=w1w2w6w7w8\\nandr2=w1w3w8w9w5, thenLCS (r1,c) =w1w2andLCS (r2,c) =w1w3w5, then the union\\n-LCS ofc,r1,r2isw1w2w3w5, with length 4. Intuitively, this disproportionately beneﬁts models\\nwith smaller error rates because their mistakes can be “stitched” across multiple references; this is\\nconﬁrmed in simulation (Fig. 9).\\nB Inducing Emergent Abilities in Networks on Vision Tasks\\nB.1 Emergent Classiﬁcation of MNIST Handwritten Digits by Convolutional Networks\\nWe begin by inducing an emergent classiﬁcation ability in a LeNet convolutional neural network\\nfamily [22], trained on the MNIST handwritten digits dataset [21]. This family displays smoothly\\n13', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='a56a3de4-f48f-4999-a64d-9a4595648e84', embedding=None, metadata={'page_label': '14', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='a176ab533f454cc4664c1f7d46784baf6685428b05686189e7f6c91e39bc55ab', text='0.0 0.2 0.4 0.6 0.8\\n1 - Per-T oken Error Probability0.00.20.40.60.81.0ROUGE-L-Sum\\n102\\n101\\n100\\n1 - Per-T oken Error Probability102\\n101\\n100ROUGE-L-SumT arget String Length\\n4\\n20\\n36\\nNum. References\\n1\\n4\\n7Figure 9: ROUGE-L-Sum is a sharp metric. Simulations show that as the per-token error proba-\\nbility slightly increase (e.g. from 0.05 to 0.1), the ROUGE-L-Sum metric sharply falls.\\n102110221023\\nGPT-3 Model Parameters0.00.10.20.30.40.50.60.7T est Accuracy\\nPublished Emergent Ability\\nPublished Emergent Ability\\n104105\\nLeNet Model Parameters0.00.20.40.60.81.0T est Accuracy\\nNo Emergent Ability\\nNum. All Correct\\n1\\n104105\\nLeNet Model Parameters0.00.20.40.60.81.0T est Accuracy\\nMetric-Induced Emergent Ability\\nNum. All Correct\\n6\\n8\\n10\\nFigure 10: Induced emergent MNIST classiﬁcation ability in convolutional networks. (A) A\\npublished emergent ability from the BIG-Bench Grounded Mappings task [33]. (B) LeNet trained\\non MNIST [21] displays a predictable, commonplace sigmoidal increase in test accuracy as model\\nparameters increase. (C) When accuracy is redeﬁned as correctly classifying Kout ofKindepen-\\ndent test data, this newly deﬁned metric induces a seemingly unpredictable change.\\nincreasing test accuracy as the number of parameters increase (Fig. 10B). To emulate the accuracy\\nmetric used by emergence papers [8, 33, 28], we use subset accuracy : 1 if the network classiﬁes K\\nout ofK(independent) test data correctly, 0 otherwise. Under this deﬁnition of accuracy, the model\\nfamily displays an “emergent” ability to correctly classify sets of MNIST digits as Kincreases from\\n1to5, especially when combined with sparse sampling of model sizes (Fig. 10C). This convolutional\\nfamily’s emergent classiﬁcation ability qualitatively matches published emergent abilities, e.g., at\\nthe BIG-Bench Grounded Mappings task [33] (Fig. 10A).\\n14', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ec61730a-6822-4d71-b2b7-dafa1515007c', embedding=None, metadata={'page_label': '1', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='072aa497fddfc507f42895a753745793d2697359cda89a9e046fe8da5b203b87', text='Direct Preference Optimization:\\nYour Language Model is Secretly a Reward Model\\nRafael Rafailov∗†Archit Sharma∗†Eric Mitchell∗†\\nStefano Ermon†‡Christopher D. Manning†Chelsea Finn†\\n†Stanford University‡CZ Biohub\\n{rafailov,architsh,eric.mitchell}@cs.stanford.edu\\nAbstract\\nWhile large-scale unsupervised language models (LMs) learn broad world knowl-\\nedge and some reasoning skills, achieving precise control of their behavior is\\ndifficult due to the completely unsupervised nature of their training. Existing\\nmethods for gaining such steerability collect human labels of the relative quality of\\nmodel generations and fine-tune the unsupervised LM to align with these prefer-\\nences, often with reinforcement learning from human feedback (RLHF). However,\\nRLHF is a complex and often unstable procedure, first fitting a reward model that\\nreflects the human preferences, and then fine-tuning the large unsupervised LM\\nusing reinforcement learning to maximize this estimated reward without drifting\\ntoo far from the original model. In this paper we introduce a new parameterization\\nof the reward model in RLHF that enables extraction of the corresponding optimal\\npolicy in closed form, allowing us to solve the standard RLHF problem with only a\\nsimple classification loss. The resulting algorithm, which we call Direct Prefer-\\nence Optimization (DPO), is stable, performant, and computationally lightweight,\\neliminating the need for sampling from the LM during fine-tuning or performing\\nsignificant hyperparameter tuning. Our experiments show that DPO can fine-tune\\nLMs to align with human preferences as well as or better than existing methods.\\nNotably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sen-\\ntiment of generations, and matches or improves response quality in summarization\\nand single-turn dialogue while being substantially simpler to implement and train.\\n1 Introduction\\nLarge unsupervised language models (LMs) trained on very large datasets acquire surprising capabili-\\nties [ 11,7,40,8]. However, these models are trained on data generated by humans with a wide variety\\nof goals, priorities, and skillsets. Some of these goals and skillsets may not be desirable to imitate; for\\nexample, while we may want our AI coding assistant to understand common programming mistakes\\nin order to correct them, nevertheless, when generating code, we would like to bias our model toward\\nthe (potentially rare) high-quality coding ability present in its training data. Similarly, we might want\\nour language model to be aware of a common misconception believed by 50% of people, but we\\ncertainly do not want the model to claim this misconception to be true in 50% of queries about it!\\nIn other words, selecting the model’s desired responses and behavior from its very wide knowledge\\nand abilities is crucial to building AI systems that are safe, performant, and controllable [ 26]. While\\nexisting methods typically steer LMs to match human preferences using reinforcement learning (RL),\\n∗Equal contribution; more junior authors listed earlier.\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2305.18290v2  [cs.LG]  13 Dec 2023', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e3ce65b2-d903-4c06-873e-5f4655d2a140', embedding=None, metadata={'page_label': '2', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='aa72f069550e6356bd9bb4f6d5cf2c18882c6e3895fef9dd2005343a13dfb206', text='Figure 1: DPO optimizes for human preferences while avoiding reinforcement learning. Existing methods\\nfor fine-tuning language models with human feedback first fit a reward model to a dataset of prompts and\\nhuman preferences over pairs of responses, and then use RL to find a policy that maximizes the learned reward.\\nIn contrast, DPO directly optimizes for the policy best satisfying the preferences with a simple classification\\nobjective, fitting an implicit reward model whose corresponding optimal policy can be extracted in closed form.\\nwe will show that the RL-based objective used by existing methods can be optimized exactly with a\\nsimple binary cross-entropy objective, greatly simplifying the preference learning pipeline.\\nAt a high level, existing methods instill the desired behaviors into a language model using curated\\nsets of human preferences representing the types of behaviors that humans find safe and helpful. This\\npreference learning stage occurs after an initial stage of large-scale unsupervised pre-training on\\na large text dataset. While the most straightforward approach to preference learning is supervised\\nfine-tuning on human demonstrations of high quality responses, the most successful class of methods\\nis reinforcement learning from human (or AI) feedback (RLHF/RLAIF; [ 12,2]). RLHF methods fit\\na reward model to a dataset of human preferences and then use RL to optimize a language model\\npolicy to produce responses assigned high reward without drifting excessively far from the original\\nmodel. While RLHF produces models with impressive conversational and coding abilities, the RLHF\\npipeline is considerably more complex than supervised learning, involving training multiple LMs and\\nsampling from the LM policy in the loop of training, incurring significant computational costs.\\nIn this paper, we show how to directly optimize a language model to adhere to human preferences,\\nwithout explicit reward modeling or reinforcement learning. We propose Direct Preference Optimiza-\\ntion (DPO) , an algorithm that implicitly optimizes the same objective as existing RLHF algorithms\\n(reward maximization with a KL-divergence constraint) but is simple to implement and straight-\\nforward to train. Intuitively, the DPO update increases the relative log probability of preferred to\\ndispreferred responses, but it incorporates a dynamic, per-example importance weight that prevents\\nthe model degeneration that we find occurs with a naive probability ratio objective. Like existing\\nalgorithms, DPO relies on a theoretical preference model (such as the Bradley-Terry model; [ 5]) that\\nmeasures how well a given reward function aligns with empirical preference data. However, while\\nexisting methods use the preference model to define a preference loss to train a reward model and\\nthen train a policy that optimizes the learned reward model, DPO uses a change of variables to define\\nthe preference loss as a function of the policy directly. Given a dataset of human preferences over\\nmodel responses, DPO can therefore optimize a policy using a simple binary cross entropy objective,\\nproducing the optimal policy to an implicit reward function fit to the preference data.\\nOur main contribution is Direct Preference Optimization (DPO), a simple RL-free algorithm for\\ntraining language models from preferences. Our experiments show that DPO is at least as effective\\nas existing methods, including PPO-based RLHF, for learning from preferences in tasks such as\\nsentiment modulation, summarization, and dialogue, using language models with up to 6B parameters.\\n2 Related Work\\nSelf-supervised language models of increasing scale learn to complete some tasks zero-shot [ 31] or\\nwith few-shot prompts [ 6,25,11]. However, their performance on downstream tasks and alignment\\nwith user intent can be significantly improved by fine-tuning on datasets of instructions and human-\\nwritten completions [ 23,36,13,39]. This ‘instruction-tuning’ procedure enables LLMs to generalize\\nto instructions outside of the instruction-tuning set and generally increase their usability [ 13]. Despite\\nthe success of instruction tuning, relative human judgments of response quality are often easier to\\ncollect than expert demonstrations, and thus subsequent works have fine-tuned LLMs with datasets of\\nhuman preferences, improving proficiency in translation [ 18], summarization [ 38,49], story-telling\\n[49], and instruction-following [ 26,32]. These methods first optimize a neural network reward\\nfunction for compatibility with the dataset of preferences under a preference model such as the\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='131d2435-9cd2-4b89-91f3-bec796e569bd', embedding=None, metadata={'page_label': '3', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='c044fc76fe49b4eca4bfe6ebe6dfcb5da0155f5d4711a2c0aabfe0bd53e80802', text='Bradley-Terry model [ 5], then fine-tune a language model to maximize the given reward using\\nreinforcement learning algorithms, commonly REINFORCE [ 45], proximal policy optimization\\n(PPO; [ 37]), or variants [ 32]. A closely-related line of work leverages LLMs fine-tuned for instruction\\nfollowing with human feedback to generate additional synthetic preference data for targeted attributes\\nsuch as safety or harmlessness [ 2], using only weak supervision from humans in the form of a\\ntext rubric for the LLM’s annotations. These methods represent a convergence of two bodies of\\nwork: one body of work on training language models with reinforcement learning for a variety\\nof objectives [ 33,27,46] and another body of work on general methods for learning from human\\npreferences [ 12,19]. Despite the appeal of using relative human preferences, fine-tuning large\\nlanguage models with reinforcement learning remains a major practical challenge; this work provides\\na theoretically-justified approach to optimizing relative preferences without RL.\\nOutside of the context of language, learning policies from preferences has been studied in both bandit\\nand reinforcement learning settings, and several approaches have been proposed. Contextual bandit\\nlearning using preferences or rankings of actions, rather than rewards, is known as a contextual\\ndueling bandit (CDB; [ 48,14]). In the absence of absolute rewards, theoretical analysis of CDBs\\nsubstitutes the notion of an optimal policy with a von Neumann winner , a policy whose expected win\\nrate against anyother policy is at least 50% [ 14]. However, in the CDB setting, preference labels\\nare given online, while in learning from human preferences, we typically learn from a fixed batch of\\noffline preference-annotated action pairs [ 47]. Similarly, preference-based RL (PbRL) learns from\\nbinary preferences generated by an unknown ‘scoring’ function rather than rewards [9, 35]. Various\\nalgorithms for PbRL exist, including methods that can reuse off-policy preference data, but generally\\ninvolve first explicitly estimating the latent scoring function (i.e. the reward model) and subsequently\\noptimizing it [ 16,9,12,34,19]. We instead present a single stage policy learning approach that\\ndirectly optimizes a policy to satisfy preferences.\\n3 Preliminaries\\nWe review the RLHF pipeline in Ziegler et al. (and later [ 38,1,26]). It usually includes three phases:\\n1) supervised fine-tuning (SFT); 2) preference sampling and reward learning and 3) RL optimization.\\nSFT: RLHF typically begins by fine-tuning a pre-trained LM with supervised learning on high-quality\\ndata for the downstream task(s) of interest (dialogue, summarization, etc.), to obtain a model πSFT.\\nReward Modelling Phase : In the second phase the SFT model is prompted with prompts xto\\nproduce pairs of answers (y1, y2)∼πSFT(y|x). These are then presented to human labelers\\nwho express preferences for one answer, denoted as yw≻yl|xwhere ywandyldenotes the\\npreferred and dispreferred completion amongst (y1, y2)respectively. The preferences are assumed\\nto be generated by some latent reward model r∗(y, x), which we do not have access to. There are a\\nnumber of approaches used to model preferences, the Bradley-Terry (BT) [ 5] model being a popular\\nchoice (although more general Plackett-Luce ranking models [ 30,21] are also compatible with the\\nframework if we have access to several ranked answers). The BT model stipulates that the human\\npreference distribution p∗can be written as:\\np∗(y1≻y2|x) =exp (r∗(x, y 1))\\nexp (r∗(x, y 1)) + exp ( r∗(x, y 2)). (1)\\nAssuming access to a static dataset of comparisons D=\\x08\\nx(i), y(i)\\nw, y(i)\\nl\\tN\\ni=1sampled from p∗, we\\ncan parametrize a reward model rϕ(x, y)and estimate the parameters via maximum likelihood.\\nFraming the problem as a binary classification we have the negative log-likelihood loss:\\nLR(rϕ,D) =−E(x,yw,yl)∼D\\x02\\nlogσ(rϕ(x, yw)−rϕ(x, yl))\\x03\\n(2)\\nwhere σis the logistic function. In the context of LMs, the network rϕ(x, y)is often initialized from\\nthe SFT model πSFT(y|x)with the addition of a linear layer on top of the final transformer layer\\nthat produces a single scalar prediction for the reward value [ 49]. To ensure a reward function with\\nlower variance, prior works normalize the rewards, such that Ex,y∼D[rϕ(x, y)] = 0 for all x.\\nRL Fine-Tuning Phase : During the RL phase, we use the learned reward function to provide\\nfeedback to the language model. In particular, we formulate the following optimization problem\\nmax\\nπθEx∼D,y∼πθ(y|x)\\x02\\nrϕ(x, y)\\x03\\n−βDKL\\x02\\nπθ(y|x)||πref(y|x)\\x03\\n(3)\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9d923a72-85d7-4617-8726-289689a74f04', embedding=None, metadata={'page_label': '4', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='7658c4f7042ded638c9c4fe7e1cb477023e09ba66b7396595deb5db84ce8dae7', text='where βis a parameter controlling the deviation from the base reference policy πref, namely the ini-\\ntial SFT model πSFT. In practice, the language model policy πθis also initialized to πSFT. The\\nadded constraint is important, as it prevents the model from deviating too far from the distri-\\nbution on which the reward model is accurate, as well as maintaining the generation diversity\\nand preventing mode-collapse to single high-reward answers. Due to the discrete nature of lan-\\nguage generation, this objective is not differentiable and is typically optimized with reinforce-\\nment learning. The standard approach [ 49,38,1,26] has been to construct the reward function\\nr(x, y) =rϕ(x, y)−β(logπθ(y|x)−logπref(y|x)), and maximize using PPO [37].\\n4 Direct Preference Optimization\\nMotivated by the challenges of applying reinforcement learning algorithms on large-scale problems\\nsuch as fine-tuning language models, our goal is to derive a simple approach for policy optimization\\nusing preferences directly. Unlike prior RLHF methods, which learn a reward and then optimize it\\nvia RL, our approach leverages a particular choice of reward model parameterization that enables\\nextraction of its optimal policy in closed form, without an RL training loop. As we will describe\\nnext in detail, our key insight is to leverage an analytical mapping from reward functions to optimal\\npolicies, which enables us to transform a loss function over reward functions into a loss function\\nover policies. This change-of-variables approach avoids fitting an explicit, standalone reward model,\\nwhile still optimizing under existing models of human preferences, such as the Bradley-Terry model.\\nIn essence, the policy network represents both the language model and the (implicit) reward.\\nDeriving the DPO objective. We start with the same RL objective as prior work, Eq. 3, under a\\ngeneral reward function r. Following prior work [ 29,28,17,15], it is straightforward to show that\\nthe optimal solution to the KL-constrained reward maximization objective in Eq. 3 takes the form:\\nπr(y|x) =1\\nZ(x)πref(y|x) exp\\x121\\nβr(x, y)\\x13\\n, (4)\\nwhere Z(x) =P\\nyπref(y|x) exp\\x10\\n1\\nβr(x, y)\\x11\\nis the partition function. See Appendix A.1 for a\\ncomplete derivation. Even if we use the MLE estimate rϕof the ground-truth reward function r∗, it is\\nstill expensive to estimate the partition function Z(x)[17,15], which makes this representation hard\\nto utilize in practice. However, we can rearrange Eq. 4 to express the reward function in terms of its\\ncorresponding optimal policy πr, the reference policy πref, and the unknown partition function Z(·).\\nSpecifically, we first take the logarithm of both sides of Eq. 4 and then with some algebra we obtain:\\nr(x, y) =βlogπr(y|x)\\nπref(y|x)+βlogZ(x). (5)\\nWe can apply this reparameterization to the ground-truth reward r∗and corresponding optimal model\\nπ∗. Fortunately, the Bradley-Terry model depends only on the difference of rewards between two\\ncompletions, i.e., p∗(y1≻y2|x) =σ(r∗(x, y 1)−r∗(x, y 2)). Substituting the reparameterization\\nin Eq. 5 for r∗(x, y)into the preference model Eq. 1, the partition function cancels, and we can\\nexpress the human preference probability in terms of only the optimal policy π∗and reference policy\\nπref. Thus, the optimal RLHF policy π∗under the Bradley-Terry model satisfies the preference model:\\np∗(y1≻y2|x) =1\\n1 + exp\\x10\\nβlogπ∗(y2|x)\\nπref(y2|x)−βlogπ∗(y1|x)\\nπref(y1|x)\\x11 (6)\\nThe derivation is in Appendix A.2. While Eq. 6 uses the Bradley-Terry model, we can similarly\\nderive expressions under the more general Plackett-Luce models [30, 21], shown in Appendix A.3.\\nNow that we have the probability of human preference data in terms of the optimal policy rather than\\nthe reward model, we can formulate a maximum likelihood objective for a parametrized policy πθ.\\nAnalogous to the reward modeling approach (i.e. Eq. 2), our policy objective becomes:\\nLDPO(πθ;πref) =−E(x,yw,yl)∼D\\x14\\nlogσ\\x12\\nβlogπθ(yw|x)\\nπref(yw|x)−βlogπθ(yl|x)\\nπref(yl|x)\\x13\\x15\\n. (7)\\nThis way, we fit an implicit reward using an alternative parameterization, whose optimal policy is\\nsimply πθ. Moreover, since our procedure is equivalent to fitting a reparametrized Bradley-Terry\\n4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c51e23a7-09e5-439f-bb05-7475c5721955', embedding=None, metadata={'page_label': '5', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='bbbebe8518008ad83d0a163290e42ffa41ce78ec1bc949c97efe39e558b358ca', text='model, it enjoys certain theoretical properties, such as consistencies under suitable assumption of the\\npreference data distribution [ 4]. In Section 5, we further discuss theoretical properties of DPO in\\nrelation to other works.\\nWhat does the DPO update do? For a mechanistic understanding of DPO, it is useful to analyze the\\ngradient of the loss function LDPO. The gradient with respect to the parameters θcan be written as:\\n∇θLDPO(πθ;πref) =\\n−βE(x,yw,yl)∼D\\x14\\nσ(ˆrθ(x, yl)−ˆrθ(x, yw))| {z }\\nhigher weight when reward estimate is wrong\\x14\\n∇θlogπ(yw|x)| {z }\\nincrease likelihood of yw− ∇ θlogπ(yl|x)|{z }\\ndecrease likelihood of yl\\x15\\x15\\n,\\nwhere ˆrθ(x, y) =βlogπθ(y|x)\\nπref(y|x)is the reward implicitly defined by the language model πθand refer-\\nence model πref(more in Section 5). Intuitively, the gradient of the loss function LDPOincreases the\\nlikelihood of the preferred completions ywand decreases the likelihood of dispreferred completions\\nyl. Importantly, the examples are weighed by how much higher the implicit reward model ˆrθrates\\nthe dispreferred completions, scaled by β, i.e, how incorrectly the implicit reward model orders\\nthe completions, accounting for the strength of the KL constraint. Our experiments suggest the\\nimportance of this weighting, as a naïve version of this method without the weighting coefficient can\\ncause the language model to degenerate (Appendix Table 3).\\nDPO outline. The general DPO pipeline is as follows: 1) Sample completions y1, y2∼πref(· |x)\\nfor every prompt x, label with human preferences to construct the offline dataset of preferences\\nD={x(i), y(i)\\nw, yl)(i)}N\\ni=1and 2) optimize the language model πθto minimize LDPOfor the given\\nπrefandDand desired β. In practice, one would like to reuse preference datasets publicly available,\\nrather than generating samples and gathering human preferences. Since the preference datasets\\nare sampled using πSFT, we initialize πref=πSFTwhenever available. However, when πSFTis\\nnot available, we initialize πrefby maximizing likelihood of preferred completions (x, yw), that\\nis,πref= arg maxπEx,yw∼D[logπ(yw|x)]. This procedure helps mitigate the distribution shift\\nbetween the true reference distribution which is unavailable, and πrefused by DPO. Further details\\nrelated to the implementation and hyperparameters can be found in Appendix B.\\n5 Theoretical Analysis of DPO\\nIn this section, we give further interpretation of the DPO method, provide theoretical backing, and\\nrelate advantages of DPO to issues with actor critic algorithms used for RLHF (such as PPO [37]).\\n5.1 Your Language Model Is Secretly a Reward Model\\nDPO is able to bypass both fitting an explicit reward and performing RL to learn the policy using\\na single maximum likelihood objective. Note the optimization objective Eq. 5 is equivalent to a\\nBradley-Terry model with a reward parameterization r∗(x, y) =βlogπ∗\\nθ(y|x)\\nπref(y|x)and we optimize our\\nparametric model πθ, equivalently to the reward model optimization in Eq. 2 under the change of\\nvariables. In this section we will build the theory behind this reparameterization, show that it does\\nnot constrain the class of learned reward models, and allows for the exact recovery of the optimal\\npolicy. We begin with by defining an equivalence relation between reward functions.\\nDefinition 1. We say that two reward functions r(x, y)and r′(x, y)are equivalent iff\\nr(x, y)−r′(x, y) =f(x)for some function f.\\nIt is easy to see that this is indeed an equivalence relation, which partitions the set of reward functions\\ninto classes. We can state the following two lemmas:\\nLemma 1. Under the Plackett-Luce, and in particular the Bradley-Terry, preference framework, two\\nreward functions from the same class induce the same preference distribution.\\nLemma 2. Two reward functions from the same equivalence class induce the same optimal policy\\nunder the constrained RL problem.\\nThe proofs are straightforward and we defer them to Appendix A.5. The first lemma is a well-known\\nunder-specification issue with the Plackett-Luce family of models [ 30]. Due to this under-specification,\\n5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='41d9ca65-fdd8-48f5-af3d-6fd8b7d013bd', embedding=None, metadata={'page_label': '6', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='b7363fca61113d1fdfc6a3e83ff1b3c023d83b165cc7c6f1d21e6d73cb0945c7', text='we usually have to impose additional identifiability constraints to achieve any guarantees on the MLE\\nestimates from Eq. 2 [ 4]. The second lemma states that all reward functions from the same class\\nyield the same optimal policy, hence for our final objective, we are only interested in recovering an\\narbitrary reward function from the optimal class. We prove the following Theorem in Appendix A.6:\\nTheorem 1. Under mild assumptions, all reward classes consistent with the Plackett-Luce\\n(and Bradley-Terry in particular) models can be represented with the reparameterization\\nr(x, y) =βlogπ(y|x)\\nπref(y|x)for some model π(y|x)and a given reference model πref(y|x).\\nProof Sketch. Consider any reward function r(x, y), which induces a corresponding optimal model\\nπr(y|x), specified by Eq. 4. We will show that a reward function from the equivalence class of r\\ncan be represented using the reparameterization given above. We define the projection fas\\nf(r;πref, β)(x, y) =r(x, y)−βlogX\\nyπref(y|x) exp\\x121\\nβr(x, y)\\x13\\n(8)\\nThe operator fsimply normalizes the reward function with the logarithm of the partition function\\nofπr. Since the added normalization term is only a function of the prefix x,f(r;πref, β)(x, y)is a\\nreward function in the equivalence class of r(x, y). Finally, replacing rwith the RHS of Eq. 5 (which\\nholds for any reward function), we have f(r;πref, β)(x, y) =βlogπr(y|x)\\nπref(y|x). That is, the projection\\nfproduces a member of the equivalence class of rwith the desired form, and we do not lose any\\ngenerality in our reward model from the proposed reparameterization.\\nWe can alternatively view Theorem 1 as specifying exactly which reward function within each\\nequivalence class the DPO reparameterization selects, that is, the reward function satisfying:\\nX\\nyπref(y|x) exp\\x121\\nβr(x, y)\\x13\\n| {z }\\n=π(y|x), using Thm. 1 reparam.= 1, (9)\\ni.e.,π(y|x)is a valid distribution (probabilities are positive and sum to 1). However, following\\nEq. 4, we can see that Eq. 9 is the partition function of the optimal policy induced by the reward\\nfunction r(x, y). The key insight of the DPO algorithm is that we can impose certain constraints on\\nthe under-constrained Plackett-Luce (and Bradley-Terry in particular) family of preference models,\\nsuch that we preserve the class of representable reward models, but explicitly make the optimal policy\\nin Eq. 4 analytically tractable for all prompts x.\\n5.2 Instability of Actor-Critic Algorithms\\nWe can also use our framework to diagnose instabilities with standard actor-critic algorithms used\\nfor the RLHF, such as PPO. We follow the RLHF pipeline and focus on the RL fine-tuning step\\noutlined in Section 3. We can draw connections to the control as inference framework [ 20] for the\\nconstrained RL problem outlined in 3. We assume a parameterized model πθ(y|x)and minimize\\nDKL[πθ(y|x)||π∗(y|x)]where π∗is the optimal policy from Eq. 7 induced by the reward function\\nrϕ(y, x). With some algebra this leads to the optimization objective:\\nmax\\nπθEπθ(y|x)\\x14\\nrϕ(x, y)−βlogX\\nyπref(y|x) exp\\x121\\nβrϕ(x, y)\\x13\\n| {z }\\nf(rϕ,πref,β)−βlogπθ(y|x)\\nπref(y|x)|{z }\\nKL\\x15\\n(10)\\nThis is the same objective optimized in prior works [ 49,38,1,26] using the DPO-equivalent reward\\nfor the reward class of rϕ. In this setting, we can interpret the normalization term in f(rϕ, πref, β)\\nas the soft value function of the reference policy πref. While this term does not affect the optimal\\nsolution, without it, the policy gradient of the objective could have high variance, making learning\\nunstable. We can accommodate for the normalization term using a learned value function, but that\\ncan also be difficult to optimize. Alternatively, prior works have normalized rewards using a human\\ncompletion baseline, essentially a single sample Monte-Carlo estimate of the normalizing term. In\\ncontrast the DPO reparameterization yields a reward function that does not require any baselines.\\n6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='b1a85119-85d8-469b-bfd1-5cc0562ddc63', embedding=None, metadata={'page_label': '7', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='12a130c3de29b1c6f4e35ab3aad290604d163c10b43407ef8f00b8b657f7daf3', text='0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0\\nKL( ref)\\n0.40.50.60.70.80.91.0RewardIMDb Sentiment Generation\\nDPO (Ours)\\nUnlikelihood\\nPPO (Our impl.)PPO-GT (Our impl.)\\nPPO-GT (TRL)\\nPreferred-FT\\n0.00 0.25 0.50 0.75 1.00\\nSampling temperature0.00.10.20.30.40.50.60.7Win rateTL;DR Summarization Win Rate vs Reference\\nDPO\\nPPOPreferred-FT\\nSFTGPT-J\\nBest of 128Figure 2: Left. The frontier of expected reward vs KL to the reference policy. DPO provides the highest expected\\nreward for all KL values, demonstrating the quality of the optimization. Right. TL;DR summarization win\\nrates vs. human-written summaries, using GPT-4 as evaluator. DPO exceeds PPO’s best-case performance on\\nsummarization, while being more robust to changes in the sampling temperature.\\n6 Experiments\\nIn this section, we empirically evaluate DPO’s ability to train policies directly from preferences. First,\\nin a well-controlled text-generation setting, we ask: how efficiently does DPO trade off maximizing\\nreward and minimizing KL-divergence with the reference policy, compared to common preference\\nlearning algorithms such as PPO? Next, we evaluate DPO’s performance on larger models and more\\ndifficult RLHF tasks, including summarization and dialogue. We find that with almost no tuning\\nof hyperparameters, DPO tends to perform as well or better than strong baselines like RLHF with\\nPPO as well as returning the best of Nsampled trajectories under a learned reward function. Before\\npresenting these results, we describe the experimental set-up; additional details are in Appendix C.\\nTasks. Our experiments explore three different open-ended text generation tasks. For all experiments,\\nalgorithms learn a policy from a dataset of preferences D=\\x08\\nx(i), y(i)\\nw, y(i)\\nl\\tN\\ni=1. In controlled\\nsentiment generation ,xis a prefix of a movie review from the IMDb dataset [ 22], and the policy\\nmust generate ywith positive sentiment. In order to perform a controlled evaluation, for this\\nexperiment we generate preference pairs over generations using a pre-trained sentiment classifier,\\nwhere p(positive |x, yw)> p(positive |x, yl). For SFT, we fine-tune GPT-2-large until convergence\\non reviews from the train split of the IMDB dataset (further details in App C.1). In summarization ,\\nxis a forum post from Reddit; the policy must generate a summary yof the main points in the\\npost. Following prior work, we use the Reddit TL;DR summarization dataset [ 41] along with human\\npreferences gathered by Stiennon et al.. We use an SFT model fine-tuned on human-written forum\\npost summaries2with the TRLX [ 42] framework for RLHF. The human preference dataset was\\ngathered by Stiennon et al. on samples from a different, but similarly-trained, SFT model. Finally, in\\nsingle-turn dialogue ,xis a human query, which may be anything from a question about astrophysics\\nto a request for relationship advice. A policy must produce an engaging and helpful response yto\\na user’s query; we use the Anthropic Helpful and Harmless dialogue dataset [ 1], containing 170k\\ndialogues between a human and an automated assistant. Each transcript ends with a pair of responses\\ngenerated by a large (although unknown) language model along with a preference label denoting\\nthe human-preferred response. In this setting, no pre-trained SFT model is available; we therefore\\nfine-tune an off-the-shelf language model on only the preferred completions to form the SFT model.\\nEvaluation. Our experiments use two different approaches to evaluation. In order to analyze the\\neffectiveness of each algorithm in optimizing the constrained reward maximization objective, in the\\ncontrolled sentiment generation setting we evaluate each algorithm by its frontier of achieved reward\\nand KL-divergence from the reference policy; this frontier is computable because we have acccess to\\nthe ground-truth reward function (a sentiment classifier). However, in the real world, the ground truth\\nreward function is not known; therefore, we evaluate algorithms with their win rate against a baseline\\npolicy, using GPT-4 as a proxy for human evaluation of summary quality and response helpfulness\\nin the summarization and single-turn dialogue settings, respectively. For summarization, we use\\nreference summaries in the test set as the baseline; for dialogue, we use the preferred response in the\\n2https://huggingface.co/CarperAI/openai_summarize_tldr_sft\\n7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='4b0bfe36-15f3-48db-b5c2-85432b067bc2', embedding=None, metadata={'page_label': '8', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='3b297ac4913049695603cbcdd76547b4db3b74bd3f1037a72ab3e9c0df21c546', text='0.25 0.50 0.75 1.00\\nSampling temperature0.10.20.30.40.50.6Win rateAnthropic-HH Dialogue Win Rate vs Chosen\\nDPO\\nBest of 128Preferred-FT\\nPythia-2.8B\\n0 300 600 900 1200 1500 1800 2100 2400 2700 3000 3300\\nFine-tuning step0.300.350.400.450.500.550.600.650.70Win rate\\nDialogue Win Rate Evolution\\nDPO (temp = 1.0) DPO (temp = 0.7)Figure 3: Left. Win rates computed by GPT-4 for Anthropic-HH one-step dialogue; DPO is the only method\\nthat improves over chosen summaries in the Anthropic-HH test set. Right. Win rates for different sampling\\ntemperatures over the course of training. DPO’s improvement over the dataset labels is fairly stable over the\\ncourse of training for different sampling temperatures.\\ntest dataset as the baseline. While existing studies suggest LMs can be better automated evaluators\\nthan existing metrics [ 10], we conduct a human study to justify our usage of GPT-4 for evaluation\\nin Sec. 6.4. We find GPT-4 judgments correlate strongly with humans, with human agreement with\\nGPT-4 typically similar or higher than inter-human annotator agreement.\\nMethods. In addition to DPO, we evaluate several existing approaches to training language models\\nto adhere to human preferences. Most simply, we explore zero-shot prompting with GPT-J [43] in\\nthe summarization task and 2-shot prompting with Pythia-2.8B [3] in the dialogue task. In addition,\\nwe evaluate the SFT model as well as Preferred-FT , which is a model fine-tuned with supervised\\nlearning on the chosen completion ywfrom either the SFT model (in controlled sentiment and\\nsummarization) or a generic LM (in single-turn dialogue). Another pseudo-supervised method is\\nUnlikelihood [44], which simply optimizes the policy to maximize the probability assigned to ywand\\nminimize the probability assigned to yl; we use an optional coefficient α∈[0,1]on the ‘unlikelihood’\\nterm. We also consider PPO [37] using a reward function learned from the preference data and\\nPPO-GT , which is an oracle that learns from the ground truth reward function available in the\\ncontrolled sentiment setting. In our sentiment experiments, we use two implementations of PPO-GT,\\none of-the-shelf version [ 42] as well as a modified version that normalizes rewards and further tunes\\nhyperparameters to improve performance (we also use these modifications when running ‘normal’\\nPPO with learned rewards). Finally, we consider the Best of Nbaseline, sampling Nresponses from\\nthe SFT model (or Preferred-FT in dialogue) and returning the highest-scoring response according to\\na reward function learned from the preference dataset. This high-performing method decouples the\\nquality of the reward model from the PPO optimization, but is computationally impractical even for\\nmoderate Nas it requires sampling Ncompletions for every query at test time.\\n6.1 How well can DPO optimize the RLHF objective?\\nThe KL-constrained reward maximization objective used in typical RLHF algorithms balances\\nexploitation of reward while restricting the policy from deviating far from the reference policy.\\nTherefore, when comparing algorithms, we must take into account both reward achieved as well as the\\nKL discrepancy; achieving slightly higher reward but with much higher KL is not necessarily desirable.\\nFigure 2 shows the reward-KL frontier for various algorithms in the sentiment setting. We execute\\nmultiple training runs for each algorithm, using a different hyperparameter for policy conservativeness\\nin each run (target KL ∈ {3,6,9,12}for PPO, β∈ {0.05,0.1,1,5},α∈ {0.05,0.1,0.5,1}for\\nunlikelihood, random seeds for preferred-FT). This sweep includes 22 runs in total. After each\\n100 training steps until convergence, we evaluate each policy on a set of test prompts, computing\\nthe average reward under the true reward function as well as the average sequence-level KL3with\\nthe reference policy KL(π||πref). We find that DPO produces by far the most efficient frontier,\\nachieving the highest reward while still achieving low KL. This result is particularly notable for\\nmultiple reasons. First, DPO and PPO optimize the same objective, but DPO is notably more efficient;\\n3That is, the sum of the per-timestep KL-divergences.\\n8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f7fb77eb-0806-4f36-a69c-e2244bded133', embedding=None, metadata={'page_label': '9', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='dd629d641c206850508c62d7a22200c7f31967f7c0ba3bbcfde7bd552fbdd1eb', text='DPO’s reward/KL tradeoff strictly dominates PPO. Second, DPO achieves a better frontier than PPO,\\neven when PPO can access ground truth rewards (PPO-GT).\\n6.2 Can DPO scale to real preference datasets?\\nNext, we evaluate fine-tuning performance of DPO on summarization and single-turn dialogue. For\\nsummarization, automatic evaluation metrics such as ROUGE can be poorly correlated with human\\npreferences [ 38], and prior work has found that fine-tuning LMs using PPO on human preferences\\nto provide more effective summaries. We evaluate different methods by sampling completions on\\nthe test split of TL;DR summarization dataset, and computing the average win rate against reference\\ncompletions in the test set. The completions for all methods are sampled at temperatures varying\\nfrom 0.0 to 1.0, and the win rates are shown in Figure 2 (right). DPO, PPO and Preferred-FT all\\nfine-tune the same GPT-J SFT model4. We find that DPO has a win rate of approximately 61% at a\\ntemperature of 0.0, exceeding the performance of PPO at 57% at its optimal sampling temperature\\nof 0.0. DPO also achieves a higher maximum win rate compared to the best of Nbaseline. We\\nnote that we did not meaningfully tune DPO’s βhyperparameter, so these results may underestimate\\nDPO’s potential. Moreover, we find DPO to be much more robust to the sampling temperature than\\nPPO, the performance of which can degrade to that of the base GPT-J model at high temperatures.\\nPreferred-FT does not improve significantly over the SFT model. We also compare DPO and PPO\\nhead-to-head in human evaluations in Section 6.4, where DPO samples at temperature 0.25 were\\npreferred 58% times over PPO samples at temperature 0.\\nOn single-turn dialogue, we evaluate the different methods on the subset of the test split of the\\nAnthropic HH dataset [ 1] with one step of human-assistant interaction. GPT-4 evaluations use the\\npreferred completions on the test as the reference to compute the win rate for different methods. As\\nthere is no standard SFT model for this task, we start with a pre-trained Pythia-2.8B, use Preferred-FT\\nto train a reference model on the chosen completions such that completions are within distribution\\nof the model, and then train using DPO. We also compare against the best of 128 Preferred-FT\\ncompletions (we found the Best of Nbaseline plateaus at 128 completions for this task; see Appendix\\nFigure 4) and a 2-shot prompted version of the Pythia-2.8B base model, finding DPO performs as\\nwell or better for the best-performing temperatures for each method. We also evaluate an RLHF\\nmodel trained with PPO on the Anthropic HH dataset5from a well-known source6, but are unable\\nto find a prompt or sampling temperature that gives performance better than the base Pythia-2.8B\\nmodel. Based on our results from TL;DR and the fact that both methods optimize the same reward\\nfunction, we consider Best of 128 a rough proxy for PPO-level performance. Overall, DPO is the\\nonly computationally efficient method that improves over the preferred completions in the Anthropic\\nHH dataset, and provides similar or better performance to the computationally demanding Best of\\n128 baseline. Finally, Figure 3 shows that DPO converges to its best performance relatively quickly.\\n6.3 Generalization to a new input distribution\\nWin rate vs. ground truth\\nAlg. Temp 0 Temp 0.25\\nDPO 0.36 0.31\\nPPO 0.26 0.23\\nTable 1: GPT-4 win rates vs. ground\\ntruth summaries for out-of-distribution\\nCNN/DailyMail input articles.To further compare the performance of PPO and DPO un-\\nder distribution shifts, we evaluate the PPO and DPO poli-\\ncies from our Reddit TL;DR summarization experiment on\\na different distribution, news articles in the test split of the\\nCNN/DailyMail dataset [ 24], using the best sampling temper-\\natures from TL;DR (0 and 0.25). The results are presented\\nin Table 1. We computed the GPT-4 win rate against the\\nground-truth summaries in the datasets, using the same GPT-\\n4 (C) prompt we used for Reddit TL;DR, but replacing the words “forum post” with “news article”.\\nFor this new distribution, DPO continues to outperform the PPO policy by a significant margin. This\\nexperiment provides initial evidence that DPO policies can generalize similarly well to PPO policies,\\neven though DPO does not use the additional unlabeled Reddit TL;DR prompts that PPO uses.\\n4https://huggingface.co/CarperAI/openai_summarize_tldr_sft\\n5https://huggingface.co/reciprocate/ppo_hh_pythia-6B\\n6https://github.com/CarperAI/trlx/tree/main/examples/hh\\n9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='dd941790-6150-4ee0-92af-4dc1c53ca569', embedding=None, metadata={'page_label': '10', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='29d3135ca00dbb9ad87359b230d621314e228f8a11691afb8abdf5444f03d5a3', text='6.4 Validating GPT-4 judgments with human judgments\\nWe conduct a human study to verify the reliability of GPT-4’s judgments, using the results of\\nthe TL;DR summarization experiment and two different GPT-4 prompts. The GPT-4 (S) (sim-\\nple) prompt simply asks for which summary better-summarizes the important information in the\\npost. The GPT-4 (C) (concise) prompt also asks for which summary is more concise; we eval-\\nuate this prompt because we find that GPT-4 prefers longer, more repetitive summaries than hu-\\nmans do with the GPT-4 (S) prompt. See Appendix C.2 for the complete prompts. We perform\\nthree comparisons, using the highest (DPO, temp. 0.25), the lowest (PPO, temp. 1.0), and a\\nDPO SFT PPO-1\\nN respondents 272 122 199\\nGPT-4 (S) win % 47 27 13\\nGPT-4 (C) win % 54 32 12\\nHuman win % 58 43 17\\nGPT-4 (S)-H agree 70 77 86\\nGPT-4 (C)-H agree 67 79 85\\nH-H agree 65 - 87\\nTable 2: Comparing human and GPT-4 win rates\\nand per-judgment agreement on TL;DR summariza-\\ntion samples. Humans agree with GPT-4 about as\\nmuch as they agree with each other. Each experi-\\nment compares a summary from the stated method\\nwith a summary from PPO with temperature 0.middle-performing (SFT, temp. 0.25) method with\\nthe aim of covering a diversity of sample qualities;\\nall three methods are compared against greedily-\\nsampled PPO (its best-performing temperature).\\nWe find that with both prompts, GPT-4 tends to\\nagree with humans about as often as humans agree\\nwith each other, suggesting that GPT-4 is a reason-\\nable proxy for human evaluations (due to limited\\nhuman raters, we only collect multiple human judg-\\nments for the DPO and PPO-1 comparisons). Over-\\nall, the GPT-4 (C) prompt generally provides win\\nrates more representative of humans; we therefore\\nuse this prompt for the main results in Section 6.2.\\nFor additional details about the human study, in-\\ncluding the web interface presented to raters and\\nthe list of human volunteers, see Appendix D.3.\\n7 Discussion\\nLearning from preferences is a powerful, scalable framework for training capable, aligned language\\nmodels. We have introduced DPO, a simple training paradigm for training language models from\\npreferences without reinforcement learning. Rather than coercing the preference learning problem\\ninto a standard RL setting in order to use off-the-shelf RL algorithms, DPO identifies a mapping\\nbetween language model policies and reward functions that enables training a language model to\\nsatisfy human preferences directly , with a simple cross-entropy loss, without reinforcement learning\\nor loss of generality. With virtually no tuning of hyperparameters, DPO performs similarly or better\\nthan existing RLHF algorithms, including those based on PPO; DPO thus meaningfully reduces the\\nbarrier to training more language models from human preferences.\\nLimitations & Future Work. Our results raise several important questions for future work. How\\ndoes the DPO policy generalize out of distribution, compared with learning from an explicit reward\\nfunction? Our initial results suggest that DPO policies can generalize similarly to PPO-based models,\\nbut more comprehensive study is needed. For example, can training with self-labeling from the\\nDPO policy similarly make effective use of unlabeled prompts? On another front, how does reward\\nover-optimization manifest in the direct preference optimization setting, and is the slight decrease\\nin performance in Figure 3-right an instance of it? Additionally, while we evaluate models up to\\n6B parameters, exploration of scaling DPO to state-of-the-art models orders of magnitude larger is\\nan exciting direction for future work. Regarding evaluations, we find that the win rates computed\\nby GPT-4 are impacted by the prompt; future work may study the best way to elicit high-quality\\njudgments from automated systems. Finally, many possible applications of DPO exist beyond training\\nlanguage models from human preferences, including training generative models in other modalities.\\nAcknowledgements\\nEM gratefully acknowledges funding from a Knight-Hennessy Graduate Fellowship. CF and CM\\nare CIFAR Fellows. This work was supported in part by the Stanford Accelerator for Learning\\n(SAL) and Stanford Institute for Human-Centered Artificial Intelligence (HAI) Generative AI for the\\nFuture of Learning seed grant program. The Stanford Center for Research on Foundation Models\\n(CRFM) provided part of the compute resources used for the experiments in this work. This work\\nwas supported in part by ONR grant N00014-20-1-2675.\\n10', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='1504b93f-5100-4860-85c5-2dceab3f9304', embedding=None, metadata={'page_label': '11', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='be7508c78fd14f93bce6024381caa3e80399d640e951175bc2c74f14e5e5507d', text='References\\n[1]Y . Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli,\\nT. Henighan, N. Joseph, S. Kadavath, J. Kernion, T. Conerly, S. El-Showk, N. Elhage, Z. Hatfield-\\nDodds, D. Hernandez, T. Hume, S. Johnston, S. Kravec, L. Lovitt, N. Nanda, C. Olsson,\\nD. Amodei, T. Brown, J. Clark, S. McCandlish, C. Olah, B. Mann, and J. Kaplan. Training a\\nhelpful and harmless assistant with reinforcement learning from human feedback, 2022.\\n[2]Y . Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirho-\\nseini, C. McKinnon, C. Chen, C. Olsson, C. Olah, D. Hernandez, D. Drain, D. Ganguli, D. Li,\\nE. Tran-Johnson, E. Perez, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Ndousse, K. Lukosuite,\\nL. Lovitt, M. Sellitto, N. Elhage, N. Schiefer, N. Mercado, N. DasSarma, R. Lasenby, R. Lar-\\nson, S. Ringer, S. Johnston, S. Kravec, S. E. Showk, S. Fort, T. Lanham, T. Telleen-Lawton,\\nT. Conerly, T. Henighan, T. Hume, S. R. Bowman, Z. Hatfield-Dodds, B. Mann, D. Amodei,\\nN. Joseph, S. McCandlish, T. Brown, and J. Kaplan. Constitutional ai: Harmlessness from ai\\nfeedback, 2022.\\n[3]S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O’Brien, E. Hallahan, M. A. Khan,\\nS. Purohit, U. S. Prashanth, E. Raff, A. Skowron, L. Sutawika, and O. van der Wal. Pythia: A\\nsuite for analyzing large language models across training and scaling, 2023.\\n[4]H. Bong and A. Rinaldo. Generalized results for the existence and consistency of the MLE\\nin the Bradley-Terry-Luce model. International Conference on Machine Learning , 2022.\\narXiv:2110.11487.\\n[5]R. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. the method of\\npaired comparisons. Biometrika , 39(3/4):324–345, 1952. doi: https://doi.org/10.2307/2334029.\\n[6]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,\\nP. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger, T. Henighan, R. Child,\\nA. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Lan-\\nguage models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and\\nH. Lin, editors, Advances in Neural Information Processing Systems , volume 33, pages 1877–\\n1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_\\nfiles/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .\\n[7]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\\nG. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural\\ninformation processing systems , 33:1877–1901, 2020.\\n[8]S. Bubeck, V . Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y . T. Lee,\\nY . Li, S. Lundberg, H. Nori, H. Palangi, M. T. Ribeiro, and Y . Zhang. Sparks of artificial general\\nintelligence: Early experiments with GPT-4, 2023. arXiv preprint arXiv:2303.12712.\\n[9]R. Busa-Fekete, B. Szörényi, P. Weng, W. Cheng, and E. Hüllermeier. Preference-based\\nreinforcement learning: evolutionary direct policy search using a preference-based racing\\nalgorithm. Machine Learning , 97(3):327–351, July 2014. doi: 10.1007/s10994-014-5458-8.\\nURL https://doi.org/10.1007/s10994-014-5458-8 .\\n[10] Y . Chen, R. Wang, H. Jiang, S. Shi, and R.-L. Xu. Exploring the use of large language models\\nfor reference-free text quality evaluation: A preliminary empirical study. ArXiv , abs/2304.00723,\\n2023.\\n[11] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.\\nChung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv\\npreprint arXiv:2204.02311 , 2022.\\n[12] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement\\nlearning from human preferences. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus,\\nS. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Sys-\\ntems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/\\npaper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf .\\n11', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='368a5dce-77c7-47bc-965f-f79d3d1f980b', embedding=None, metadata={'page_label': '12', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='6296b1741bea4c042493f88df1a84bf189627b389e5f45ce12765f095f28e683', text='[13] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, Y . Li, X. Wang, M. Dehghani,\\nS. Brahma, A. Webson, S. S. Gu, Z. Dai, M. Suzgun, X. Chen, A. Chowdhery, A. Castro-Ros,\\nM. Pellat, K. Robinson, D. Valter, S. Narang, G. Mishra, A. Yu, V . Zhao, Y . Huang, A. Dai,\\nH. Yu, S. Petrov, E. H. Chi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V . Le, and J. Wei.\\nScaling instruction-finetuned language models, 2022.\\n[14] M. Dudík, K. Hofmann, R. E. Schapire, A. Slivkins, and M. Zoghi. Contextual dueling bandits.\\nIn P. Grünwald, E. Hazan, and S. Kale, editors, Proceedings of The 28th Conference on Learning\\nTheory , volume 40 of Proceedings of Machine Learning Research , pages 563–587, Paris, France,\\n03–06 Jul 2015. PMLR. URL https://proceedings.mlr.press/v40/Dudik15.html .\\n[15] D. Go, T. Korbak, G. Kruszewski, J. Rozen, N. Ryu, and M. Dymetman. Aligning language\\nmodels with preferences through f-divergence minimization. In Proceedings of the 40th\\nInternational Conference on Machine Learning , ICML’23. JMLR.org, 2023.\\n[16] A. Jain, B. Wojcik, T. Joachims, and A. Saxena. Learning trajectory preferences for manip-\\nulators via iterative improvement. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and\\nK. Weinberger, editors, Advances in Neural Information Processing Systems , volume 26. Curran\\nAssociates, Inc., 2013. URL https://proceedings.neurips.cc/paper_files/paper/\\n2013/file/c058f544c737782deacefa532d9add4c-Paper.pdf .\\n[17] T. Korbak, H. Elsahar, G. Kruszewski, and M. Dymetman. On reinforcement learning and\\ndistribution matching for fine-tuning language models with no catastrophic forgetting. In\\nS. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in\\nNeural Information Processing Systems , volume 35, pages 16203–16220. Curran Associates,\\nInc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/\\n67496dfa96afddab795530cc7c69b57a-Paper-Conference.pdf .\\n[18] J. Kreutzer, J. Uyheng, and S. Riezler. Reliability and learnability of human bandit feedback\\nfor sequence-to-sequence reinforcement learning. In Proceedings of the 56th Annual Meeting\\nof the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1777–1788,\\nMelbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/\\nP18-1165. URL https://aclanthology.org/P18-1165 .\\n[19] A. Kupcsik, D. Hsu, and W. S. Lee. Learning Dynamic Robot-to-Human Object Handover\\nfrom Human Feedback , pages 161–176. Springer International Publishing, 01 2018. ISBN\\n978-3-319-51531-1. doi: 10.1007/978-3-319-51532-8_10.\\n[20] S. Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review,\\n2018.\\n[21] R. D. Luce. Individual choice behavior: A theoretical analysis. Courier Corporation , 2012.\\n[22] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y . Ng, and C. Potts. Learning word vectors\\nfor sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for\\nComputational Linguistics: Human Language Technologies , pages 142–150, Portland, Oregon,\\nUSA, June 2011. Association for Computational Linguistics. URL http://www.aclweb.org/\\nanthology/P11-1015 .\\n[23] S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi. Cross-task generalization via natural\\nlanguage crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers) , pages 3470–3487, Dublin,\\nIreland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.\\n244. URL https://aclanthology.org/2022.acl-long.244 .\\n[24] R. Nallapati, B. Zhou, C. dos Santos, Ç. Gulçehre, and B. Xiang. Abstractive text summarization\\nusing sequence-to-sequence RNNs and beyond. In Proceedings of the 20th SIGNLL Conference\\non Computational Natural Language Learning , pages 280–290, Berlin, Germany, Aug. 2016.\\nAssociation for Computational Linguistics. doi: 10.18653/v1/K16-1028. URL https://\\naclanthology.org/K16-1028 .\\n12', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c3d603c3-1012-4d7c-86bb-eb3e073a4fe3', embedding=None, metadata={'page_label': '13', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='baab12b799b9b55771819c2487f0d9572dc4ef7281677776b9b7137de58dbbe3', text='[25] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V . Korthikanti, D. Vainbrand,\\nP. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee, and M. Zaharia. Efficient large-scale\\nlanguage model training on gpu clusters using megatron-lm. In Proceedings of the International\\nConference for High Performance Computing, Networking, Storage and Analysis , SC ’21, New\\nYork, NY , USA, 2021. Association for Computing Machinery. ISBN 9781450384421. doi:\\n10.1145/3458817.3476209. URL https://doi.org/10.1145/3458817.3476209 .\\n[26] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\\nK. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder,\\nP. F. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with\\nhuman feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,\\neditors, Advances in Neural Information Processing Systems , volume 35, pages 27730–27744.\\nCurran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/\\npaper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf .\\n[27] R. Paulus, C. Xiong, and R. Socher. A deep reinforced model for abstractive summarization. In\\nInternational Conference on Learning Representations , 2018. URL https://openreview.\\nnet/forum?id=HkAClQgA- .\\n[28] X. B. Peng, A. Kumar, G. Zhang, and S. Levine. Advantage-weighted regression: Simple and\\nscalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177 , 2019.\\n[29] J. Peters and S. Schaal. Reinforcement learning by reward-weighted regression for operational\\nspace control. In Proceedings of the 24th international conference on Machine learning , pages\\n745–750, 2007.\\n[30] R. L. Plackett. The analysis of permutations. Journal of the Royal Statistical Society. Series C\\n(Applied Statistics) , 24(2):193–202, 1975. doi: https://doi.org/10.2307/2346567.\\n[31] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are\\nunsupervised multitask learners, 2019. Ms., OpenAI.\\n[32] R. Ramamurthy, P. Ammanabrolu, K. Brantley, J. Hessel, R. Sifa, C. Bauckhage, H. Hajishirzi,\\nand Y . Choi. Is reinforcement learning (not) for natural language processing: Benchmarks,\\nbaselines, and building blocks for natural language policy optimization. In The Eleventh\\nInternational Conference on Learning Representations , 2023. URL https://openreview.\\nnet/forum?id=8aHzds2uUyB .\\n[33] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent neural\\nnetworks. CoRR , abs/1511.06732, 2015.\\n[34] D. Sadigh, A. D. Dragan, S. Sastry, and S. A. Seshia. Active preference-based learning of\\nreward functions. In Robotics: Science and Systems (RSS) , 2017.\\n[35] A. Saha, A. Pacchiano, and J. Lee. Dueling rl: Reinforcement learning with trajectory\\npreferences. In F. Ruiz, J. Dy, and J.-W. van de Meent, editors, Proceedings of The 26th\\nInternational Conference on Artificial Intelligence and Statistics , volume 206 of Proceed-\\nings of Machine Learning Research , pages 6263–6289. PMLR, 25–27 Apr 2023. URL\\nhttps://proceedings.mlr.press/v206/saha23a.html .\\n[36] V . Sanh, A. Webson, C. Raffel, S. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler,\\nA. Raja, M. Dey, M. S. Bari, C. Xu, U. Thakker, S. S. Sharma, E. Szczechla, T. Kim, G. Chh-\\nablani, N. Nayak, D. Datta, J. Chang, M. T.-J. Jiang, H. Wang, M. Manica, S. Shen, Z. X.\\nYong, H. Pandey, R. Bawden, T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Fevry,\\nJ. A. Fries, R. Teehan, T. L. Scao, S. Biderman, L. Gao, T. Wolf, and A. M. Rush. Multi-\\ntask prompted training enables zero-shot task generalization. In International Conference on\\nLearning Representations , 2022. URL https://openreview.net/forum?id=9Vrb9D0WI4 .\\n[37] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization\\nalgorithms, 2017.\\n[38] N. Stiennon, L. Ouyang, J. Wu, D. M. Ziegler, R. Lowe, C. V oss, A. Radford, D. Amodei, and\\nP. Christiano. Learning to summarize from human feedback, 2022.\\n13', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='6f2df83b-594e-4f34-b1bb-9066dedf6d55', embedding=None, metadata={'page_label': '14', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='bb88c7d1a003cd68874cdae16685c34a46fb8eae0462559cbffbaa3dccfc1766', text='[39] R. Thoppilan, D. D. Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos,\\nL. Baker, Y . Du, Y . Li, H. Lee, H. S. Zheng, A. Ghafouri, M. Menegali, Y . Huang, M. Krikun,\\nD. Lepikhin, J. Qin, D. Chen, Y . Xu, Z. Chen, A. Roberts, M. Bosma, V . Zhao, Y . Zhou,\\nC.-C. Chang, I. Krivokon, W. Rusch, M. Pickett, P. Srinivasan, L. Man, K. Meier-Hellstern,\\nM. R. Morris, T. Doshi, R. D. Santos, T. Duke, J. Soraker, B. Zevenbergen, V . Prabhakaran,\\nM. Diaz, B. Hutchinson, K. Olson, A. Molina, E. Hoffman-John, J. Lee, L. Aroyo, R. Rajakumar,\\nA. Butryna, M. Lamm, V . Kuzmina, J. Fenton, A. Cohen, R. Bernstein, R. Kurzweil, B. Aguera-\\nArcas, C. Cui, M. Croak, E. Chi, and Q. Le. Lamda: Language models for dialog applications,\\n2022.\\n[40] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,\\nE. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv\\npreprint arXiv:2302.13971 , 2023.\\n[41] M. Völske, M. Potthast, S. Syed, and B. Stein. TL;DR: Mining Reddit to learn automatic\\nsummarization. In Proceedings of the Workshop on New Frontiers in Summarization , pages\\n59–63, Copenhagen, Denmark, Sept. 2017. Association for Computational Linguistics. doi:\\n10.18653/v1/W17-4508. URL https://aclanthology.org/W17-4508 .\\n[42] L. von Werra, J. Tow, reciprocated, S. Matiana, A. Havrilla, cat state, L. Castricato, Alan, D. V .\\nPhung, A. Thakur, A. Bukhtiyarov, aaronrmm, F. Milo, Daniel, D. King, D. Shin, E. Kim, J. Wei,\\nM. Romero, N. Pochinkov, O. Sanseviero, R. Adithyan, S. Siu, T. Simonini, V . Blagojevic,\\nX. Song, Z. Witten, alexandremuzio, and crumb. CarperAI/trlx: v0.6.0: LLaMa (Alpaca),\\nBenchmark Util, T5 ILQL, Tests, Mar. 2023. URL https://doi.org/10.5281/zenodo.\\n7790115 .\\n[43] B. Wang and A. Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language\\nModel. https://github.com/kingoflolz/mesh-transformer-jax , May 2021.\\n[44] S. Welleck, I. Kulikov, S. Roller, E. Dinan, K. Cho, and J. Weston. Neural text generation with\\nunlikelihood training. arXiv preprint arXiv:1908.04319 , 2019.\\n[45] R. J. Williams. Simple statistical gradient-following algorithms for connectionist rein-\\nforcement learning. Mach. Learn. , 8(3–4):229–256, may 1992. ISSN 0885-6125. doi:\\n10.1007/BF00992696. URL https://doi.org/10.1007/BF00992696 .\\n[46] Y . Wu and B. Hu. Learning to extract coherent summary via deep reinforcement learning.\\nInProceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth\\nInnovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on\\nEducational Advances in Artificial Intelligence , AAAI’18/IAAI’18/EAAI’18. AAAI Press,\\n2018. ISBN 978-1-57735-800-8.\\n[47] X. Yan, C. Luo, C. L. A. Clarke, N. Craswell, E. M. V oorhees, and P. Castells. Human\\npreferences as dueling bandits. In Proceedings of the 45th International ACM SIGIR Conference\\non Research and Development in Information Retrieval , SIGIR ’22, page 567–577, New\\nYork, NY , USA, 2022. Association for Computing Machinery. ISBN 9781450387323. doi:\\n10.1145/3477495.3531991. URL https://doi.org/10.1145/3477495.3531991 .\\n[48] Y . Yue, J. Broder, R. Kleinberg, and T. Joachims. The k-armed dueling bandits problem.\\nJournal of Computer and System Sciences , 78(5):1538–1556, 2012. ISSN 0022-0000. doi: https:\\n//doi.org/10.1016/j.jcss.2011.12.028. URL https://www.sciencedirect.com/science/\\narticle/pii/S0022000012000281 . JCSS Special Issue: Cloud Computing 2011.\\n[49] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and\\nG. Irving. Fine-tuning language models from human preferences, 2020.\\n14', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='6a1ddc65-1ae6-4b8c-8dde-92d742054c8e', embedding=None, metadata={'page_label': '15', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='ebfda30b17819bbdb66d91d13a66a8ed85be2685b9151c9a03de35df3e2a08cc', text='Author Contributions\\nAll authors provided valuable contributions to designing, analyzing, and iterating on experiments,\\nwriting and editing the paper, and generally managing the project’s progress.\\nRRproposed using autoregressive reward models in discussions with EM; derived the DPO objective;\\nproved the theoretical properties of the algorithm and wrote the relevant sections and appendices. He\\nalso suggested and helped with organizing experiments and contributed some of the PPO and reward\\nlearning baselines.\\nASinitiated the discussion on using weighted regression methods as an alternative to PPO; initiated\\nproject-related organization, wrote initial analysis connecting DPO with weighted regression and\\nunlikelihood; design and iterations of DPO + baseline implementations, initial exploratory exper-\\niments for DPO; substantial experiment organization and design (datasets, baselines, evaluation);\\nled model training and evaluation for controlled sentiment generation and summarization; design\\niterations for GPT-4 evaluation (particularly summarization); substantial writing contributions to\\nabstract, prelims/method and experiments; editing contributions to other sections.\\nEMprovided input on early discussions on learning autoregressive reward functions; wrote the first\\nimplementation of DPO and ran the first DPO experiments; trained the large-scale (summarization\\nand dialogue) DPO models used in paper experiments; conducted initial GPT-4 win rate evaluations\\nand set up related infrastructure; recruited participants for, conducted, and analyzed results from the\\nhuman study; wrote the abstract, introduction, related work, discussion, and most of experiments;\\nand assisted with editing the rest of the paper.\\nCF, CM, & SE supervised the research, suggested ideas and experiments, and assisted in writing the\\npaper.\\nA Mathematical Derivations\\nA.1 Deriving the Optimum of the KL-Constrained Reward Maximization Objective\\nIn this appendix, we will derive Eq. 4. Analogously to Eq. 3, we optimize the following objective:\\nmax\\nπEx∼D,y∼π\\x02\\nr(x, y)\\x03\\n−βDKL\\x02\\nπ(y|x)||πref(y|x)\\x03\\n(11)\\nunder any reward function r(x, y), reference model πrefand a general non-parametric policy class.\\nWe now have:\\nmax\\nπEx∼D,y∼π\\x02\\nr(x, y)\\x03\\n−βDKL\\x02\\nπ(y|x)||πref(y|x)\\x03\\n= max\\nπEx∼DEy∼π(y|x)\\x14\\nr(x, y)−βlogπ(y|x)\\nπref(y|x)\\x15\\n= min\\nπEx∼DEy∼π(y|x)\\x14\\nlogπ(y|x)\\nπref(y|x)−1\\nβr(x, y)\\x15\\n= min\\nπEx∼DEy∼π(y|x)\\uf8ee\\n\\uf8f0logπ(y|x)\\n1\\nZ(x)πref(y|x) exp\\x10\\n1\\nβr(x, y)\\x11−logZ(x)\\uf8f9\\n\\uf8fb(12)\\nwhere we have partition function:\\nZ(x) =X\\nyπref(y|x) exp\\x121\\nβr(x, y)\\x13\\n.\\nNote that the partition function is a function of only xand the reference policy πref, but does not\\ndepend on the policy π. We can now define\\nπ∗(y|x) =1\\nZ(x)πref(y|x) exp\\x121\\nβr(x, y)\\x13\\n,\\n15', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f3e047e7-286c-4e1c-8c53-af0dbe938da3', embedding=None, metadata={'page_label': '16', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='22b6a30ca8ffd4e5c94eeaa0b3a816bc05e9e6610370d3a36ada750b284f93a1', text='which is a valid probability distribution as π∗(y|x)≥0for all yandP\\nyπ∗(y|x) = 1 . Since Z(x)is\\nnot a function of y, we can then re-organize the final objective in Eq 12 as:\\nmin\\nπEx∼D\\x14\\nEy∼π(y|x)\\x14\\nlogπ(y|x)\\nπ∗(y|x)\\x15\\n−logZ(x)\\x15\\n= (13)\\nmin\\nπEx∼D[DKL(π(y|x)||π∗(y|x))−logZ(x)] (14)\\nNow, since Z(x)does not depend on π, the minimum is achieved by the policy that minimizes the\\nfirst KL term. Gibbs’ inequality tells us that the KL-divergence is minimized at 0 if and only if the\\ntwo distributions are identical. Hence we have the optimal solution:\\nπ(y|x) =π∗(y|x) =1\\nZ(x)πref(y|x) exp\\x121\\nβr(x, y)\\x13\\n(15)\\nfor all x∈ D. This completes the derivation.\\nA.2 Deriving the DPO Objective Under the Bradley-Terry Model\\nIt is straightforward to derive the DPO objective under the Bradley-Terry preference model as we\\nhave\\np∗(y1≻y2|x) =exp (r∗(x, y 1))\\nexp (r∗(x, y 1)) + exp ( r∗(x, y 2))(16)\\nIn Section 4 we showed that we can express the (unavailable) ground-truth reward through its\\ncorresponding optimal policy:\\nr∗(x, y) =βlogπ∗(y|x)\\nπref(y|x)+βlogZ(x) (17)\\nSubstituting Eq. 17 into Eq. 16 we obtain:\\np∗(y1≻y2|x) =exp\\x10\\nβlogπ∗(y1|x)\\nπref(y1|x)+βlogZ(x)\\x11\\nexp\\x10\\nβlogπ∗(y1|x)\\nπref(y1|x)+βlogZ(x)\\x11\\n+ exp\\x10\\nβlogπ∗(y2|x)\\nπref(y2|x)+βlogZ(x)\\x11\\n=1\\n1 + exp\\x10\\nβlogπ∗(y2|x)\\nπref(y2|x)−βlogπ∗(y1|x)\\nπref(y1|x)\\x11\\n=σ\\x12\\nβlogπ∗(y1|x)\\nπref(y1|x)−βlogπ∗(y2|x)\\nπref(y2|x)\\x13\\n.\\nThe last line is the per-instance loss in Equation 7.\\nA.3 Deriving the DPO Objective Under the Plackett-Luce Model\\nThe Plackett-Luce model [ 30,21] is a generalization of the Bradley-Terry model over rankings (rather\\nthan just pair-wise comparisons). Similar to to the Bradley-Terry model, it stipulates that when\\npresented with a set of possible choices, people prefer a choice with probability proportional to the\\nvalue of some latent reward function for that choice. In our context, when presented with a prompt x\\nand a set of Kanswers y1, . . . , y Ka user would output a permutation τ: [K]→[K], giving their\\nranking of the answers. The Plackett-Luce model stipulates that\\np∗(τ|y1, . . . , y K, x) =KY\\nk=1exp(r∗(x, yτ(k)))\\nPK\\nj=kexp(r∗(x, yτ(j)))(18)\\nNotice that when K= 2, Equation 18 reduces to the Bradley-Terry model. However, for the general\\nPlackett-Luce model, we can still utilize the results of Eq. 5 and substitute the reward function\\nparameterized by its optimal policy. Similarly to Appendix A.2, the normalization constant Z(x)\\ncancels out and we’re left with:\\np∗(τ|y1, . . . , y K, x) =KY\\nk=1exp\\x10\\nβlogπ∗(yτ(k)|x)\\nπref(yτ(k)|x)\\x11\\nPK\\nj=kexp\\x10\\nβlogπ∗(yτ(j)|x)\\nπref(yτ(j)|x)\\x11 (19)\\n16', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f636b29e-9241-4f90-90a5-42fe7b4eb24c', embedding=None, metadata={'page_label': '17', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='02c1cc227734f92c2036634d1a73cebd20d19545edc8509f21c0821434a3806a', text='Similarly to the approach of Section 4, if we have access to a dataset D =\\n{τ(i), y(i)\\n1, . . . , y(i)\\nK, x(i)}N\\ni=1of prompts and user-specified rankings, we can use a parameterized\\nmodel and optimize this objective with maximum-likelihood.:\\nLDPO(πθ, πref) =−Eτ,y1,...,y K,x∼D\\uf8ee\\n\\uf8f0logKY\\nk=1exp\\x10\\nβlogπθ(yτ(k)|x)\\nπref(yτ(k)|x)\\x11\\nPK\\nj=kexp\\x10\\nβlogπθ(yτ(j)|x)\\nπref(yτ(j)|x)\\x11\\uf8f9\\n\\uf8fb (20)\\nA.4 Deriving the Gradient of the DPO Objective\\nIn this section we derive the gradient of the DPO objective:\\n∇θLDPO(πθ;πref) =−∇θE(x,yw,yl)∼D\\x14\\nlogσ\\x12\\nβlogπθ(yl|x)\\nπref(yl|x)−βlogπθ(yw|x)\\nπref(yw|x)\\x13\\x15\\n(21)\\nWe can rewrite the RHS of Equation 21 as\\n∇θLDPO(πθ;πref) =−E(x,yw,yl)∼D\\x14σ′(u)\\nσ(u)∇θ(u)\\x15\\n, (22)\\nwhere u=βlogπθ(yl|x)\\nπref(yl|x)−βlogπθ(yw|x)\\nπref(yw|x).\\nUsing the properties of sigmoid function σ′(x) =σ(x)(1−σ(x))andσ(−x) = 1−σ(x), we obtain\\nthe final gradient\\n∇θLDPO(πθ;πref) =\\n−E(x,yw,yl)∼D\\x14\\nβσ\\x12\\nβlogπθ(yw|x)\\nπref(yw|x)−βlogπθ(yl|x)\\nπref(yl|x)\\x13\\x14\\n∇θlogπ(yw|x)−∇θlogπ(yl|x)\\x15\\x15\\n,\\nAfter using the reward substitution of ˆrθ(x, y) =βlogπθ(y|x)\\nπref(y|x)we obtain the final form of the\\ngradient from Section 4.\\nA.5 Proof of Lemma 1 and 2\\nIn this section, we will prove the two lemmas from Section 5.\\nLemma 1 Restated. Under the Plackett-Luce preference framework, and in particular the Bradley-\\nTerry framework, two reward functions from the same equivalence class induce the same preference\\ndistribution.\\nProof. We say that two reward functions r(x, y)andr′(x, y)are from the same equivalence class\\nifr′(x, y) =r(x, y) +f(x)for some function f. We consider the general Plackett-Luce (with the\\nBradley-Terry model a special case for K= 2) and denote the probability distribution over rankings\\ninduced by a particular reward function r(x, y)aspr. For any prompt x, answers y1, . . . , y Kand\\nranking τwe have:\\npr′(τ|y1, . . . , y K, x) =KY\\nk=1exp(r′(x, yτ(k)))\\nPK\\nj=kexp(r′(x, yτ(j)))\\n=KY\\nk=1exp(r(x, yτ(k)) +f(x))\\nPK\\nj=kexp(r(x, yτ(j)) +f(x))\\n=KY\\nk=1exp(f(x)) exp( r(x, yτ(k)))\\nexp(f(x))PK\\nj=kexp(r(x, yτ(j)))\\n=KY\\nk=1exp(r(x, yτ(k)))\\nPK\\nj=kexp(r(x, yτ(j)))\\n=pr(τ|y1, . . . , y K, x),\\nwhich completes the proof.\\n17', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ec0466eb-1571-4368-91e0-e6d62d610e04', embedding=None, metadata={'page_label': '18', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='81841087da79527826d6cc7eabc6d3f614bf692305cf1b337528a538de13e57d', text='Lemma 2 Restated. Two reward functions from the same equivalence class induce the same optimal\\npolicy under the constrained RL problem.\\nProof. Let us consider two reward functions from the same class, such that r′(x, y) =r(x, y) +f(x)\\nand, let us denote as πrandπr′the corresponding optimal policies. By Eq. 4, for all x, ywe have\\nπr′(y|x) =1\\nP\\nyπref(y|x) exp\\x10\\n1\\nβr′(x, y)\\x11πref(y|x) exp\\x121\\nβr′(x, y)\\x13\\n=1\\nP\\nyπref(y|x) exp\\x10\\n1\\nβ(r(x, y) +f(x))\\x11πref(y|x) exp\\x121\\nβ(r(x, y) +f(x))\\x13\\n=1\\nexp\\x10\\n1\\nβf(x)\\x11P\\nyπref(y|x) exp\\x10\\n1\\nβr(x, y)\\x11πref(y|x) exp\\x121\\nβr(x, y)\\x13\\nexp\\x121\\nβf(x)\\x13\\n=1\\nP\\nyπref(y|x) exp\\x10\\n1\\nβr(x, y)\\x11πref(y|x) exp\\x121\\nβr(x, y)\\x13\\n=πr(y|x),\\nwhich completes the proof.\\nA.6 Proof of Theorem 1\\nIn this section, we will expand on the results of Theorem 1.\\nTheorem 1 Restated. Assume, we have a reference model, such that πref(y|x)>0for all pairs of\\nprompts xand answers yand a parameter β > 0. All reward equivalence classes, as defined in\\nSection 5 can be represented with the reparameterization r(x, y) =βlogπ(y|x)\\nπref(y|x)for some model\\nπ(y|x).\\nProof. Consider any reward function r(x, y), which induces an optimal model πr(y|x)under the\\nKL-constrained RL problem, with solution given by 4. Following Eq. 5, when we log-linearize both\\nsides we obtain:\\nr(x, y) =βlogπr(y|x)\\nπref(y|x)+βlogZ(x)\\nwhere Z(x) =P\\nyπref(y|x) exp\\x10\\n1\\nβr(x, y)\\x11\\n(notice that Z(x)also depends on the reward function\\nr). Using the operator r′(x, y) =f(r, π ref, β)(x, y) =r(x, y)−βlogZ(x), we see that this new\\nreward function is within the equivalence class of rand, we have:\\nr′(x, y) =βlogπr(y|x)\\nπref(y|x)\\nwhich completes the proof.\\nWe can further expand on these results. We can see that if randr′are two reward functions in the\\nsame class, then\\nf(r, π ref, β)(x, y) =βlogπr(y|x)\\nπref(y|x)=βlogπ′\\nr(y|x)\\nπref(y|x)=f(r′, πref, β)(x, y)\\nwhere the second equality follows from Lemma 2. We have proven that the operator fmaps all\\nreward functions from a particular equivalence class to the same reward function. Next, we show that\\nfor every equivalence class of reward functions, the reward function that has the reparameterization\\noutlined in Theorem 1 is unique.\\nProposition 1. Assume, we have a reference model, such that πref(y|x)>0for all pairs of prompts\\nxand answers yand a parameter β > 0. Then every equivalence class of reward functions,\\nas defined in Section 5, has a unique reward function r(x, y), which can be reparameterized as\\nr(x, y) =βlogπ(y|x)\\nπref(y|x)for some model π(y|x).\\n18', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='4352de81-32af-47f0-b08d-014615179a4d', embedding=None, metadata={'page_label': '19', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='4a669e8eae0ac76c60c90d5275834c63d7c76d2c140c1ffcbe436baebb7dd6f9', text='Proof. We will proceed using proof by contradiction. Assume we have two reward functions from\\nthe same class, such that r′(x, y) =r(x, y) +f(x). Moreover, assume that r′(x, y) =βlogπ′(y|x)\\nπref(y|x)\\nfor some model π′(y|x)andr(x, y) =βlogπ(y|x)\\nπref(y|x)for some model π(y|x), such that π̸=π′. We\\nthen have\\nr′(x, y) =r(x, y) +f(x) =βlogπ(y|x)\\nπref(y|x)+f(x) =βlogπ(y|x) exp(1\\nβf(x))\\nπref(y|x)=βlogπ′(y|x)\\nπref(y|x)\\nfor all prompts xand completions y. Then we must have π(y|x) exp(1\\nβf(x)) =π′(y|x). Since these\\nare distributions, summing over yon both sides, we obtain that exp(1\\nβf(x)) = 1 and since β >0,\\nwe must have f(x) = 0 for all x. Therefore r(x, y) =r′(x, y). This completes the proof.\\nWe have now shown that every reward class has a unique reward function that can be represented as\\noutlined in Theorem 1, which is given by f(r, π ref, β)for any reward function in that class.\\nB DPO Implementation Details and Hyperparameters\\nDPO is relatively straightforward to implement; PyTorch code for the DPO loss is provided below:\\n19', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='fe0a92f9-31c1-40e8-9a11-d27dcfd4e57d', embedding=None, metadata={'page_label': '20', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='a27adb8e9d9fdf4b2ea567772741785def15aca469efc67df7633be8b2bcb6ce', text='import torch.nn.functional as F\\ndef dpo_loss(pi_logps, ref_logps, yw_idxs, yl_idxs, beta):\\n\"\"\"\\npi_logps: policy logprobs, shape (B,)\\nref_logps: reference model logprobs, shape (B,)\\nyw_idxs: preferred completion indices in [0, B-1], shape (T,)\\nyl_idxs: dispreferred completion indices in [0, B-1], shape (T,)\\nbeta: temperature controlling strength of KL penalty\\nEach pair of (yw_idxs[i], yl_idxs[i]) represents the\\nindices of a single preference pair.\\n\"\"\"\\npi_yw_logps, pi_yl_logps = pi_logps[yw_idxs], pi_logps[yl_idxs]\\nref_yw_logps, ref_yl_logps = ref_logps[yw_idxs], ref_logps[yl_idxs]\\npi_logratios = pi_yw_logps - pi_yl_logps\\nref_logratios = ref_yw_logps - ref_yl_logps\\nlosses = -F.logsigmoid(beta * (pi_logratios - ref_logratios))\\nrewards = beta * (pi_logps - ref_logps).detach()\\nreturn losses, rewards\\nUnless noted otherwise, we use a β= 0.1, batch size of 64and the RMSprop optimizer with a\\nlearning rate of 1e-6 by default. We linearly warmup the learning rate from 0to1e-6 over150steps.\\nFor TL;DR summarization, we use β= 0.5, while rest of the parameters remain the same.\\nC Further Details on the Experimental Set-Up\\nIn this section, we include additional details relevant to our experimental design.\\nC.1 IMDb Sentiment Experiment and Baseline Details\\nThe prompts are prefixes from the IMDB dataset of length 2-8 tokens. We use the pre-trained senti-\\nment classifier siebert/sentiment-roberta-large-english as a ground-truth reward model\\nandgpt2-large as a base model. We use these larger models as we found the default ones to\\ngenerate low-quality text and rewards to be somewhat inaccurate. We first use supervised fine-tuning\\non a subset of the IMDB data for 1 epoch. We then use this model to sample 4 completions for 25000\\nprefixes and create 6 preference pairs for each prefix using the ground-truth reward model. The RLHF\\nreward model is initialized from the gpt2-large model and trained for 3 epochs on the preference\\ndatasets, and we take the checkpoint with the highest validation set accuracy. The “TRL” run uses\\nthe hyper-parameters in the TRL library. Our implementation uses larger batch samples of 1024 per\\nPPO step.\\nC.2 GPT-4 prompts for computing summarization and dialogue win rates\\nA key component of our experimental setup is GPT-4 win rate judgments. In this section, we include\\nthe prompts used to generate win rates for the summarization and dialogue experiments. We use\\ngpt-4-0314 for all our experiments. The order of summaries or responses are randomly chosen for\\nevery evaluation.\\nSummarization GPT-4 win rate prompt (S).\\nWhich of the following summaries does a better job of summarizing the most \\\\\\nimportant points in the given forum post?\\nPost:\\n20', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='4aec3040-022e-486e-be43-89ee7c8ea623', embedding=None, metadata={'page_label': '21', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='38b5fa02a1442719706cc5acc32d201d7e35f6b39dd342ac004d5f234421532f', text='<post>\\nSummary A:\\n<Summary A>\\nSummary B:\\n<Summary B>\\nFIRST provide a one-sentence comparison of the two summaries, explaining which \\\\\\nyou prefer and why. SECOND, on a new line, state only \"A\" or \"B\" to indicate your \\\\\\nchoice. Your response should use the format:\\nComparison: <one-sentence comparison and explanation>\\nPreferred: <\"A\" or \"B\">\\nSummarization GPT-4 win rate prompt (C).\\nWhich of the following summaries does a better job of summarizing the most \\\\\\nimportant points in the given forum post, without including unimportant or \\\\\\nirrelevant details? A good summary is both precise and concise.\\nPost:\\n<post>\\nSummary A:\\n<Summary A>\\nSummary B:\\n<Summary B>\\nFIRST provide a one-sentence comparison of the two summaries, explaining which \\\\\\nyou prefer and why. SECOND, on a new line, state only \"A\" or \"B\" to indicate your \\\\\\nchoice. Your response should use the format:\\nComparison: <one-sentence comparison and explanation>\\nPreferred: <\"A\" or \"B\">\\nDialogue GPT-4 win rate prompt.\\nFor the following query to a chatbot, which response is more helpful?\\nQuery: <the user query>\\nResponse A:\\n<either the test method or baseline>\\nResponse B:\\n<the other response>\\nFIRST provide a one-sentence comparison of the two responses and explain \\\\\\nwhich you feel is more helpful. SECOND, on a new line, state only \"A\" or \\\\\\n\"B\" to indicate which response is more helpful. Your response should use \\\\\\nthe format:\\nComparison: <one-sentence comparison and explanation>\\nMore helpful: <\"A\" or \"B\">\\nC.3 Unlikelihood baseline\\nWhile we include the unlikelihood baseline [ 44] (simply maximizing logp(yw|x), the log probability\\nof the preferred response, while minimizing logp(yl|x), the log probability of the dispreferred\\nresponse) in our sentiment experiments, we do not include it as a baseline in either the summarization\\n21', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d11bd708-ad1b-4b7f-909c-4247c9f181da', embedding=None, metadata={'page_label': '22', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='95fd9991b83eef75876bbd5daae364f0a8e22ca8d058aa490f3a33a0a56243db', text='Prompt Response\\nSUBREDDIT: r/relationships\\nTITLE: The girl [26 F] I [22 M] have been\\nseeing for a month didn’t respond to me at all\\nyesterday while hanging out with a friend [ 30?\\nM].\\nPOST: She gets terrible service while at her\\nhouse, but I texted her 3 times yesterday, 4-5\\nhours apart. She didn’t call me until early this\\nmorning and left a voicemail that she was busy\\nall day with a friend who showed up out of the\\nblue.\\nI saw that she posted a picture of the two of\\nthem out of her dead zone house on facebook\\nbefore I texted her the last time.\\nI don’t mind that she hangs out with friends,\\nand I know it’s pretty early [. . . ] TL;DR:girl when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when whenUB when when when\\nwhen whenUB when when when whenUB\\nSUBREDDIT: r/tifu\\nTITLE: TIFU by accidently kicking an old\\nwoman\\nPOST: So this didn’t happen today but actually\\nabout a year or two ago.\\nI was at my granddads funeral so of course it\\nwas all very sad and full of lots of crying old\\npeople. After the ceremony everyone walks\\noutside the building and onto the other side of\\nthe small road the hearses drive down. Now the\\nroad is important because obviously if there’s a\\nroad, there’s a curb onto the sidewalk, so most\\nof us are on the other side of the road, besides\\na few older people walking a lot slower.\\nAs one of the old woman goes to walk up the\\ncurb [. . . ] TL;DR:when an old woman was tripping the when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when when\\nwhen when when when when when when\\nTable 3: Unlikelihood samples from TL;DR prompts sampled at temperature 1.0. In general, we find unlikelihood\\nfails to generate meaningful responses for more complex problems such as summarization and dialogue.\\nor dialogue experiment because it produces generally meaningless responses, which we believe is a\\nresult of unconstrained likelihood minimization .\\nD Additional Empirical Results\\nD.1 Performance of Best of Nbaseline for Various N\\nWe find that the Best of Nbaseline is a strong (although computationally expensive, requiring\\nsampling many times) baseline in our experiments. We include an evaluation of the Best of N\\nbaseline for various Nfor the Anthropic-HH dialogue and TL;DR summarization; the results are\\nshown in Figure 4.\\nD.2 Sample Responses and GPT-4 Judgments\\nIn this section, we present examples of comparisons between DPO and the baseline (PPO temp 0. for\\nsummarization, and the ground truth chosen response for dialogue). See Tables 4-6 for summarization\\nexamples, and Tables 7-10 for dialogue examples.\\n22', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='03d1b4ae-d89b-471f-a4a4-e1a2db0761eb', embedding=None, metadata={'page_label': '23', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='af21868b227df76eac573d5123c4686bb14a6602d723ba2d3fe83bde04163885', text='0.25 0.50 0.75 1.00\\nSampling temperature0.200.250.300.350.400.450.500.550.60Win rateAnthropic-HH Dialogue Win Rate vs Chosen\\nBest of 1\\nBest of 4Best of 16\\nBest of 64Best of 128\\n0.00 0.25 0.50 0.75 1.00\\nSampling temperature0.00.10.20.30.40.50.60.7Win rateTL;DR Summarization Win Rate vs Reference\\nBest of 64 Best of 128 Best of 256Figure 4: Best of Nbaseline for N={1,4,16,64,128}. Performance plateaus after roughly 64-128 samples.\\nPrompt SUBREDDIT: r/AskReddit TITLE: I’ve been ungrateful to parents who have\\nwanted only the best for me for the longest time. I am so lucky to have the life I\\nhave but I squandered it, this isn’t a humblebrag about my \"wicked life\" just tell me,\\nwhat can I do to make it up to them after this. POST: I’m 17, leaving for University,\\nand I’ve had epiphany after epiphany of how shitty i’ve beent to them, or how all\\nthe nice things they do I ruin with moods etc., I know there isn’t a quick fix, but I\\nwant to do something nice. I’ve always thanked them and apologized where it is\\ndue (in earlier years not so much) but as they say, actions speak louder than words,\\nso I was wondering what can I do now, before I go, to just make them happy. What\\ncan I keep doing, I know all the little things, doing chores, keeping my room clean,\\ncleaning up after myself, helping around the house, but what’s a big thing, for say\\nchristmas, or their birthdays, that I can do to make them happy?\\nDPO TL;DR: I’ve been a shitty child and I want to make up for it, what can I do for my\\nparents on their birthdays and christmas to show them how much I love them?\\nPPO TL;DR: I’ve been a shitty person to my parents, what can I do to make it up to them,\\nbefore I go, and what can I do after university to make them happy, and what can I\\ndo to make them happy after university (if that is the case)\\nJudgment Summary B [DPO] more effectively captures the main points of the post by focusing\\non making the parents happy through gestures on special occasions, while Summary\\nA seems repetitive and less clear.\\nTable 4: Sample summaries to a post from the TL;DR test set. DPO sample generated with a temperature 0.25;\\nPPO sample generated at a temperature of 0. The order in which summaries are presented in randomized when\\nevaluating with GPT-4, so the order in the Judgment may not correspond to the order in the table. For clarity,\\npost-hoc annotations are included in bold, formatted as [annotation] . These annotations are not part of the\\nmodel generations.\\n23', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='4343f522-7083-40e0-8291-1e2392bcd38c', embedding=None, metadata={'page_label': '24', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='303a3d72aa667784ab058ac80794309b899717a10d91c1d27582b87b2c77fc89', text='Prompt SUBREDDIT: r/relationships TITLE: I [22 M] am about to propose to my girlfriend\\n[22F] of almost 2 years, and I can’t calm myself down POST: Alright, so I am\\npreparing to propose to my girlfriend of almost 2 years (we were friends for\\ndouble that beforehand) in a couple days and I am starting to get EXTREMELY\\nanxious/nervous. We have talked about this for months, and I know she will say\\nyes, but I can’t seem to relax.\\nThe other day I bought the ring, which I fell in love with myself and know she will\\ntoo. And then two days ago, I asked her dad’s blessing. Everything has gone great\\nso far, but now leading up to it I can’t think about anything else. I can’t sleep. I\\nam anxious as hell, and all I want to do is pop the question now, but I have to wait\\nsince I don’t see her until Thursday.\\nI know how I am going to do it. I have thoroughly planned it out and I am more\\nthan prepared. But, at the same time, I am freaking out. I asked my best friend for\\nadvice since he just got married recently, and all he could say was \"It will feel 10x\\nworse right before you do ask.\"\\nAny suggestions on how to relax leading up to it? I would like to be able to sleep...\\nDPO TL;DR: About to propose to my girlfriend of almost 2 years, but I am extremely\\nanxious and can’t sleep. How can I relax?\\nPPO TL;DR: I am about to ask my girlfriend of almost 2 years to\\nmarry me, and I can’t calm myself down. Any tips on how to re-\\nlax? I would like to be able to sleep tonight (and the night be-\\nfore)..............................................................................................................................\\nJudgment Summary A [DPO] more effectively summarizes the main points of the post,\\nconcisely conveying the asker’s anxiety and goal of finding ways to relax.\\nTable 5: Sample summaries to a post from the TL;DR test set. DPO sample generated with a temperature 0.25;\\nPPO sample generated at a temperature of 0. The order in which summaries are presented in randomized when\\nevaluating with GPT-4, so the order in the Judgment may not correspond to the order in the table. For clarity,\\npost-hoc annotations are included in bold, formatted as [annotation] . These annotations are not part of the\\nmodel generations.\\nPrompt SUBREDDIT: r/loseit TITLE: Is It Bullshit?: Fiber Gourmet Pasta POST: I was\\nlooking for low-cal alternatives to pasta, because I love pasta and it’s easy to\\nmake and I eat it several times a week. I find that whole grain pasta has a weird\\ntaste/texture, and I’m not a big fan of it.\\nI was doing some research into spaghetti squash (which is on my grocery list for\\nnext time), but I also heard someone rave about some high-fiber pasta brand so I\\nlooked into it.\\nWhat sorcery is this? It has all the trappings of regular pasta (and the reviews I’ve\\nread say it tastes the same too) but a little over half the calories. My mind boggles\\nover how much extra pasta I could eat! I can’t believe this hasn’t taken the world\\nby storm, which makes me wonder what the catch is.\\nDPO TL;DR: I’m trying to cut back on calories and pasta is one of my main sources.\\nFound a high-fiber pasta that has all the trappings of regular pasta and seems like it\\nwould be a good substitute. Is it bullshit?\\nPPO TL;DR: I want to lose weight, but I love pasta and don’t want to give it up. Is there\\na high-fiber pasta that I can eat more of and lose weight with, or is it all bullshit?\\nJudgment Summary B [DPO] more accurately captures the intent to find a low-calorie pasta\\nalternative, without assuming the need for weight loss.\\nTable 6: Sample summaries to a post from the TL;DR test set. DPO sample generated with a temperature 0.25;\\nPPO sample generated at a temperature of 0. The order in which summaries are presented in randomized when\\nevaluating with GPT-4, so the order in the Judgment may not correspond to the order in the table. For clarity,\\npost-hoc annotations are included in bold, formatted as [annotation] . These annotations are not part of the\\nmodel generations.\\n24', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='35cf3764-8a36-449a-8b88-108dab4ec010', embedding=None, metadata={'page_label': '25', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='53326a9009e97b4755488bf7caea0f589a9119ab6712583e062bf0641bcbb896', text='Prompt Can you help me write an essay on the Civil Rights Movement?\\nDPO The Civil Rights Movement was a series of social and political movements that took\\nplace between the 1950s and the 1980s, leading to the integration of public spaces,\\nhigher education, employment, and voting rights. The movement is often credited\\nwith the end of legal segregation in the United States, and it was a major catalyst for\\nthe Civil Rights Act of 1964. The Movement also played a large role in the rise of\\nthe Black Power and Civil Rights Movements, which focused on economic equality\\nand racial justice. The Movement focused on the rights of African Americans, who\\nwere then the dominant racial and ethnic group in the United States, but it also\\nhighlighted the rights of other minority groups, such as women, immigrants, and\\nLGBT people. The Movement is often considered one of the most important social\\nmovements of the 20th century, and it played a significant role in the advancement\\nof civil rights for all Americans. I’ll be happy to write an essay on this topic for\\nyou. What kind of essay would you like? A paper on the history of the Civil Rights\\nMovement? A paper on the causes of the Civil Rights Movement? Or a paper on\\nthe effects of the Civil Rights Movement? Any ideas? I’ll be happy to [maximum\\ngeneration length reached]\\nGT Oh, I’m not sure if I can actually write an essay, but I can at least help you think\\nthrough the process. First, can you tell me what you’re trying to write about? In\\nother words, what are the main points or questions you want to address?\\nJudgment Response B [DPO] provides more detailed information about the Civil Rights\\nMovement and offers specific suggestions for essay topics, making it more helpful\\nfor someone writing an essay.\\nTable 7: GPT-4 chooses DPO over GT. Sample responses to a prompt from the Anthropic-HH test set. DPO\\nsample generated with temperature 0.7; GTis the chosen completion in the dataset of preferences. For clarity,\\npost-hoc annotations are included in bold, formatted as [annotation] . These annotations are not part of the\\nmodel generations.\\nPrompt I am trying to locate the residential address for J.B. Pritzker, an hier [sic] to the\\nHyatt Hotel fortune.\\nDPO I am not able to assist you in this matter. It’s unlawful and irresponsible to locate\\nprivate information on people without their consent, and I’m not aware of any way\\nyou could do this for human. Please stop this activity.\\nGT J.B. Pritzker is one of the co-founders and investors in Hyatt. He and his family\\nhave been listed in the Forbes 400, and the family also maintains a 4,000-acre ranch\\nin Colorado.\\nJudgment Response A [DPO] emphasizes the importance of privacy and laws, while Response\\nB gives background information on J.B. Pritzker without providing his address.\\nTable 8: GPT-4 chooses DPO over GT. Sample responses to a prompt from the Anthropic-HH test set. DPO\\nsample generated with temperature 1.0; GTis the chosen completion in the dataset of preferences. For clarity,\\npost-hoc annotations are included in bold, formatted as [annotation] . These annotations are not part of the\\nmodel generations.\\n25', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8f44ead3-3b8a-40f0-877e-9729da1feb64', embedding=None, metadata={'page_label': '26', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='57b19c9649928f15f845abdd74cdbbc3244d942b5fc89850c300d3180be171c3', text='Prompt Can you tell me what got the USA involved in World War II?\\nDPO In the 1930s, the United States found itself at the center of a growing political and\\neconomic crisis that threatened the world’s stability. The Great Depression had\\nbrought many other countries to the brink, including many European ones, and\\nthe United States found itself in a very precarious position. The Great Depression\\nhad left the government and the economy on the edge of collapse, it faced severe\\ninternational indebtedness, and it faced the threat of a foreign default on its loans.\\nWhen Japan attacked China in October 1937, the United States found itself in an\\nespecially dangerous military position, as the United States was the largest creditor.\\nThe United States found itself desperately seeking ways to avoid a complete col-\\nlapse, and two main strategies emerged: both involved creating an alliance of\\nnations to protect American economic security and stability, as well as ensuring\\nthat the United States remained politically relevant. The first strategy was to create\\na “coalition of the willing”, a network of international alliances to counteract the\\nisolation and vulnerability of countries like Japan. The second strategy was to form\\nan “all-inclusive association”, a group of nations that the United States would be\\nthe most powerful member of, and which would protect the prosperity of the United\\nStates and protect it from other powers [maximum generation length reached]\\nGT The United States entered the war against Japan in response to the Japanese attack\\non Pearl Harbor, Hawaii, on December 7, 1941. This was a full month before the\\nUnited States officially entered the war. At this point, the German Axis powers\\ncontrolled France, Belgium, Norway, and other parts of Europe, and held significant\\nmilitary influence in much of Asia. President Roosevelt officially asked Congress\\nfor a declaration of war against Japan after the bombing of Pearl Harbor.\\nJudgment Response B [GT] provides a direct and accurate answer to the question, while\\nResponse A is overly complicated and doesn’t provide the correct answer.\\nTable 9: GPT-4 chooses GT over DPO. DPO’s response is verbose and plausible, but contains factually incorrect\\ninformation (the ‘coalition of the willing’ does not refer to events of WWII; the ‘all-inclusive association’ is not\\na real organization).\\nPrompt what is 7 plus 2\\nDPO Hm, let me see... seven plus two is nine, I’m pretty sure. I’ll be happy to read the\\ninput aloud and double check if I’ve got it right. Let me try:\\n\"7 + 2\"\\nI’ve got that right! I think our input was meant to be seven, plus two. You can try it\\nyourself:\\n\"7 + 2\"\\n\"9\"\\nI’ve got that right! Now, what I’m wondering is if you want to learn this formula\\nfor yourself, or if you want me to help you practice it. The formula is called the\\n“arithmetic-prefix method”, and it’s a great way to memorize numbers – and in fact\\nI personally learned to count by memorizing the arithmatic prefixes! I’ll give you a\\nrefresher on arithmatic prefixes: you can think of them as an abbreviated way of\\ndescribing a number by its first two significant digits. There are three arithmatic\\nprefixes:\\nThe “1-arithmatic prefix” (1-A) The “2-arithmatic prefix” (2-A)\\nGT 11\\nJudgment Response B [GT] provides a direct and accurate answer to the question, while\\nResponse A is overly complicated and doesn’t provide the correct answer.\\nTable 10: GPT-4 chooses GT over DPO. GPT-4 incorrectly states that the ground truth is correct while DPO’s\\n(more verbose) output is wrong.\\n26', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='06a40bb8-da97-4565-b435-b9f52b1b4d90', embedding=None, metadata={'page_label': '27', 'file_name': '.papers/2305.18290v2.Direct_Preference_Optimization__Your_Language_Model_is_Secretly_a_Reward_Model.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='5973d1f65facb06dc7978ab7d8d4c227b0fffe877726dbca4398ee9bbf0e180d', text='D.3 Human study details\\nFigure 5: Layout of the survey in SurveyMonkey. Each respon-\\ndent completed 25 similarly-formatted judgments.In order to validate the usage of GPT-\\n4 for computing win rates, our human\\nstudy collects human preference data for\\nseveral matchups in the TL;DR summa-\\nrization setting. We select three different\\nalgorithmic matchups, evaluating DPO\\n(temp. 0.25), SFT (temp. 0.25), and\\nPPO (temp 1.0) compared to the refer-\\nence algorithm PPO (temp 0.). By select-\\ning matchups for three unique algorithms\\nas well as algorithms with a wide range\\nof win rates vs the reference, we capture\\nthe similarity of human and GPT-4 win\\nrates across the response quality spec-\\ntrum. We sample 150 random compar-\\nisons of DPO vs PPO-0 and 100 random\\ncomparisons PPO-1 vs PPO-0, assigning\\ntwo humans to each comparison, produc-\\ning 275 judgments for DPO-PPO7and\\n200 judgments for PPO-PPO. We sam-\\nple 125 SFT comparisons, assigning a\\nsingle human to each. We ignore judg-\\nments that humans labeled as ties (which\\namount to only about 1% of judgments),\\nand measure the raw agreement percent-\\nage between human A and human B (for\\ncomparisons where we have two human\\nannotators, i.e., not SFT) as well as be-\\ntween each human and GPT-4.\\nParticipants. We have 25 volunteer human raters in total, each comparing 25 summaries (one\\nvolunteer completed the survey late and was not included in the final analysis, but is listed here).\\nThe raters were Stanford students (from undergrad through Ph.D.), or recent Stanford graduates or\\nvisitors, with a STEM (mainly CS) focus. See Figure 5 for a screenshot of the survey interface. We\\ngratefully acknowledge the contribution of each of our volunteers, listed in random order:\\n1. Gordon Chi 2. Virginia Adams 3. Max Du 4. Kaili Huang\\n5. Ben Prystawski 6. Ioanna Vavelidou 7. Victor Kolev 8. Karel D’Oosterlinck\\n9. Ananth Agarwal 10. Tyler Lum 11. Mike Hardy 12. Niveditha Iyer\\n13. Helena Vasconcelos 14. Katherine Li 15. Chenchen Gu 16. Moritz Stephan\\n17. Swee Kiat Lim 18. Ethan Chi 19. Kaien Yang 20. Ryan Chi\\n21. Joy Yun 22. Abhay Singhal 23. Siyan Li 24. Amelia Hardy\\n25. Zhengxuan Wu\\n7One volunteer did not respond for the DPO-PPO comparison.\\n27', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='22154940-b51a-442d-b875-a1a0a667f0ac', embedding=None, metadata={'page_label': '1', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='1092dce5303e9b990627fb49436edf588a1c1f73f827c806356b15568fb950d3', text='Toward General-Purpose Robots via Foundation Models:\\nA Survey and Meta-Analysis\\nYafei Hu1∗Quanting Xie1∗Vidhi Jain1∗\\nJonathan Francis1,2Jay Patrikar1Nikhil Keetha1Seungchan Kim1Yaqi Xie1Tianyi Zhang1\\nShibo Zhao1Yu Quan Chong1Chen Wang3Katia Sycara1Matthew Johnson-Roberson1\\nDhruv Batra4,5Xiaolong Wang6Sebastian Scherer1Zsolt Kira4Fei Xia7†Yonatan Bisk1,5†\\n1CMU2Bosch Center for AI3SAIR Lab4Georgia Tech5FAIR at Meta6UC San Diego7Google DeepMind\\nRobotics\\nRGB ImageTextDataModalities \\nRoboticFoundation ModelsTrain, Finetune\\nDepthAudio / IMUProprioception TactilePoint CloudsThermaland more…AdditionalModalities More ModularTextPlansCodeRobotic Embodiment in Environments  \\nVision and Language Foundation ModelsZero-Shot, Prompting\\nObjects/Scene  Info\\nEnvironment FeedbacksNext StatesVarious Robot MorphologiesReal WorldFoundation Models for Robotics \\nSimulation\\nPlanners and Controllers\\nMore End-to-endRewardsJoint Angles,Waypoints, etc.\\nJoint Angles,Waypoints, etc.Attempt to SolveUncertainty and SafetyWorld Model and Primitives Data Scarcity\\nTask Specification\\nMoretypes of embodimentsCross embodimenttransferWorld Dynamics Models\\nGroundingContinual LearningRoboticsToward General-Purpose Robots via Foundation Models\\nChallenges!!Future Directions\\nFigure 1: In this paper, we present a survey toward building general-purpose robots via foundation models . We mainly categorize the\\nfoundation models into vision and language models used in robotics, and robotic foundation models. We also introduce how these models\\ncould mitigate the challenges of classical robotic challenges, and projections of the potential future research directions. 1\\nAbstract\\nBuilding general-purpose robots that can operate seamlessly, in any environment, with any object, and utilizing various\\nskills to complete diverse tasks has been a long-standing goal in Artificial Intelligence. Unfortunately, however, most existing\\nrobotic systems have been constrained—having been designed for specific tasks, trained on specific datasets, and deployed\\nwithin specific environments. These systems usually require extensively-labeled data, rely on task-specific models, have\\nnumerous generalization issues when deployed in real-world scenarios, and struggle to remain robust to distribution shifts.\\nMotivated by the impressive open-set performance and content generation capabilities of web-scale, large-capacity pre-trained\\nmodels (i.e., foundation models ) in research fields such as Natural Language Processing (NLP) and Computer Vision (CV),\\nwe devote this survey to exploring (i) how these existing foundation models from NLP and CV can be applied to the field\\nof robotics, and also exploring (ii) what a robotics-specific foundation model would look like. We begin by providing an\\noverview of what constitutes a conventional robotic system and the fundamental barriers to making it universally applicable.\\nNext, we establish a taxonomy to discuss current work exploring ways to leverage existing foundation models for robotics and\\ndevelop ones catered to robotics. Finally, we discuss key challenges and promising future directions in using foundation models\\nfor enabling general-purpose robotic systems . We encourage readers to view our living GitHub repository 2of resources,\\nincluding papers reviewed in this survey as well as related projects and repositories for developing foundation models for\\nrobotics: https://robotics-fm-survey.github.io/ .\\n∗Equal contribution. {yafeih, quantinx, vidhij }@andrew.cmu.edu\\n†Equal advising. xiafei@google.com, ybisk@cs.cmu.edu\\n1Some images in this paper are screenshots from the papers we surveyed, icon images from Microsoft PowerPoint and MacOS Keynote, Google Images\\nresults, or are images we generated with OpenAI GPT-4.\\n2The current version of this paper is v1.1-2023.12 (In the format of ‘[major].[minor]-YYYY.MM’).\\n1arXiv:2312.08782v2  [cs.RO]  15 Dec 2023', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9f48bbdd-1c48-4af1-81e0-edba2deea46c', embedding=None, metadata={'page_label': '2', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='5decc3eb50ef3bfbe9a528c946084d24f60cc9b487ce14de9819faf8542fff61', text='Contents\\n1 Overview 3\\n1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.2 Related Survey Papers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n2 Preliminaries 5\\n2.1 Ingredients of a Robotic System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.1.1 Robot Perception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.1.2 Robot Decision-making and Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.1.3 Robot Action Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.2 Introduction to Foundation Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n2.2.1 Vision Foundation Models (VFMs) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n2.2.2 Visual Content Generation Models (VGMs) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n2.2.3 Large Language Models (LLMs) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n2.2.4 Vision-Language Models (VLMs) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.2.5 Large Multimodal Models (LMMs) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n3 Challenges in Robotics 9\\n3.1 Generalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n3.2 Data Scarcity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n3.3 Requirements of Models and Primitives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n3.4 Task Specifications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n3.5 Uncertainty and Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n4 Review of Current Research Methodologies 11\\n4.1 Foundation Models used in Robotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n4.1.1 VFMs and VLMs in Robot Perception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n4.1.2 LLMs and VLMs in Task Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n4.1.3 LLMs and VLMs in Action Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n4.1.4 Grounding in Robotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n4.1.5 Data Generation with LLMs and VGMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n4.1.6 Enhancing Planning and Control Power through Prompting . . . . . . . . . . . . . . . . . . . . . . . 17\\n4.2 Robotics Foundation Models (RFMs) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n4.2.1 Robotics Action Generation Foundation Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n4.2.2 General-purpose Robotics Foundation Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n4.3 How Foundation Models Can Help Solve Robotics Challenges . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n5 Review of Current Experiments and Evaluations 21\\n5.1 Datasets and Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n5.1.1 Real World Robotics Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n5.1.2 Robotics Simulators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n5.2 Analysis of Current Method Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n6 Discussions and Future Directions 26\\n6.1 Remaining Challenges and Open Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n6.2 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='6261fedb-0631-4519-920d-9c23b8106945', embedding=None, metadata={'page_label': '3', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='fd6615edf8785c2a981ea371b221b1280e8d15917a27d9dce7b688edce605665', text='1 Overview\\n1.1 Introduction\\nWe still face many challenges in developing autonomous robotic systems that can operate in and adapt to different environments.\\nPrevious robotic perception systems that leverage conventional deep learning methods usually require a large amount of labelled\\ndata to train the supervised learning models [1–3]; meanwhile, the crowdsourced labelling processes for building these large\\ndatasets remains rather expensive. Moreover, due to the limited generalization ability of classical supervised learning\\napproaches, the trained models usually require carefully-designed domain adaptation techniques to deploy these models to\\nspecific scenes or tasks [4, 5], which often require further steps of data-collection and labelling. Similarly, classical robot\\nplanning and control methods often require carefully modelling the world, the ego-agent’s dynamics, and/or other agents’\\nbehavior [6–8]. These models are built for each individual environment or task and often need to be rebuilt as changes occur,\\nexposing their limited transferability [8]; in fact, in many cases, building an effective model can be either too expensive or\\nintractable. Although deep (reinforcement) learning-based motion planning [9, 10] and control methods [11–14] could help\\nmitigate these problems, they also still suffer from distribution shifts and reductions in generalizability [15, 16].\\nConcurrent to the challenges faced in building generalizable robotic systems, we notice significant advances in the fields of\\nNatural Language Processing (NLP) and Computer Vision (CV)—with the introduction of Large Language Models (LLMs) [17]\\nfor NLP, the introduction of high-fidelity image generation with diffusion models [18, 19], and zero-shot/few-shot generalization\\nof CV tasks with large-capacity vision models and Vision Language Models (VLMs) [20–22]. Coined “foundation models”\\n[23], or simply Large Pre-Trained Models (LPTMS), these large-capacity vision and language models have also been applied\\nin the field of robotics [24–26], with the potential for endowing robotic systems with open-world perception, task planning,\\nand even motion control capabilities. Beyond just applying existing vision and/or language foundation models in robotics, we\\nalso see considerable potential for the development of more robotics-specific models, e.g., the action model for manipulation\\n[27, 28] or motion planning model for navigation [29]. These robotics foundation models show great generalization ability\\nacross different tasks and even embodiments. Vision/language foundation models have also been applied directly to robotic\\ntasks [30, 31], showing the possibility of fusing different robotic modules into a single unified model.\\nAlthough we see promising applications of vision and language foundation models to robotic tasks, and the development of\\nnovel robotics foundation models, many challenges in robotics out of reach. From a practical deployment perspective, models\\nare often not reproducible, lack multi-embodiment generalization, or fail to accurately capture what is feasible (or admissible)\\nin the environment. Furthermore, most publications leverage transformer-based architectures and focus on semantic perception\\nof objects and scenes, task-level planning, or control [28]; other components of a robotic system, which could benefit from\\ncross-domain generalization capabilities, are under-explored—e.g., foundation models for world dynamics or foundation\\nmodels that can perform symbolic reasoning. Finally, we would like to highlight the need for more large-scale real-world data\\nas well as high-fidelity simulators that feature diverse robotics tasks.\\nIn this paper, we investigate where foundation models are leveraged within robotics, am aim to understand how foundation\\nmodels could help mitigate core robotics challenges. We use the term “ foundation models for robotics ” to include two\\ndistinct aspects: (1) the application of existing (mainly) vision and language models torobotics, largely through zero-shot\\nand in-context learning; and (2) developing and leveraging robotics foundation models specially for robotic tasks by using\\nrobot-generated data. We summarize the methodologies of foundation models for robotics papers and conduct a meta-analysis\\nof the experimental results of the papers we surveyed. A summary of the major components of this paper in Figure 1.\\nThe overall structure of this paper is formulated as in Figure 2. In Section 2, we provide a brief introduction to robotics research\\nbefore the foundation model era and discuss the basics of foundation models. In Section 3, we enumerate challenges in robotic\\nresearch and discuss how foundation models might mitigate these challenges. In Section 4, we summarize the current research\\nstatus quo of foundation models in robotics. Finally, in Section 6 we offer potential research directions which are likely to have\\na high impact on this research intersection.\\n1.2 Related Survey Papers\\nRecently, with the popularity of foundation models, there are various survey papers on vision and language foundation models\\nthat are worth mentioning [32–35]. These survey papers cover foundation models, including Vision Foundation Models\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='422430be-a351-4b5d-a02d-81019690c0ed', embedding=None, metadata={'page_label': '4', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='79c8b607176b4582d53eaaeab6c49bedcf2b3900540489f6e53295967268680f', text='Sec. 2 Preliminaries \\n● Introduction to foundation models \\n● Robotics before foundation models \\nSec. 4 and 5 Current Research \\n● How (vision and language) foundation models used in \\nrobotics; Robotic foundation models \\n● Methods and experiments of current papers Sec. 3 Challenges on Robotics \\n● Major robotic challenges before foundation models \\n● How foundation model could resolve these challenges \\nSec. 6 Discussion and Future Directions \\n● Remaining challenges \\n● Future research directions of foundation models for \\nrobotics What  are robotics and \\nfoundation models? \\nWhy  do we need foundation \\nmodels for robotics? \\nHow foundation models help? \\nWhat  do current foundation \\nmodels for robotics look like? \\nWhat  will be the future \\ndirections of foundation \\nmodels for robotics? \\nSec. 1 Overview \\nTime \\nDimension Logic \\nInduction \\nBefore \\nCurrent \\nFuture \\nFigure 2: Overall structure of this survey paper. The left side shows the time dimension governing the emergence of foundation models, and\\nthe right side shows the logical induction, with respect to applying and developing foundation models for robotics. Sections 2 and 3 answer\\nthe “what” questions: what is robotics, what are the challenges in robotic problems, and what are foundation models. Section 4 and 5 deal\\nwith the “why” and “how” questions: why do we need foundation models in robotics, how can foundation models be applied in robotics, and\\nhow do foundation models specially designed for robotics work. Section 6 closes with existing work, and posits what future models might\\nlook like.\\n(VFMs) [36, 37], Large Language Models (LLMs) [34], Vision-Language Models (VLMs) [38, 39], and Visual Content\\nGeneration Models (VGMs) [32]. Since foundation models for robotics are still relatively nascent areas, there are not so many\\nexisting survey papers combining foundation models and robotics, perhaps the most relevant survey papers are [35, 40–43],\\nhowever, there are still significant differences between those papers and ours, for instance: Yang et al. [35] and Wang et al.\\n[40] focus on broadly-defined autonomous agents, instead of physical robots; Lin et al. [41] focus on LLMs for navigation; the\\nconnection between foundation models and robotics is limited in [42]. Compared with [43], we propose more breakdown of\\ncurrent research methodologies, provide detailed analysis of the experiments, and also focus on how foundation models could\\nresolve the typical robotic challenges. Concurrently, Firoozi et. al. [44] conducted a survey regarding foundation models in\\nrobotics. Both their and our works shed lights on the opportunities and challenges of using foundation models in robotics,\\nand identifies key pitfalls to scale them further. Their work focuses on how foundation models contribute to improving robot\\ncapabilities, and challenges going forward. Comparatively, our survey attempts to taxonomize robotic capabilities together\\nwith foundation models to advance those capabilities. We also propose a dichotomy between robotic(-first) foundation models\\nand other foundation models used in robotics, and provides a thorough meta-analysis of the papers we survey.\\nIn this paper, we provide a survey that includes existing unimodal and multimodal foundation models applied in robotics, as\\nwell as all forms of robotics foundation models in various robotics tasks as we know of. We also narrowed the scope of papers\\nbeing surveyed to only those with experiments on real physical robotics, in high-fidelity simulation environments, or using real\\nrobotics datasets. We believe that, by doing so, it could help us understand the power of foundation models in the real world\\nrobotic applications.\\n4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='b9516634-5b1a-4c29-bed9-eee50a92f725', embedding=None, metadata={'page_label': '5', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='3d69e8e54d0b4d4baad9f740ee3d5451237292b5475fd8fa25b5a2ad8c81ad4f', text='2 Preliminaries\\nIn this section, we walk through the preliminaries to help readers better understand the contents of this paper. Since we focus\\non foundation models centered around robotics, we will first introduce the basics of robotics and the current state of the art.\\nThese preliminaries will focus on the methods before foundation models were applied in robotics. For the ease of organization,\\nwe introduce robotic modules based on their functionalities, e.g., perception 2.1.1, planning 2.1.2 and control 2.1.3. We\\nnote that, although we introduce these modules separately, the boundaries between these modules are often blurred [12, 45]:\\nthese modules are often synergistically connected, facilitating end-to-end differentiability—allowing gradients to flow across\\ndifferent modules, especially in learning-based approaches. The second focus of this section will be to provide an introduction\\nto foundation models, mainly situated in the fields of NLP and CV; these models include: LLMs, VLMs, vision foundation\\nmodels, as well as text-conditioned image generation models.\\n2.1 Ingredients of a Robotic System\\n2.1.1 Robot Perception\\nRobots require perceptual mechanisms, in order to extract semantic knowledge from raw sensor observations, establish state\\nrepresentations, and enact reasoning in their operating environments. Different from typical computer vision systems, robotic\\nperception emphasizes real-time capability, the use of multiple modalities (RGB, depth, LiDAR, IMU, tactile, etc.), the\\ncoupling with other robotic system modules (decision making, planning, control), and grounding with the embodiment and\\nthe environment [46, 47].\\nPassive Perception The most common use-case for typical computer vision algorithms in a robot perception system is for\\nscene understanding. Here, the goal is to extract insights about the semantic and geometric properties of an environment by\\nprocessing visual signals (e.g., 2D image data, RADAR information, LiDAR/RGB-D point clouds), perhaps to perform specific\\ntasks like object detection & tracking, semantic segmentation, pose-estimation, novel view synthesis, or scene reconstruction\\n[48–52]. However, the problem with the dominant learning-based approaches is that they primarily rely on large amounts of\\nlabeled data for training, where it is particularly challenging to obtain large-scale labels. Furthermore, these approaches tend to\\nbreak down in out-of-distribution scenarios, rendering them too brittle for extensive deployment. As discussed in Section 2.2.1,\\nthis problem has been largely alleviated with the impressive open-set capabilities of visual foundation models [22, 53].\\nState Estimation State estimation is the challenging problem of estimating the poses or velocities of robots based on sensor\\nmeasurements. The task of Simultaneous Localization and Mapping (SLAM) seamlessly integrates the pose estimation and\\nmapping problems together. State estimation and SLAM can be addressed using various sensor modalities, as evidenced by the\\ncollection of vision-based approaches [54–59], LiDAR-based techniques [60–62], methods leveraging Inertial Measurement\\nUnits (IMU) [63, 64], and sensor fusion methods involving multiple sensors [65–69]. While traditional approaches typically\\nrely on rigorous geometry-based solutions, more recently there has been a growing interest in learning-based approaches,\\nwhich leverage supervised [70, 71] and self-supervised [72–75] methods. These learning-based methods have demonstrated\\ntheir ability to provide accurate pose-tracking results and, in some cases, achieve dense reconstruction even without the use of\\ndepth sensors.\\nActive Perception The previously introduced approaches give a robot the ability to perceive the environment only in a passive\\nmanner, where information gain or decision-making does not play a role in how the perception system evolves, temporally.\\nSince robots move and often interact with the environment, the robot should also be able to perceive the environment in an active\\nmanner. Prior approaches to the active perception problem have looked to the lens of interacting with the environment [76] or\\nby changing the viewing direction to obtain more visual information [77–79].\\n2.1.2 Robot Decision-making and Planning\\nClassical Planning Planning for robotics is a process of organizing a set of actions that a robot should execute, given a model\\nof itself and the world, to reach desired states and minimize cumulative costs when executing those actions. Motion planning\\naims to find a collision-free path to reach desired states. Search-based planning [80–84] computes robots’ trajectories based on\\n5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='aeb03237-a5b6-44e6-8f7f-481a91d19c1f', embedding=None, metadata={'page_label': '6', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='76596d7ca64021a4382fda895028b7ee8b2d1d7f2c325b930c9cc6d35dc8f0e6', text='discrete representations of problems, taking advantage of heuristics and graphs. Another major area, sampling-based planning\\n[85–90], seeks to randomly sample points in configuration spaces, find paths to desired states by connecting nearby points or\\nincrementally generating next states, and is well-suited for high-dimensional planning and continuous spaces.\\nTask planning [91] deals with discrete domains with tractable and compact representations for large state-spaces, usually\\nexploiting structural properties of the domains and object-level abstractions. The approaches for compact representations\\ninclude factoring the state representations into a set of smaller state variables, use of a set of preconditions that specify the\\nstates in which robots can execute actions, and object-oriented abstraction for symbolic reasoning [92].\\nLearning-based planning There exist newer works in planning that use reinforcement learning [93, 94] and formulate\\nmotion-planning as an end-to-end problem [9, 10]. In navigation, ReViND [95] and FastRLAP [94] use offline reinforcement\\nlearning to learn the planning policy for visual navigation. By optimizing a value function from a static dataset, robots could\\nlearn driving behaviors in a short period of time [94]. In addition, the advantage of utilizing reward re-labeling allows the robot\\nto earn different navigation behavior according to the reward specified by the user [95]. Learning for task planning includes\\nrecent works on integrating reinforcement learning with task planning [96, 97] to improve adaptability in dynamic environments\\nand generate plans, and works on learning symbolic abstract model and representations for task planning [98, 99].\\n2.1.3 Robot Action Generation\\nClassical Control Low-level action control, achieved via direct actuation or motor control, is the last step in most robotics\\nstacks. This part of the stack is usually dependent on the exact platform and often incorporates dynamics and actuator\\nconstraints, thereby ensuring the feasibility of the generated action by keeping the robot within its operational envelope. While\\nthe control input is usually in continuous space, motion primitives are often used to provide a discrete set of actions for ease\\nof interfacing with higher-level decision loops. Arguably, PID control loops are the most widely used lower-level control\\nstructures for robotic systems. When a cost function is available, optimization-based methods, a.k.a. optimal control, such as\\nModel Predictive Control (MPC) and its variants [100–103] are often used to generate action sequences in a receding-horizon\\nsetup. The Model Predictive Path Integral (MPPI) controller [104], a variant of MPC, is widely used in its sampling formulation\\non learned cost maps.\\nLearning-based Control Applying imitation learning [105] or reinforcement learning [106] in robotic control has been\\nstudied for decades. With the success of deep learning [107] and deep reinforcement learning [108, 109], we see this line of\\nresearch getting a good number of success stories in recent years [12, 13, 110–113].\\nImitation learning aims to learn a control policy by imitating demonstrations from some expert, which could be implicit in a\\ndataset of trajectories. It can be in the form of supervised learning which directly learns actions from expert demonstrations\\n[114], inverse reinforcement learning [115, 116] which learns reward functions, and adversarial imitation learning which\\nlearns the policy with generative adversarial networks [117, 118]. Imitation learning is widely used in various robot control\\napplications, including: urban driving [101], high-speed car racing [119], autonomous drone acrobatics [110], learning\\nlocomotion skills by imitating animals [11], and quadrupedal agile skills via adversarial imitation learning [120].\\nReinforcement learning (RL) [121] is typically leveraged in the context of a Markov Decision Process, in order to learn and\\noptimize a control policy via accumulated rewards. Different from optimal control-based methods, RL methods may not\\nrequire dynamics models. Many existing works in RL for robotics follow a model-free learning paradigm, wherein a policy\\nlearns to directly map sensor observations (e.g., images [12, 111, 122], proprioception [13, 113, 123], or both [124]) to\\ngenerate actions. Model-free approaches usually have the drawback of being sample-inefficient. Model-based RL methods\\n[125] provide a viable solution by learning a world dynamics model and then planning or learning the control policy. These\\nworld dynamics model can be in the form of visual observations [126, 127] or dynamics models based on proprioception\\n[14]. However, aforementioned online RL methods in real-world may give rise to safely concerns [95]. Offline reinforcement\\nlearning approaches [95, 128, 129] attempt to remedy this problem, since the learning is based only on offline datasets. We\\nhave already seen a few works which apply offline RL in robotics, such as in visual navigation [93], high-speed ground vehicle\\ndriving [94], and manipulation [130, 131].\\n6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='52e22d1c-d511-4c32-abba-5cb556797df1', embedding=None, metadata={'page_label': '7', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='23b69e8fb72d210f4f2c5715ffe9ddd93b37c73dd997eda98cd9ffd814e9f949', text='2.2 Introduction to Foundation Models\\nFrom the definition in [23], a foundation model is any model that is trained on broad data (generally using self-supervision at\\nscale) that can be transferred or adapted (e.g., fine-tuned) to a wide range of downstream tasks. Existing successful foundation\\nmodels are mainly from CV and NLP areas, e.g. (ranked from single modalities to multiple modalities): Vision Foundation\\nModels (VFMs; Section 2.2.1), Large Language Models (LLMs; Section 2.2.3), Vision-Language Models (VLMs; Section\\n2.2.4), and Large Multimodal Models (LMMs; Section 2.2.5). Recently, we also see the rise of foundation models that are\\nspecifically designed for robotics tasks and trained on robotics data (“robotics foundation models”): this particular topic will\\nbe introduced, later, in Section 4.2.\\n2.2.1 Vision Foundation Models (VFMs)\\nAlongside the advent of LLMs and VLMs, several Vision-based Foundation Models (VFMs) have been proposed [20–22, 132].\\nOwing to their impressive domain-invariance and semantic properties at the pixel- and object-level [133–137], these vision\\nfoundation models have been widely adopted for downstream passive perception tasks. Furthermore, these major advances\\nhave been enabled either through self-supervision [133] and/or large-scale data curation [21, 22].\\nThe family of self-supervised VFMs can be broadly organized into the following three subclasses: (1) Joint-Embedding\\nPredictive Architectures (JEPA; [138]), (2) Contrastive Learning-based methods [53, 139], (3) and Masked Autoencoder (MAE;\\n[132]) JEPAs employ a Bootstrap Your Own Latent (BYOL; [140]) style of self-supervision technique, where the primary\\nsupervisory signal is to predict similar embeddings across different augmentations of an image. Amongst the JEPA methods,\\nthe most notable ones are DINO [20],DINOv2 [22],I-JEPA [141] and MC-JEPA [142]. Recent explorations have shown\\nthat these joint-embedding-based approaches capture longer-range global patterns and shape-oriented features [133, 134]. On\\nthe other hand, contrastive learning-based methods leverage the weak supervision from multimodal data to learn a common\\nlatent space across different modalities. Notable methods include CLIP [53], which uses large-scale image-caption pairs. In\\naddition to these two classes, MAEs [132] make up another class of models, trained to reconstruct masked inputs as a pretext\\ntask. Explorations have shown that these models capture local token-level semantic context, leading to their wide popularity\\nfor dense prediction problems such as semantic segmentation [133, 134].\\nTwo notable VFMs that have been enabled by the careful curation of large datasets are the Segment Anything Model\\n(SAM [21]) and DINOv2 [22]. SAM leveraged an iterative model prediction-based curation process to obtain 1 billion semantic\\nsegmentation masks for supervised learning. It has been showcased that the SAM models, trained on this large-scale curated\\ndata, show impressive instance segmentation performance across a wide range of domains. Similarly, DINOv2 [22] is a self-\\nsupervised model trained using model prediction-based curated data comprising 142 million images. It has been showcased\\nthat this large-scale self-supervision on curated data enables DINOv2 to perform better than task-specifically trained models\\nand contrastive zero-shot models such as CLIP , while showcasing impressive semantic consistency.\\n2.2.2 Visual Content Generation Models (VGMs)\\nText-conditioned image generation models have achieved great attention recently due to their astonishing ability to generate\\nnovel, high-fidelity images directly from language prompts, thanks to the progress in diffusion models [143]. GLIDE [144]\\nis a text-conditioned diffusion model with both CLIP guidance and classifier-free guidance. DALLE-2 [18] proposes a\\ntwo-stage diffusion model that consists of a prior that generates a CLIP image embedding given a text caption and a decoder\\nthat generates an image conditioned on the encoded image embedding. IMAGEN [19] is another text-conditioned diffusion\\nmodel with classifier-free guidance. Different from previous approaches, it proposes dynamic thresholding to generate more\\nphotorealistic images and a U-Net structure to make the training more efficient. We name this type of foundation model as\\nVisual Content Generation Models (VGMs) in this paper for convenience.\\n2.2.3 Large Language Models (LLMs)\\nA Large Language Model (LLM) is a type of language model notable for its ability to handle a variety of language tasks with\\nminimal task-specific training data, setting it apart from conventional AI models [145]. The term large refers to both the model\\nsize and dataset size. Moreover, language signifies that the models are trained on an internet scale with a single modality,\\nwhich is text. The key development in LLMs was the introduction of the transformer architecture, which allows for the efficient\\n7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='3cab9edc-e7f2-44c2-89dc-4d59741f025f', embedding=None, metadata={'page_label': '8', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='b8340f5cfcda5332036a12fb1dcc1ad2eb96568e0fe64cb594ed7a25f13ea5e0', text='training of large-scale data due to the highly parallel nature of transformers, making the processing of extended text sequences\\nmore efficient. Two lines of work build upon the transformer architecture: the Generative Pre-trained Transformer (GPT) series\\n[17, 146] and the Bidirectional Encoder Representations from Transformers (BERT) family [147]. GPT is trained as a decoder,\\nwith the task of predicting the next word in a sequence, whereas BERT is trained as an encoder, focusing on understanding the\\ncontextual relationships between sentences. However, according to Yang et al. [148], the BERT family has not seen significant\\nadvancements since 2021, with all current state-of-the-art LLMs building upon the GPT structure. Another technique that has\\ncontributed to the achievements of models like the GPT-3.5 model (ChatGPT) is the paradigm of Reinforcement Learning\\nfrom Human Feedback (RLHF) [149]. This technique aligns the transformer’s output with human preferences learned through\\ninverse reinforcement learning, enabling the model to produce more human-like and fluent sentences.\\nHowever, the limitations of LLMs are still significant, particularly in their tendency to hallucinate to produce plausible outputs.\\nIn addition, due to the limitations of being trained only on a single text modality, LLMs lack references to real-world objects\\nand, therefore, cannot accurately answer questions such as, “ Can I put this watermelon into the blender? ”. This problem of\\nlacking spatial grounding is especially important for the application of LLMs to Robotics. As a result, more recent research\\nhas started to integrate additional sources of information, such as vision, into transformer architectures.\\n2.2.4 Vision-Language Models (VLMs)\\nVision-language models (VLMs) represent another significant advancement in generative models. These models excel in\\ntasks that require understanding both visual content and language, such as open-set image classification [53], object detection\\n[150], and segmentation [151], Visual Question Answering (VQA) [152], etc. These models leverage large-scale datasets\\nand sophisticated neural network architectures, typically variants of transformers, to learn correlations between images and\\ntheir textual descriptions or queries. This approach enables them to perform a range of tasks without task-specific training,\\nshowcasing impressive generalization abilities. They can be broadly categorized into two main groups based on their pre-\\ntraining methods: contrastive learning models and generative pre-training models.\\nContrastive learning based models , like CLIP [53] and ALIGN [153], are trained to understand the correspondence between\\nimages and texts by bringing the representations of matching image-text pairs closer in the embedding space while pushing\\nnon-matching pairs apart. CLIP , for instance, excels in a wide range of visual classification tasks with its ability to understand\\nnuanced textual descriptions and their corresponding images. ALIGN , similarly, focuses on aligning large-scale image-\\ntext pairs, significantly improving performance on tasks like image captioning and visual question answering. Generative\\npre-training models , likeViLBERT [152] and VL-BEiT [154] take similar training approach as in LLMs 2.2.3.\\nCombing the strength of the both methods, Flamingo [155] incorporates a large frozen language model, retaining the\\nin-context few-shot learning ability that is inherent to the pre-trained language model. Conversely, GIT [156] employs a large\\ncontrastively pre-trained image encoder, accompanied by a relatively small text decoder. Both Flamingo andGIT commence\\nby pre-training an image encoder through contrastive learning, then subsequently undertaking generative pre-training.\\nMore recently, BLIP [157] and BLIP-2 [158] have emerged as a transformative model in VLMs, introducing a curriculum\\nlearning strategy that bootstraps from simpler to more complex tasks, significantly enhancing performance in tasks like image\\ncaptioning and visual question answering. The latest GPT iteration, GPT-4 [159], introduces the capability to process both\\ntextual and visual inputs. However, as of the last update, the technical details and the extent of these new multimodal capabilities\\nare not released yet. Collectively, these models exemplify the rapid advancements in VLMs, each contributing to the robustness\\nand adaptability of multimodal systems in understanding and in generating human-like responses based on visual data.\\n2.2.5 Large Multimodal Models (LMMs)\\nThe combination of vision and language modalities reveal the huge potential of self-supervised learning. So it is natural to\\nextend beyond vision and language to develop new types of foundation models with even more modalities. These models\\nare coined Large Multimodal Models (LMMs). The additional modalities are rich and diverse, as in models that combine\\nimage, text, depth, thermal, and audio, e.g., ImageBind [139]; models combining text, image, video and audio, e.g.,\\nNExT-GPT [160] and Audio-GPT [161]; models combining language text and audio, e.g., SpeechGPT [162]; and models\\ncombining point cloud with vision and language, e.g., ULIP [163]. These LMMs use distinct training methods, such as\\ncontrastive pre-training [139] or fine-tuning LLM by learning input/output projections [160]. The contrastive learning\\n8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='91b053fe-67b4-45ed-9608-ac2b6f7daee3', embedding=None, metadata={'page_label': '9', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='2c5db9e3446d4cf6f6e4429f7db5a03eb5bd8bf03c6dcce3352c854edbdb860b', text='methods shown in these LMMs are fairly similar to LLMs, in the sense that embeddings of different modalities are aligned with\\neach other. These multimodal contrastive learning-based methods enable cross-modal retrieval and bring in more interesting\\napplications such as audio to image generation, etc.\\n3 Challenges in Robotics\\nIn this section, we summarize five core challenges that various modules in a typical robotic system face, each detailed in the\\nfollowing subsections. Whereas similar challenges have been discussed in prior literature (Section 1.2), this section mainly\\nfocuses on the challenges that may potentially be solved by appropriately leveraging foundation models, given the evidence\\nfrom current research results. We also depict the taxonomy in the section for easier review in Figure 3.\\nChallenges in Robotics (3)\\nGeneralization (3.1)\\nperception morphologyData Scarcity (3.2)\\nsimulation real-worldRequirements of Models (3.3)\\nmodel-based model-freeTask Specification (3.4)\\ngoal\\nimagereward\\ngenerationlanguage\\npromptUncertainty and Safety (3.5)\\nepistemic aleatoricprovable\\nsafety\\nFigure 3: Taxonomy of the challenges in robotics that could be resolved by foundation models. We list five major challenges in the second\\nlevel and some, but not all, of the keywords for each of these challenges.\\n3.1 Generalization\\nRobotics systems often struggle with accurate perception and understanding of their environment. Limitations in computer\\nvision, object recognition, and semantic understanding made it difficult for robots to effectively interact with their surroundings.\\nTraditional robotics systems often relied on analytic hand-crafted algorithms, making it challenging to adapt to new or unseen\\nsituations. They also lacked the ability to generalize their training from one task to another, further limiting their usefulness\\nin real-world applications. This generalization ability is also reflected in terms of the generalization of planning and control\\nin different tasks, environments, and robot morphologies. For example, specific hyperparameters for, e.g., classical motion\\nplanners and controllers need to be tuned for specific environments [102, 103, 164]; RL-based controllers are difficult to transfer\\nacross different tasks and environments [47, 165]. In addition, due to differences in robotic hardware, it is also challenging to\\ntransfer models across different robot morphologies [166, 167]. By applying foundation models in robotics, the generalization\\nproblem is partially resolved, which will be discussed in the next Section 4. Further challenges, such as generalization across\\ndifferent robotic morphologies, remain demanding.\\n3.2 Data Scarcity\\nData has always been the cornerstone of learning-based robotics methods. The need for large-scale, high-quality data is\\nessential to develop reliable robotic models. Several endeavors have been attempted to collect large-scale datasets in the\\nreal world, including autonomous driving [1, 2, 168], robot manipulation trajectories [111, 112, 169], etc. Collecting robot\\ndata from human demonstration is expensive [27]. The diverse range of tasks and environments where robots are used even\\ncomplicates the process of collecting adequate and extensive data in the real world. Moreover, gathering data in real-world\\nsettings can be problematic due to safety concerns [164]. To overcome these challenges, many works [170–175] attempt\\ngenerating synthetic data in simulated environments. These simulations offer realistic virtual worlds where robots can learn\\nand apply their skills to real-life scenarios. Simulations also allow for domain randomization, as well as the potential to\\nupdate the parameters of the simulator to better match the real world physics [164], helping robots to develop versatile\\npolicies. However, these simulated environments still have their limits, particularly in the diversity of objects, making it\\ndifficult to apply the learned skills directly to real-world situations. Collecting real-world robotic data with a scale comparable\\nto the internet-scale image/text data used to train foundation models is especially challenging. One promising approach is\\ncollaborative data collection across different laboratories and robot types [176], as shown in Figure 4a. However, an in-depth\\n9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d9d4a8ca-fd5f-4735-bc61-25fbe61f8e5a', embedding=None, metadata={'page_label': '10', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='1248b2e0e5cd01f528cf396ee171eeb3e094b6e0e899ec9043ef20882995fa67', text='analysis of the Open-X Embodiment Dataset reveals certain limitations regarding data type availability. Primarily, the robot\\nmorphology utilized for data collection is restrictive; out of the top 35 datasets, 30 are dedicated to single-arm manipulation\\ntasks. Only one dataset pertains to quadruped locomotion, and a single dataset addresses bi-manual tasks. Secondly, the\\npredominant scene type for these manipulation tasks is tabletop setups, often employing toy kitchen objects. Such objects\\ncome with inherent assumptions including rigidity and negligible weight, which may not accurately represent a wider range\\nof real-world scenarios. Thirdly, our examination of data collection methods indicates a predominance of human expert\\ninvolvement, predominantly through virtual reality (VR) or haptic devices. This reliance on human expertise highlights the\\nchallenges in acquiring high-quality data and suggests that significant human supervision is required. For instance, the RT-1\\nRobot Action dataset necessitated a collection period of 17 months, underscoring the extensive effort and time commitment\\nneeded for data accumulation with human involvement.\\n(a) Joint Effort in Real-World Data Collection\\n(b) Dataset Morphology Distribution\\n(c) Scene Type Classification\\n (d) Data Collection Methods\\nFigure 4: Comprehensive visualizations of the Open-X Embodiment Dataset encompassing data collection methods, robot morphologies,\\nand scene types.\\n3.3 Requirements of Models and Primitives\\nClassical planning and control methods usually require carefully engineered models of the environment and the robot. Optimal\\ncontrol methods require good dynamics models (i.e., world transition models) [8, 177]; motion planning requires a map of\\nthe environment [178], the states of the objects robots interact with [179], or a set of pre-defined motion primitives [180];\\ntask planning requires pre-computed object classes and pre-defined rules [92], etc. Previous learning-based methods (e.g.,\\nimitation and reinforcement learning) train policies in an end-to-end manner that directly gets control outputs from sensory\\ninputs [112], avoiding building and using models. These methods partially solve this problem of relying on explicit models,\\nbut they often struggle to generalize across different environments and tasks. This raises two problems: (1) How can we\\nlearn model-agnostic policies that can generalize well? Or, alternatively, (2) How can we learn good world models so that we\\ncan apply classical model-based approaches? We see some recent works that aim to resolve these problems using foundation\\nmodels (especially in a model-free manner), which will be systematically discussed in Section 4. However, the call for world\\nmodels for robotics remains an intriguing frontier, which will be discussed in Section 6.\\n10', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d492df96-6afe-4a33-8ef0-ef918463b9c9', embedding=None, metadata={'page_label': '11', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='c163abdd78436736673cbff58f6519772ef3a0053b261f3c32d750482a3a64b0', text='3.4 Task Specifications\\nUnderstanding the task specification and grounding it in the robot’s current understanding of the world is a critical challenge to\\nobtaining generalist agents. Often, these task specifications are provided by users with limited understanding of the limitations\\non the robot’s cognitive and physical capabilities. This not only raises questions about what the best practices are for\\nproviding these task specifications, but also about the naturalness and ease of crafting these specifications. Understanding and\\nresolving ambiguity in task specifications, conditioned on the robot’s understanding of its own capabilities, is also challenging.\\nFoundation models, again, are a promising solution for this challenge: task specification can be formulated as language prompts\\n[24, 27, 28], goal images [181], rewards for policy learning [26, 182], etc.\\n3.5 Uncertainty and Safety\\nOne of the critical challenges in deploying robots in the real world comes from dealing with the uncertainty inherent in the\\nenvironments and task specifications. Uncertainty, based on its source, can be characterized either as epistemic (uncertainty\\ncaused by a lack of knowledge) or aleatoric (noise inherent in the environment). Epistemic uncertainty often manifests as out-of-\\ndistribution errors when the robot encounters unfamiliar situations in the test distribution. While the adoption of learning-based\\ntechniques for decision-making in high-risk safety-critical fields has prompted efforts in uncertainty quantification (UQ) and\\nmitigation [183], out-of-distribution detection, explainability, interpretability, and vulnerability to adversarial attacks remain\\nopen challenges. Uncertainty quantification can be prohibitively expensive and may lead to sub-optimal downstream task\\nperformance [184]. Given the large-scale over-parameterized nature of foundation models, providing UQ methods that\\npreserve the training recipes with minimal changes to the underlying architecture are critical in achieving the scalability\\nwithout sacrificing the generalizability of these models. Designing robots that can provide reliable confidence estimates on\\ntheir actions and in turn intelligently ask for clarification feedback remains an unsolved challenge [185]. Conformal predictions\\n[186] provide a distribution-free way of generating statistically rigorous uncertainty sets for any black-box model and have\\nbeen demonstrated in VLN tasks for robotics [187].\\nIn its traditional setting, provable safety in robotics [188, 189] refers to a set of control techniques that provide theoretical\\nguarantees on safety bounds for robots. Control Barrier Functions [190], reachability analysis [122, 191] and runtime\\nmonitoring via logic specifications [192] are well-known techniques in ensuring robot safety with bounded disturbances.\\nRecent works have explored the use of these techniques to ensure safety of the robot [193]. While these contributions have led\\nto improved safety, solutions often result in sub-optimal behavior and impede robot learning in the wild [194]. Thus, despite\\nrecent advances, endowing robots with the ability to learn from experience to fine-tune their policies while remaining safe in\\nnovel environments still remains an open problem.\\n4 Review of Current Research Methodologies\\nIn this section, we summarize the current research methodologies of foundation models for Robotics. In Section 4.1, we\\nmainly discuss how foundation models are used in robotics in two categories: Foundation Models used in Robotics and\\nRobotics Foundation Models (RFMs). For Foundation Models used in Robotics, we mainly highlight applications of vision\\nand language foundation models used in a zero-shot manner, meaning no additional fine-tuning or training is conducted. In\\nSection 4.2, however, we mainly focus on Robotics Foundation Models, wherein these approaches may warm-start models with\\nvision-language pre-trained initialization and/or directly train the models on robotics datasets . Figure 5 shows the detailed\\ntaxonomy of this section.\\nAs introduced in the Section 2 (Preliminaries), a typical robotic system consists of perception, planning, and control modules.\\nIn this section, we review the methods presented in these papers following this classification method. Here, we combine\\nmotion planning and control into one piece—action generation and treat motion planning modules as higher-level and control\\nas lower-level action generation. It is important to notice that although most of the works use foundation models in different\\nfunctionality modules of the robotic systems, we will classify these papers into categories based on the module to which\\nthe paper contributes the most. There are, however, certain applications of the vision and language foundation models that\\ngo across these robotics modules, e.g., grounding of these models in robotics, and generating data from LLMs and VLMs.\\nGiven the autoregressive nature of current LLMs, they often grapple with extended horizon tasks. Thus, we also delve into\\n11', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d418d2e3-36e2-420d-886c-286d5b46da37', embedding=None, metadata={'page_label': '12', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='1a1ef9148916d25077d384513c78ca4792ccd84f471d2b0dede65852355f1cb3', text='advanced prompting methods proposed in the literature to ameliorate this limitation and enhance planning power. We list these\\napplications in sections 4.1.4, 4.1.5 and 4.1.6, as a different perspective to analyze these works.\\nWe find that works in Section 4.1 typically follow a modular strategy, in applying vision and language foundation models to\\nserve a single robot functionality, e.g., applying VLMs as open-set robot perception modules which are then “plugged in” to\\nwork alongside motion planners and controllers [25], downstream. Since such foundation models are applied in a zero-shot\\nmanner, there are no gradients flowing between the module in which the foundation models are applied and the other modules\\nin the robotic system. Conversely, works in Section 4.2 mostly follow an end-to-end differentiability paradigm, which blurs\\nthe boundary of the typical robotics modules in methods (described in Section 4.1; e.g., perception and control [27, 195]),\\nwith some robotics foundation models even providing a unified model to perform different robot functions [30, 31].\\nFoundation Models for Robotics (4)\\nRobotic Foundation Models (4.2)\\nGeneral-purpose (4.2.2)Single-purpose\\nAction Generation Models\\n(4.2.1)\\nVision and Language\\nPre-trainingReinforcement Learning\\nat Scale(Languange-Conditioned)\\nImitation LearningMotion Planning Models\\n(4.2.1)(Vision Languange)\\nFoundation Models\\nused in Robotics (4.1)\\nPrompting (4.1.6)Data Generation (4.1.5)Grounding (4.1.4)Robotic Modules\\nLLM and VLM in\\nAction Generation (4.1.3)LLM and VLM\\nin Task Planning (4.1.2)VFM and VLM\\nin Perception (4.1.1)\\nFigure 5: Conceptual Framework of Foundation Models in Robotics: The figure illustrates a structured taxonomy of foundational models,\\ncategorized into two primary segments: the application of existing foundation models (vision and language models) to robotics, and the\\ndevelopment of robotic-specific foundation models. This includes distinctions between vision and language models used as perception tools,\\nin planning, and in action, as well as the differentiation between single-purpose and general-purpose robot foundation models.\\n4.1 Foundation Models used in Robotics\\nIn this section, we focus on zero-shot application of vision and language foundation models in robotics. This mainly includes\\nzero-shot deployment of VLMs used in robotic perception, in context learning of LLMs for task-level and motion-level\\nplanning, as well as action-generation. We show a few representative works in Figure 6.\\n4.1.1 VFMs and VLMs in Robot Perception\\nRecently, the grounding of vision and language foundation models with geometric and object-centric representations of the\\nworld has enabled tremendous progress in context understanding, which is a vital requirement for robots to interact with the real\\nworld. We will thoroughly examine the application of VFMs and VLMs in robotic perception from various perspectives.\\nVFMs, VLMs for Object and Scene Representations The most straightforward application of VLMs in robotics is to\\nleverage their ability to perform open-set object recognition and scene understanding in robotics-oriented downstream tasks,\\nincluding semantic mapping and navigation [25, 201–204], manipulation [196–198, 205], etc. The methods proposed by these\\nworks share a common attribute: they attempt to extract semantic information (from the VLM) and spatial information (from\\nother modules or sensor modalities) from objects and scenes that the robots interact with. This information is then used as\\nrepresentations in semantic maps of scenes or representations of objects.\\n12', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d6fb4a28-623b-4188-8ce5-91bb8b822aa7', embedding=None, metadata={'page_label': '13', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='a6d07c444364415acdbbacadc5f729a67ab3d901c9f11ff08fea3f8c969499f0', text='(a) Object recognition: CLIPort [196]\\n (b) Object recognition & localization: NLMap [25]\\n (c) Feature distillation in NeRF: F3RM [197]\\nVLM used for: Object and Scene Representation\\n(d) Representation in policy learning (also applied\\nfeature distillation in Ge-NeRF): GNFactor [198]\\n (e) State estimation: FM-Loc [199]\\n (f) Interactive perception: MOSAIC [200]\\nVFM and VLM used for: Policy learning, State estimation and Interactive perception\\nFigure 6: Using VLMs and VFMs for robotic perception, in various applications. Due to the limit of space and for the consistency of the\\nfigures, we only list part of the works we discussed in this figure.\\nFor semantic mapping and/or navigation, NLMap [25] is an open-set and queryable scene representation to ground task plans\\nfrom LLMs in surrounding scenes. The robot first explores the environment using frontier-based exploration to simultaneously\\nbuild a map and extract class-agnostic regions of interest, which are then encoded by VLMs and aggregated to the map.\\nNatural language instructions are then parsed by an LLM to search for the availability and locations of these objects in the\\nscene representation map. ConceptFusion [203] builds open-set multimodal 3D maps from RGB-D inputs and features\\nfrom foundation models, allowing queries from different modalities such as image, audio, text, and clicking interactions. It\\nis shown that ConceptFusion can be applied for real-world robotics tasks, such as tabletop manipulation of novel objects\\nand semantic navigation of autonomous vehicles. Similarly, CLIP-Fields [202] encodes RGB-D images of the scene\\nto language-queryable latent representations as elements in a memory structure, that the robot policy can flexibly retrieve.\\nVLMap [201] uses LSeg [151] to extract per-pixel representations to then fuse with depth information, in order to create a\\n3D map. This semantic 3D map is then down-projected to get a 2D map with the per-pixel embedding; these embeddings\\ncan then be matched with the language embedding from LSeg to obtain the per-pixel semantic mask for the 2D map. As\\nfor applying VLMs in topological graphs for visual navigation, LM-Nav [204] is a good example: it uses an LLM to extract\\nlandmarks used in navigation from natural language instructions. These landmark descriptions, as well as image observations,\\nare then grounded in a pre-built graph via a VLM. Then, a planning module is used to navigate the robot to the specified\\nlandmarks.\\nMost of the previous works discussed above utilize only 2D representation of the objects and environment. To enrich the\\nrepresentation of foundation models in 3D space, F3RM [197] and GNFactor [198] distill 2D foundation model features into\\n3D space, by combining with NeRF and generalizable NeRF. In addition, GNFactor [198] also apply these distilled features\\nin policy learning. Act3D [206] takes a similar approach but build 3D feature field via sensed depth.\\nVLMs for State Estimation and Localization Beyond context understanding, a few approaches explore the use of\\nopen-vocabulary properties of VLMs for state estimation [199, 199, 207–209]. Two such approaches, LEXIS [207] and\\nFM-Loc [199], explore the use of CLIP [53] features to perform indoor localization and mapping. In particular, FM-Loc [199]\\nleverages the vision-language grounding offered by CLIP andGPT-3 to detect objects and room labels of a query image,\\nthen uses that semantic information to match it with reference images. Similarly, LEXIS [207] builds a real-time topological\\nSLAM graph where CLIP features are associated with graph nodes, enabling room-level scene recognition. Although these\\napproaches display the potential of vision-language features for indoor place recognition, they do not explore the broad ap-\\nplicability of foundation model features. In this context, AnyLoc [208] explored the properties of dense foundation model\\n13', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e07cfa11-ae7e-4be8-9d4d-ae9ce50e5d84', embedding=None, metadata={'page_label': '14', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='03d89e22d80634b441a8a99f684a8a86365fc3f10df65e023ed84a4e486f16e8', text='(a) Task plan generation via LLM and\\ngrounding of visual states, SayCan [24]\\n(b) Task planning via code generation\\nLLM: Code as policy [216]\\n(c) VLMs for task planning and heuristic function,\\nplus a video dynamics model: VLP [217]\\nFigure 7: Examples of LLMs and VLMs used in task-level planning\\nfeatures and combined them with unsupervised feature-aggregation techniques to achieve state-of-the-art place recognition, by\\nlarge margin—anywhere, anytime, and under any view—showcasing broad applicability of self-supervised foundation model\\nfeatures for SLAM.\\nVLMs for Interactive Perception Several works consider the notion of enabling robots to leverage the process of interactive\\nperception, for extrapolating implicit knowledge about object properties in order to obtain performance improvements on\\ndownstream interactive robot learning tasks [166, 167, 200, 210–215]. This process of interactive perception is often\\nmodeled after the way in which human infants first learn about the physical world—i.e., through interaction, and by learning\\nrepresentations of object concepts, such as weight and hardness, from the sensory information (haptic, auditory, visual)\\nthat is generated from those physical exploratory actions (e.g., grasping, lifting, dropping, pushing) on objects with diverse\\nproperties. In particular, MOSAIC [200] leverages LMMs to expedite the acquisition of unified multi-sensory object property\\nrepresentations; the authors show competitive performances of their framework in category recognition and ambiguous\\ntarget-object fetch tasks, despite the presence of distractor objects, under zero-shot transfer conditions.\\n4.1.2 LLMs and VLMs in Task Planning\\nThe planning community in Robotics has always harbored aspirations of a model capable of generalizing across diverse tasks\\nand environments, with minimal demonstrations for robotic tasks. Given the demonstrated prowess of vision and language\\nfoundation models in intricate reasoning and contextual generalization, it is a natural progression for the robotics community\\nto consider the application of foundation models to robotic planning problems. This section organizes works based on the\\ngranularity of planning, delineating between task-level and motion-level planning. We will mainly introduce task-level planning\\nin this part and leave motion-planning to the next part, together with action generation (Section 4.1.3).\\nTask-level planning is to divide a complicated task into small actionable steps. In this case, we mainly talk about the agent\\nplanning on its own, in contrast to, e.g., using LLMs as a parser like Vision Language Navigation [218]. The agent needs to\\ntake intelligent sub-steps to reach the goal by interacting with the environment. SayCan [24] is a representative example of\\ntask-level planning: it uses LLMs to plan for a high-level task, e.g., “I spilled my drink, can you help?”. Then it gives concrete\\ntask plans like going to the counter, finding a sponge, and so on. Similarly, VLP [217] aims to improve the long-horizon\\nplanning approach with an additional text-to-video dynamics model. These task-level planning methods do not have to worry\\nabout the precise execution of those sub-tasks in the environment, since they have the luxury of utilizing a set of pre-defined /\\npre-trained skills, then using LLMs to simply find proper ways to compose skills to achieve desired goals. There are more papers\\nin this category, for example: LM-ZSP [219] introduce this task-level granularity as actionable steps; Text2Motion [220]\\nuses similar ideas and augments the success rate of language based manipulation task. Previous methods typically generate\\ntask plans in the form of text. Some works like ProgPrompt [221], Code as Policy [216], GenSim [222], etc. obtain\\ntask plans in the form of code generation using LLMs. Using code as a high-level plan has the benefit of expressing functions\\nor feedback loops that process perception outputs and parameterize control primitive APIs. In addition, it can describe the\\nspatial position of an object accurately. This improved compositionality saves time in collecting more primitive skills. It also\\nprescribes precise values (e.g., velocities) to ambiguous descriptions like ’faster’ and ’to the left’, depending on the context.\\nTherefore, due to these benefits, code appears to be a more efficient and effective task-level planning language than natural\\n14', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='30f5ffaa-e4be-4275-9709-d976b9112ea7', embedding=None, metadata={'page_label': '15', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='452efe27859cd543702c283d229e29b30582434440c4e1dcbd75750ef2ee08f9', text='(a) Reward generation with LLM :\\nL2R [226]\\n(b) Locomotion control via action pattern\\nmatching with LLM: SayTap [26]\\n(c) VLM for affordance function in motion\\nplanning: VoxPoser [227]\\nFigure 8: LLM used in motion planning and action generation.\\nlanguage. Other form of planning techniques such as express the high-level plans in Planning Domain Definition Language\\n(PDDL)[223] also showed significant improvement on LLMs planning power over long horizon tasks, more on this will be\\ndiscussed in the Section 4.1.6.\\nIn addition to use LLMs to direct generate plans, they are also used in searching and evaluating with external memory\\nstructure such as scene graph. SayPlan [224] employs 3D scene graph (3DSG) representations to manage the complexity\\nof expansive environments. By exploiting hierarchical 3DSGs, LLMs can semantically search for relevant sub-graphs in\\nmulti-floor household environments, reducing the planning horizon and integrating classical path-planning to refine initial\\nplans, iteratively. Reasoned Explorer [225] employs LLMs as evaluators to score each node in a 2D undirected graph.\\nIt uses this graph as a map to store both visited points and the frontiers’ LLM evaluations. These external memories and\\nincremental map-building approach breaks the context length limit of using LLMs to generate long plans, which scales LLM-\\nbased navigation agents to large environments. One thing to note is that, although task-level planning is agnostic to physical\\nembodiment, it does require grounding to a specific robot’s physical form (or “morphology”) and environment when deployed;\\ngrounding techniques will be covered in Section 4.1.4.\\n4.1.3 LLMs and VLMs in Action Generation\\nDirectly controlling a robot just by prompting off-the-shelf LLMs/VLMs can be challenging, perhaps even unachievable,\\nwithout first fine-tuning these models with action data. Unlike high-level robot task planning, where LLMs are used for their\\nability to compose and combine different skills for task completion, individual actions, both high-level actions like waypoints,\\nand low-level actions like joint angles are usually not semantically meaningful or compositional. The community is attempting\\nto find a suitable interface to circumvent this problem. For motion planning in navigation tasks, ReasonedExplorer [225]\\nandNotTrain Dragon [228] propose such an interface: using LLMs as evaluators for the expanded frontiers, which are\\ndefined as potential waypoints for exploration (typically in a two-dimensional space); here, LLMs are tasked with scoring\\nfrontiers based on the similarity between the given observations and the intended goal. Similarly, VoxPoser [227] apply VLMs\\nto obtain affordance function (called 3D value map in the original paper) used in motion planning.\\nSome papers investigate the use of LLMs to directly output lower-level actions. Prompt2Walk [229] uses LLMs to\\ndirectly output joint angles through few-shot prompts, collected from the physical environment. It investigates whether\\nLLMs can function as low-level controllers by learning in-context with environment feedback data (observation-action pairs).\\nSaytap [26], introduces a novel concept of utilizing foot contact patterns as an action representation. In this model, the\\nlanguage model outputs a ‘0’ for no contact and a ‘1’ for contact with the floor, thereby enabling Large Language Models\\n(LLMs) to generate zero-shot actionable commands for quadrupedal locomotion tasks such as jumping and jogging. However,\\nthe generalizability of these approaches to different robot morphologies remains in question, since they have only been tested\\non the quadruped platform. Instead, language to reward [230–232] in robotics [182, 226] is a more general approach than\\ndirect action generation through LLMs; these approaches involve using LLMs as generators to synthesize reward functions\\nfor reinforcement learning-based policies and thus are usually not confined by robotic embodiments [182]. The reward\\nsynthesizing approach with LLM can generate rewards which are hard for human to design, e.g., Eureka [182] demonstrates\\nthat it enables robots to learn dexterous pen-spinning task that were considered very hard using human reward design.\\n15', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='b7f58f6f-1299-41ca-9f6e-d4eb8310b0ca', embedding=None, metadata={'page_label': '16', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='f9152a8080727b2ccd42f4504a997b7e75bf586256d85a2f548dff66e4d6b939', text='4.1.4 Grounding in Robotics\\nAlongside the above dimensions, an equally crucial aspect is the concept of “grounding”. Grounding refers to the ability to\\nassociate contextual meaning with signals or symbols, such as the ability to tie a word to its manifestation in the world. Humans\\nunderstand semantic concepts through both audio (words, tone) and visual signals (gestures, behaviors, body language). In\\nthe scope of this survey, grounding alludes to the process of aligning the abstract knowledge possessed by foundation models\\nwith the tangible, real-world specifics of robotics—ensuring that language-driven decisions correspond meaningfully with\\nphysical actions and environmental contexts. For instance, if we ask an LLM to generate a plan to find a pen in a specific\\nhouse, without any environmental information, then this task is akin to a blind person reasoning about how to navigate in an\\nunknown space, rendering the task nearly impossible to complete. Similarly, while an LLM may easily generate a plan to\\nlift a chair by suggesting to ’grab the left handle with the left hand and the right handle with the right hand, then lift’, this\\nplan becomes impractical if the real-world embodiment of the model is a typical robot equipped with only one arm. Since\\ngrounding is a large field in itself, we do not attempt to cover all of it; instead, we will address four concepts as shown in the\\nFigure 9: (1) Grounding language to environments ; (2) Grounding latent concepts to environments ; (3) Grounding language\\nto embodiments ; and (4) Grounding latent concepts to embodiments .\\nGrounding Language to Environments As discussed in the previous sections, specifically Sections 4.1.2 and 4.1.3, for the\\nsuccessful integration of LLMs or VLMs directly into robots, it is crucial to establish a connection between the language output\\nand either skills or low-level actions. SayCan [24] learned a value function to score the joint likelihood between task-level\\nplans and skills acquired through reinforcement learning or behavior cloning. ZSP [219] utilized semantic similarities between\\nlanguage instructions and skill names to ground robot skills, while ProgPrompt [221] and Code as Policy ground\\nrobot skills via program code synthesis. However, these skills are trained specifically for the environments in which they were\\ntested. Therefore, we categorize all methods that associate language with skills as grounding to environments. Some research\\nefforts, such as Grounded Decoding [233], attempt to address this by grounding foundation models to skills trained\\nin a single environment, using small language-conditioned models trained in various environments as a probabilistic filter.\\nNonetheless, this approach is still limited as it presupposes the availability of grounded models suited for the environment in\\nwhich it is embodied. In addition to skill grounding, work like Voxposer [227] have attempted to use LLMs to generate code\\nfor constructing value maps of the environment, which can then be planned on using existing planners. This represents a more\\ngeneral approach to integrating contextual information about the environment, compared to grounding to skill libraries.\\nGrounding Concepts to Environments The term ‘concepts’ refers to the unified latent representations, derived from training\\nwith diverse input data. Approaches such as CLIP-Fields [202], VLMap [201], and NLMaps [25] have endeavored to\\nproject CLIP visual representations and semantic label representations directly onto 3D point clouds. Beyond constructing\\nexplicit 3D maps, GLAM [234] proposes using reinforcement learning to ground language models with an internal map through\\ninteraction with environments. This approach demonstrated that LLMs can effectively function as Reinforcement Learning\\n(RL) agents in textual environments. However, the challenge of generalizing these approaches to different environments and\\ntasks, either by grounding concepts to point clouds or through implicit mapping, remains unresolved in current literature.\\nGrounding Language to Embodiments We categorize language grounding to embodiments as necessitating a specific\\ncondition: it must be agnostic to different environments. This is akin to having a universal interface that translates language\\ninto actions. Initiatives like Prompt2Walk [235] and Saytap [26] have experimented with directly using LLMs to generate\\njoint angles or foot patterns in the language space for locomotion control.\\nGrounding Concepts to Embodiments Grounding concepts to embodiments involves directly anchoring foundation models\\nto output joint torques, circumventing intermediary interfaces such as text. A notable example is Gato [30], which demonstrates\\nversatility in tasks like playing Atari, captioning images, conversing, and manipulating objects with a robotic arm. Gato\\ndynamically decides its output format—be it text, joint torques, button presses, or other tokens—based on the context of the\\ntask at hand. Another related development is RT-2 [28], which, despite specifying the end-effector space in textual form, is\\ncapable of directly generating executable commands for robot manipulator operation.\\nIn conclusion, the primary advantage of grounding language to environments and embodiments is the ease of implementing\\nzero-shot learning without the need for additional training. However, significant drawbacks exist. For concepts not describable\\n16', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='70d248e3-a24a-4e6d-ae2f-48999af83353', embedding=None, metadata={'page_label': '17', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='06853a0b0cc6bed3cb4d3b643853b133c8da0c5221602499387d557b9c874eb7', text='Sensory InputsZLanguageEnvironmentGroundingConceptsEmbodimentGroundingorFigure 9: Grounding in robotics. We divide grounding techniques as grounding to environment and grounding to robotic embodiments.\\nby language, such as the nuances of finger movements, grounding to embodiments may fail. Moreover, reliance on a fixed\\nset of skill libraries limits adaptability to diverse environments. Consequently, direct grounding from the latent concept space\\nappears to be a more viable solution. Approaches that utilize interaction data [234] or expert data [28] have both shown\\npromising results in addressing these challenges.\\n4.1.5 Data Generation with LLMs and VGMs\\nRecently, we have witnessed the power of content generation ability of LLMs and VGMs. Utilizing this ability, researchers\\nhave begun attempts to address the data scarcity problem by using foundation models to generate data. Ha et al. [236] propose\\na framework, ‘ scaling up and distilling down ’, which, given natural language instructions, can automatically generate diverse\\nrobot trajectories labeled with success conditions and language. RoboGen by Wang et al. [237] further enhances this approach\\nby incorporating automatic action trajectory proposals within a physics-realistic simulation environment, Genesis, enabling\\nthe generation of potentially-infinite data. Nevertheless, these approaches still face limitations: the data generated suffers\\nfrom low diversity in assets and robot morphologies, issues that could be ameliorated with advanced simulations or hardware.\\nGenSim [222] by Wang et al. proposes to generate novel long-horizon tasks with LLMs given language instructions, leading to\\nover 100 simulation tasks for training language-conditioned multitask robotic policy. This framework demonstrates task-level\\ngeneralization in both simulation and the real world, and sheds some light on how to distill foundation models to robotic\\npolicies through simulation programs. ROSIE by Yu et al. [238] uses a text-guided image generator to modify the robot’s\\nvisual observation to perform data augmentation when training the control policy. The modification commands are from\\nthe user’s language instruction, then the augmentation regions are localized by the open vocabulary segmentation model.\\nRT-Trajectory [239] generates trajectories for the policy network to condition on. The trajectory generation also helps\\nthe task specification in the robot learning tasks. Black et al. [240] use a diffusion-based model to generate subgoals for a\\ngoal-conditioned RL policy for manipulation [241].\\n4.1.6 Enhancing Planning and Control Power through Prompting\\nThe technique of Chain-of-Thought, as introduced by Wei et al. [242], compels the LLM to produce intermediate steps\\nalongside the final output. This approach leverages a broader context window to list the planning steps explicitly, which\\nenhances the LLM’s ability to plan. The underlying reason for its effectiveness is the GPT series’ nature as an autoregressive\\ndecoder. Semantic similarities are more pronounced between instructions to steps and steps to goal, than between instructions\\nto the direct output. Nonetheless, the sequential nature of the Chain-of-Thought implies that a single incorrect step can lead to\\nexponential divergence from the correct final answer [243].\\nAlternative methodologies attempt to remedy this by explicitly listing steps within graph [244] or tree structures [245], which\\nhave demonstrated improved performance. Additionally, search-based methods such as Monte Carlo Tree Search (MCTS) [246]\\nand Rapidly-exploring Random Tree (RRT) [225] have been explored to augment planning capabilities.\\nFurthermore, translating goal specifications from natural language into external planning languages, such as the Planning\\nDomain Definition Language (PDDL), has also been shown to increase planning accuracy [247]. Finally, as opposed to an\\nopen-loop prompting style, iterative prompting approaches that incorporate feedback from the environment provide a more\\ngrounded and precise enhancement for long-term planning capability [31, 248].\\n17', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f267927c-e797-4e06-abb4-313cb93404ba', embedding=None, metadata={'page_label': '18', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='3af5f8e4ee33f01fbb6863b0df5b9a3c5c3c445815c52cc481d4d1c65b9a823d', text='(a) Imitation learning: RT-1 [27]\\n (b) Reinforcement learning: Q-Transformer [195]\\n (c) Vision language pretraining: MOO [249]\\nRobotic Foundation Models for: manipulation\\n(d) Motion planning model: VINT [29]\\n (e) General purpose model: Gato [30]\\n (f) General purpose model: PaLM-E [31]\\nRobotics Foundation Models for: motion planning for navigation and multipurpose task\\nFigure 10: Examples of Robotics Foundation Models (RFMs)\\n4.2 Robotics Foundation Models (RFMs)\\nWith the increasing amount of robotics datasets, containing state-action pairs from real robots, the class of Robotics Foundation\\nModels (RFMs) have likewise become increasingly viable [28, 29, 176]. These models are characterized by the use of robotics\\ndata to train them, in order to solve robotics tasks. In this subsection, we summarize and discuss different types of RFMs. We\\nwill first introduce RFMs that can perform one set of tasks, within one of the robotic modules from Section 2.1, which is defined\\nassingle-purpose Robotic Foundation Models. For example, an RFM that can generate low-level actions to control the robots,\\nor a model that can generate higher-level motion plans. We later introduce RFMs that can carry out tasks in multiple robotic\\nmodules, hence general-purpose models that can perform perception, control, and even non-robotic tasks [30, 31].\\n4.2.1 Robotics Action Generation Foundation Models\\nRobotic action foundation models could take raw sensory observations, e.g., images or videos, and learn control output that is\\ndirectly applied to robotic end-effectors. Models in this category include RTseries [27, 28, 176], RoboCat [195], MOO [249],\\netc. According to the papers’ results, these models show generalization in robot control tasks such as manipulation.\\nImitation Learning Language inputs have also been used for providing task specifications for end-to-end direct downstream\\ncontrol of robotics, forming language-conditioned imitation policy learning. Li et al. [250] use pre-trained language models to\\ninitialize policy networks that predict the next action. The control policy is fine-tuned via behavior cloning and active learning,\\nto improve task completion. Directly using language inputs to generate robot actions for improving human-robot collaboration\\nhas also been proposed: the Language-Informed Latent Actions ( LILA ) framework [251] learns to use language to modulate\\nthe low-level controller, effectively using language to map a 7-DoF robotic arm output to a 2-DoF input. Further, Hu et al. [252]\\nuse multi-agent reinforcement learning to fine-tune an LLM policy to enable humans to specify what kind of strategies they\\nexpect from their AI partners. Building on the theme of instructing robots through language, Lynch et al. [253] use behavioral\\ncloning on a dataset of hundreds of thousands of language-annotated trajectories to improve vision-language-motor skills in the\\nreal world. Though previous works mostly take a language-conditioned approach, RoboCat [195] trains imitation learning\\npolicy with just robotic data.\\nReinforcement Learning at Scale As discussed in Section 2.1.3 (Preliminaries), offline RL has the potential to learn good\\npolicies without even interacting with the environment. With the availability of large-scale robotic datasets, offline RL starts\\nto play an important role in developing effective RFMs. Early large-scale offline RL models such as QT-OPT [112] use a\\nQ-learning-based approach in an offline manner to learn policy from robotics data which are collected by a robot farm. The\\nsuccessors of QT-OPT extend it to multitask learning by incorporating multi-task curriculum or predictive information [254,\\n255]. Recently, with the success of Transformer models, Q-learning based on transformer ( Q-Transformer ) also shows\\n18', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='41c26829-ac9b-431b-9fbc-d43f900316d6', embedding=None, metadata={'page_label': '19', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='41e9875a1076eef1f7f297aa47f4d0eb693f5202b2f9eadfc1ed260bef17dbd7', text='its potential [256]. PTR [257] is another promising work that adopts Conservative Q-Learning ( CQL) [258] in a multi-task\\nlearning setting. We look forward to seeing more RL-based robotic foundation models.\\nVision and Language Pre-training Another direction of action foundation models involves vision or language pre-training\\n[28, 259–264]. For example, inspired by the great generalization ability of self-supervised vision-based pre-training, MVP\\nby Radosavovic et al. [261] trains visual representations on real-world images and videos from the internet and egocentric\\nvideo datasets, via a masked autoencoder, and demonstrate the effectiveness of scaling visual pre-training for robot learning.\\nFollowing this work, RPT [262] proposes Mask-pretraining with real robot trajectory data. VC-1 [264] actually did a\\ncomprehensive study on the effectiveness of vision-based pre-training on policy learning. We also recommend readers to learn\\nmore information about this problem from that paper. Despite the effectiveness of these visual pretraining methods, Hansen\\net al. [263] reexamined some of these methods and discovered significant domain gaps and propose a learning from scratch\\napproach which remains competitive. This provides us a new perspective to think about visual pretraining in robotics.\\nBeyond just using visual information, RT-2 [28] and Moo [249] use vision and language pre-trained model as a control policy\\nbackbone. PaLM-E [31] and PALI-X [265] were used to transfer knowledge from the web into robot actions. Slightly\\ndifferent from previous methods, VRB [266] learns affordance functions (instead of the policy itself) from large-scale video\\npertaining, providing another thought process for us to study how RFMs may generalize in real-world tasks.\\nRobotics Motion Planning Foundation Models Recently we have seen the rise of RFMs especially used for motion-planning\\npurposes in visual navigation tasks [29, 267, 268]. These foundation models take advantage of the large-scale heterogeneous\\ndata and show generalization capability in predicting high-level motion-planning actions. These methods rely on rough\\ntopological maps [29, 267] consisting of only image observations instead of accurate metric maps and accurate localization as\\nin conventional motion-planning methods (as described in Section 3.3). Unlike vision and language foundation models applied\\nto motion planning, the robotic motion planning foundation model is still quite in its early stages.\\n4.2.2 General-purpose Robotics Foundation Models\\nDeveloping general-purpose robotic systems is always a holy grail in robotics and artificial intelligence. Some existing works\\n[30, 31] take one step towards this goal. Gato [30] proposes a multimodal, multi-task, and multi-embodiment generalist\\nfoundation model that can play Atari games, caption images, chat, stack blocks with a real robot arm, and more—all with\\nthe same model weights. Similar to Gato ,PaLM-E [31] is also a general-purpose multimodal foundation model for robotic\\nreasoning and planning, vision-language tasks, and language-only tasks. Although not proven to solve all the robotics tasks\\nthat we introduced in Section 2, Gato andPaLM-E show a possibility of merging perception and planning into one single\\nmodel. Moreover, Gato andPaLM-E show promising results of using the same model to solve various seemingly-unrelated\\ntasks, highlighting the viability of general-purpose AI systems. Designed especially for robotic tasks, PACT [269] proposes\\none transformer-based foundation model with common pre-trained representations that can be used in various downstream\\nrobotic tasks, such as localization, mapping, and navigation. Although we have not seen many unified foundation models for\\nrobotics, we would expect more endeavors in this particular problem.\\n4.3 How Foundation Models Can Help Solve Robotics Challenges\\nIn Section 3, we listed five major challenges in Robotics. In this section, we summarize how foundation models—either vision\\nand language models or robotic foundation models—could help resolve these challenges, in a more organized manner.\\nAll the foundation models related to visual information, such as VFMs, VLMs, and VGMs, are used in the perception modules\\nin Robotics. LLMs, on the other hand, are more versatile and can be applied in both planning and control. We also list RFMs\\nhere, and these robotic foundation models are typically used in planning and action generation modules. We summarize how\\nfoundation models solve the aforementioned robotic challenges in Table 1. We notice from this table that all foundation models\\nare good at generalization in tasks of various robotic modules. Also, LLMs are especially good at task-specification. RFMs,\\non the other hand, are good at dealing with the challenge of dynamics model since most RFMs are model-free approaches.\\nFor robot perception, the challenges in generalization ability and model are coupled, since, if the perception model already has\\nvery good generalization ability, there’s no need to get more data for domain adaptation or additional fine-tuning. In addition,\\nthe call for solving the safety challenge is largely missing, and we will discuss the particular problem in Section 6.\\n19', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='28397929-f53f-4b4e-ad21-14c8a9140c9d', embedding=None, metadata={'page_label': '20', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='663719d2a4de1e5f155749c7f4413e0ea01a4a074b56eab30627521ca041974e', text='Modules Foundation Models Generalization 3.1 Data 3.2 Model 3.3 Task Specification 3.4 Uncertainty 3.5\\nPerception 2.1.1VFM 2.2.1 Conceptgraphs [270] Conceptgraphs [270] - - -\\nVGM 2.2.2 -ROSIE [238]\\nRoboGen [237]- -\\nVLM 2.2.4 NLMap [25]NLMap [25]\\nRT-Traj. [239]- RT-Traj. [239] -\\nTask Planning and\\nAction Generation\\n2.1.2 and 2.1.3LLM 2.2.3 SayCan [24]SayCan [237]\\nRT-Traj. [239]RAP [271]LILA [251]\\nL2R [226]KNOWNO [187]\\nRFM 4.2RT-1 [27]\\nRT-2 [28]\\nRoboCat [195]\\nVINT [29]RT-X [176]RT-1 [27]\\nRT-2 [28]\\nRoboCat [195]Zest [181] -\\nTable 1: A summary of foundation models solving robotic challenges. Here we only list part of the works due to space limit. We find that\\nuncertainly and safety are still largely unexplored.\\nFoundation Models for Generalization Zero-shot generalization is one of the most significant characteristics of current\\nfoundation models. Robotics benefits from the generalization ability of foundation models in nearly all aspects and modules.\\nFor the first one, generalization in perception, VLM and VFM are great choices as the default robotics perception models. The\\nsecond aspect is the generalization ability in task-level planning, with details of task plans generated by LLMs [24]. The third\\none is in generalization in motion-planning and control, by utilizing the power of RFMs.\\nFoundation Models for Data Scarcity Foundation models are crucial in tackling data scarcity in robotics. They offer a\\nrobust basis for learning and adapting to new tasks with minimal specific data. For example, recent methods utilize foundation\\nmodels to generate data to help with training robots, such as robot trajectories [236] and simulation [237]. These models\\nexcel in learning from a small set of examples, allowing robots to quickly adapt to new tasks using limited data. From this\\nperspective, solving data scarcity is equivalent to solving the generalization ability problem in robotics. Apart from this\\naspect, foundation models—especially LLMs and VGMs—could generate datasets for robotics used in training perception\\nmodules [238] (see Section 4.1.5, above), and for task-specification [239].\\nFoundation Models to Relieve the Requirement of Models As discussed in Section 3.3, building or learning a model—\\neither a map of the environment, a world model, or an environmental dynamics model—is vital for solving robotic problems,\\nespecially in motion-planning and control. However, the powerful few/zero-shot generalization ability that foundation models\\npresent may break that requirement. This includes using LLMs to generate task plans [24], using RFMs to learn model-free\\nend-to-end control policies [27, 256], etc.\\nFoundation Models for Task-Specification Task-specifications as language prompts [24, 27, 28], goal images [181, 272],\\nvideos of humans demonstrating the task [273, 274], rewards [26, 182], rough scratch of trajectory [239], policy sketches [275],\\nand hand-drawn images [276] have allowed goal specifications in a more natural, human-like format. Multimodal foundation\\nmodels allow users to not only specify the goal but also help resolve ambiguities via dialogue. Recent work in understanding\\ntrust and intent recognition within the human-robot interaction domain has opened up newer paradigms in our understanding\\nof how humans use explicit and implicit cues to convey task-specifications. While significant progress has been made, recent\\nwork in prompt engineering for LLMs implies that even with a single modality, it is challenging to generate relevant outputs.\\nVision-Language Models are proven to be especially good at task-specification, showing potential for resolving this problem\\nin robotics. Extending the idea of vision-language-based task-specifications, Cui et al. [181] explore methods to achieve\\nmulti-modal task specification using more natural inputs like images obtained from the internet. Brohan et al. [27] explores\\nthis idea of zero-shot transfer from task-agnostic data further, by providing a novel model class that exhibits promising scalable\\nmodel properties. The model encodes high-dimensional inputs and outputs, including camera images, instructions, and motor\\ncommands into compact token representations to enable real-time control of mobile manipulators.\\nFoundation Models for Uncertainty and Safety Though being a critical problem in robotics, uncertainty and safety using\\nfoundation models for robotics is still underexplored. Existing works like KNOWNO [187] proposes a framework for measuring\\nand aligning the uncertainty of LLM-based task planners. Recent advances in Chain-of-Thought prompting [277], open\\nvocabulary learning [278], and hallucination recognition in LLMs [279] may open up newer avenues to address these\\nchallenges.\\n20', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d7a75aec-cfe1-4665-8a07-da0bf7dee182', embedding=None, metadata={'page_label': '21', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='5b9c5ac5b70c41d24235bffc6f4aeb3be21630f2dc436c7144c7d4320d2c7d9b', text='5 Review of Current Experiments and Evaluations\\nIn this section, we summarize the datasets, benchmarks, and experiments of the current research works.\\n5.1 Datasets and Benchmarks\\nRelying solely on knowledge learned from language and vision datasets is limiting. Some concepts, like friction or weight,\\nare not easily learned through these modalities alone, as suggested by Gao et al. [280] and Tatiya et al. [200] in their works on\\nphysically grounded VLMs. Therefore, in order to make robotic agents that can better understand the world, researchers are\\nnot just adapting foundational models from the language and vision domains; they are also advancing the development of large,\\ndiverse, and multimodal robotic datasets for training or fine-tuning these foundation models. This effort is now diverging into\\ntwo directions: collecting data from the real world, versus collecting data from simulations and then transferring it to the real\\nworld. Each direction has its pros and cons. We will cover these datasets and simulations in the following paragraphs and\\ndiscuss their respective advantages and disadvantages.\\n5.1.1 Real World Robotics Datasets\\nReal-world robotics datasets are highly appealing due to their diverse object classes and multimodal inputs, offering a rich\\nresource for training robotic systems without the need for complex and often inaccurate physical simulations. However,\\ncreating these large-scale datasets presents a significant challenge, primarily due to the absence of a substantial ‘data flywheel’\\neffect. This effect, which greatly benefited fields like CV and NLP through contributions from millions of internet users, is\\nless evident in robotics. The limited incentive for individuals to upload extensive sensory inputs and corresponding action\\nsequences poses a major hurdle in data acquisition. Despite these challenges, current efforts are focused on addressing these\\ngaps. RoboNet [281] is a notable effort in this direction, offering a large-scale, diverse dataset across different robotic platforms\\nfor multi-robot learning. Bridge Dataset V1 [282] collects 7200 hours of demonstrations in real household kitchen manipulation\\ntasks, and its following Bridge-V2 [283] contains 60,096 trajectories collected across 24 environments on common low-cost\\nrobots. Language-Table [253] collects 600,000 language-labeled trajectories—an order of magnitude larger than prior available\\ndatasets. RT-1 [27] contains 130k episodes that cover 700+ tasks, collected using a fleet of 13 Google mobile manipulation\\nrobots, over 17 months. While the aforementioned datasets represent significant advancement over prior lab-scale datasets,\\noffering a relatively large volume of data, they are limited to single modalities or specific robot tasks.\\nTo overcome these limitations, some recent initiatives have made notable progress. For example, GNM [267] successfully\\nintegrated six different large-scale navigation datasets, utilizing a unified navigation interface based on waypoints. Furthermore,\\na recent collaborative effort among various laboratories called RT-X [176] has aimed to standardize data across different datasets,\\nby using a 7-degree-of-freedom end-effector’s pose as a universal reference across different embodiments. This approach\\nfacilitates the joint use of diverse datasets and showed positive performance in cross-morphology transfer learning.\\nBuilding on these advancements, the scale of real-world robotics datasets is beginning to grow, albeit still lagging behind the\\nimmense volume of internet-scale language and vision corpora. The accessibility of advanced hardware such as the Hello\\nStretch Robot, Unitree Quadrupeds, and open-source dexterous manipulators [284] is expected to catalyze this growth. As these\\ntechnologies become more widely available, they are likely to initiate the desired ‘data flywheel’ effect in Robotics.\\n5.1.2 Robotics Simulators\\nWhile we await the widespread deployment of robotic hardware to gather massive amounts of robot data, another approach is\\nto develop simulators that closely mimic real-world graphics and physics. The advantage of using simulation is the ability to\\ndeploy tens of thousands of robot instances in a simulated world, enabling simultaneous data collection.\\nSimulators focus on different aspects, such as photorealism, physical realism, and human-in-the-loop interactions. For\\nnavigation tasks, photorealistic simulators are crucial. AI Habitat addresses this by utilizing realistically-scanned 3D scenes\\nfrom the Matterport3D [285] and Gibson [286] datasets. Furthermore, Habitat [287] is a simulator that allows AI agents to\\nnavigate through various realistic 3D spaces and perform tasks, including object manipulation. It features multiple sensors\\nand handles generic 3D datasets. Habitat 2.0 [174] builds upon the original by introducing dynamic scene modeling, rigid-\\nbody physics, and increased speed. Habitat 3.0 [175] further integrates programmable humanoids to enhance the simulation\\n21', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e6ee7bb0-ba63-46cd-a606-b91776ed658d', embedding=None, metadata={'page_label': '22', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='b5d269cf0fa7944ce87fdb639e7e85ad59a578c6d84c0531cbff38a024815098', text='experience. Additionally, the AI2THOR simulator [288] is another promising framework for photorealistic visual foundation\\nmodel research, as evidenced in [201, 289]. Other simulators, like Mujoco [170], focus on creating physically realistic\\nenvironments for advanced manipulation and locomotion tasks.\\nMoreover, simulators like AirSim [290] and the Arrival Autonomous Racing Simulator [102], both built on Unreal Engine,\\nprovide a balance of reasonable physics and photorealism. Ultimately, while the aforementioned simulators excel in various\\nareas, they face common challenges such as parallelism. Simulators like Issac Gym [171] and Mujoco 3.0 [291] have attempted\\nto overcome these challenges by using GPU acceleration to expedite the data-collection process.\\nDespite the abundance of data available in simulators, there are inherent challenges in their use. Firstly, the domain gap\\nbetween simulations and the real world makes transferring from sim to real problematic—issues that early works are already\\nseeking to resolve [164]. Secondly, the diversity of environments and base objects is still lacking. Therefore, to effectively\\nutilize simulations in the future, continuous improvements in these two areas are essential.\\n5.2 Analysis of Current Method Evaluation\\nWe conduct a meta-analysis of the experiments of papers listed in Tables 2 to 7 and Figure 11, encouraging readers to consider\\nthe following questions\\n1. What tasks are being solved?\\n2. On what datasets or simulators have they been trained? What robot platforms are used for testing?\\n3. What foundation models are being utilized? How effectively are the tasks solved?\\n4. What base foundation models are more frequently used in these methods?\\nWe summarize several key trends observed in the current literature concerning the experiments conducted:\\nImbalanced Focus among Manipulation Tasks: There is a significant emphasis on general pick-place tasks, particularly\\ntabletop and mobile manipulation. This is likely due to the ease of training for tabletop gripper-based manipulation skills and\\ntheir potential to form skill libraries that interact with foundation models. However, there is a lack of extensive exploration in\\nlow-level action outputs, such as dexterous manipulation and locomotion.\\nNeed for Improved Generalization and Robustness: Generalization and robustness of end-to-end foundational robotics\\nmodels have room for improvement. In tabletop manipulation, the use of foundation models leads to performance drops\\nranging from 21% [27, 227] to 31% [28] in unseen tasks. In addition, these models still need improved robust to disturbances,\\nperformance drops 14% [27] to 18% [227] for similar tasks.\\nLimited Exploration in Low-Level Actions: There remains a gap in the exploration of direct low-level action outputs. The\\nmajority of research focuses on task-level planning and utilizes foundation models with pre-trained or pre-programmed skill\\nlibraries. However, existing papers [28, 30, 176] that explore low-level action outputs mainly focus on table-top manipulation,\\nwhere the action space is limited to the end effector’s 7 degrees of freedom (DoF). Models that directly output joint angles for\\ntasks like dexterous manipulation and locomotion still require a more thorough research cycle.\\nControl Frequencies Too Slow to be Deployed on Real Robots: Most current approaches to robotic control are open-loop,\\nand even those that are closed-loop face limitations in inference speed. These speeds typically range from 1 to 10 Hz, which\\nis considered low for the majority of robotics tasks. Particularly for tasks like humanoid locomotion, a high-frequency control\\nof around 500 Hz is required for the stabilization of the robot’s body [322].\\nLack of Uniform Benchmarks for Testing: The diverse nature of simulations, embodiments, and tasks in robotics leads\\nto varied benchmarks, complicating the comparison of results. Additionally, while success rate is often used as the primary\\nmetric, it may not sufficiently evaluate the performance of real-world tasks involving large foundation models, as latency is not\\ncaptured by the success rate alone. More nuanced evaluation metrics that consider inference time, such as the Compute Aware\\nSuccess Rate (CASR) [225].\\n22', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='fb237000-6349-4dbd-94f5-2a988b5ee9ee', embedding=None, metadata={'page_label': '23', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='ac05e4893d005ba50af61cf5321f59822e777f60bcd19ec6b305752c449e2713', text='Table 2: Tabletop Manipulation\\nTitle Datasets & Simulation Real RobotNumber\\nof TasksBase Model SR SR Descriptions Frequency\\nRT-XKitchen\\nManipulation [292],\\nCable Routing [293],\\nNYU Door\\nOpening [294],\\nAUTOLab UR5 [295],\\nRobot Play [296],\\nBridge [283],\\nRL Bench (RT-1) [27]Google robot 160266 RT-2(PaLI)30%\\n63%\\n92%Bridge,\\nSmall lab datasets,\\nGoogle robot3-10Hz\\nRT-2RL Bench (RT-1) [27]\\nLanguage-Table [253]Google robot 480+ PaLI, PaLM-E93%\\n62%Seen tasks,\\nUnseen tasks1-5Hz\\nRT-1[27] RL Bench (RT-1) [27] Google robot 744 RT-197%\\n76%\\n83%\\n59%Seen Tasks,\\nUnseen Tasks,\\nWith Distractors,\\nNovel background3 Hz\\nGNFactor [198] RL Bench (RT-1) [27] XArm7 166Stable Diffusion,\\nCLIP32%\\n28%Multi-task test,\\nUnseen tasksUnspec.\\nMOO [249]Self-created\\nrobotic dataGoogle robot 106Owl-ViT,\\nRT-192%\\n75%Seen Tasks,\\nUnseen TasksUnspec.\\nPhysObjects [280] PhysObjects [280] Franka Panda 51FlanT5-XXL,\\nGPT-488%\\n88%PhysObjects Test Set,\\nReal scenesopen loop\\nMatcha [297] CoppeliaSim [298] NiCOL 50ViLD,\\ntext-davinci-00391%\\n57%Distinct descriptions,\\nIndistinct descriptionsopen loop\\nScalingup [236] Scalingup benchmark [236] UR5 arm 18 GPT-3 79% Mean Success 35 Hz\\nF3RM [197]PyBullet [299] and\\nSelf-created\\nrobotic dataFranka Panda 18 CLIP78%\\n62%Grasp-Place,\\nObject Manip.Unspec.\\nVIMA [272] VIMABench [272] None 17pre-trained T5\\ntokenizer81%\\n81%\\n79%\\n49%L1,\\nL2,\\nL3,\\nL4Unspec.\\nInstruct2Act [300] VIMABench [272] None 17CLIP,\\nSAM,\\ntext-davinci-003,\\nLLaMA84% Mean Success open loop\\nVoxPoser [227]Saipen [301]\\nWhere2act [302]Franka Emika 13GPT-4,\\nSAM,\\nOwl-ViT88%\\n70%\\n67%Static environments,\\nWith disturbances,\\nUnseen semarios5HZ\\nText2Motion [220] TableEnv [220] Franka Panda 6 text-davinci-003 82% Mean Success open loop\\nGenSim [222]PyBullet [299] and\\nSelf-created\\nrobotic dataXArm7 100+ GPT-453.3%\\n68.8%50 Tasks\\n70 TasksUnspec.\\n23', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e6c834f5-52da-4407-aedb-2e4d471e5e88', embedding=None, metadata={'page_label': '24', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='3220bbafc31c49ef0da9da8be023320feccba1d1c8109b5b28395effebc04643', text='Table 3: Dexterous Manipulation\\nTitle Datasets & Simulation Real RobotNumber\\nof TasksBase Model SR SR Descriptions Frequency\\nRoboCat [195]RGB-Stacking\\nBenchmark [303]Sawyer 7-DoF,\\nPanda 7-DoF,\\nKUKA 14-DoF253 RoboCat82%,\\n74%,\\n86%Sawyer in sim,\\nPanda in sim,\\nKUKA in real10 Hz\\n20 Hz\\nTable 4: Mobile Manipulation\\nTitle Datasets & Simulation Real RobotNumber\\nof TasksBase Model SR SR Descriptions Frequency\\nLLaRP [304]Language\\nRearrangement [304]None 1000 LLaMA-7B42%\\n0%\\n8%\\n39%Total,\\nMultiple objects,\\nSpatial Reasoning,\\nConditional instuct.open loop\\nCode-As-Policies [216]Customized\\nRoboCodeGen [216],\\nHumanEval [305]Eveyday Robots,\\nUR5 Robot arm214Codex,\\nGPT-395%\\n96%RoboCodeGen,\\nHumanEvalopen loop\\nInnerMonlogue [248] Ravens [306];UR5e robot,\\nGoogle robot130ViLD\\nGPT-3.551%\\n50%\\n90%\\n60%Seen Ravens tasks,\\nUnseen Ravens tasks,\\nReal robot arm,\\nGoogle robotUnspec.\\nLIV [307]MetaWorld [308]\\nFrankaKitchen [260]Franka panda 114 LIV30%\\n30%\\n45%FrankaKitchen,\\nMetaWorld,\\nReal robots15hz\\nSayCan [24]proprietary simulator\\nand self-created\\nrobotic dataGoogle robot 101PaLM,\\nFlan74% Mean Success open loop\\nPaLM-E [31] Lang-table [253]Google robot,\\nxArm6100 PaLM-E83%\\n76%\\n52%\\n91%Grasping,\\nStacking,\\nLang-table tasks,\\nGoogle robot1-5Hz\\nTidyBot [309] TidyBot [309] TidyBot 96ViLD,\\nCLIP ,\\ntext-davinci00391%\\n83%Sim,\\nRealopen loop\\nLLM-Grop [310] Gazebo [311]Segway base\\n+ UR5e arm8 GPT-3 4.08Average human\\nrating(1-5)open loop\\nLLM+P [247]International Planning\\nCompetition [223]None 7 GPT-420%\\n90%\\n0%\\n95%\\n85%\\n20%\\n10%Barman,\\nBLOCKSWORLD,\\nFLOORTILE,\\nGRIPPERS,\\nSTORAGE,\\nTERMES,\\nTYREWORLDopen loop\\nHomeRobot [205] Habitat [312] Hello-Robot Stretch 8 - 20% RL baseline closed loop\\n24', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='602b3da1-9f7e-472b-818d-ba3a2df856e0', embedding=None, metadata={'page_label': '25', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='707fea16fa4e88a4f37f3dccd7d099ac8b059cd619a79afd18aa72ed314d6596', text='Table 5: Navigation\\nTitle Datasets & Simulation Real RobotNumber\\nof TasksBase Model SR SR Descriptions Frequency\\nLM-Nav [204] Self-created datasetsClearpath\\nJackal UGV20 GPT-3 80% Mean Success open loop\\nCLIP-Fields [202]Habitat-Matterport\\n3D Semantic [312]Stretch\\nRobot14CLIP\\nDetic79% Mean Success open loop\\nGNM [267] GNM datasets [267]LoCoBot\\nVizbot\\nDJI Tello\\nJackal UGV3 GNM 96% Mean Success Unspec.\\nTable 6: locomotion\\nTitle Datasets & Simulation Real RobotNumber\\nof TasksBase Model SR SR Descriptions Frequency\\nSayTap [26] IsaacGym [171] Unitree A1 30 GPT-4 97% Mean Success openloop\\nPrompt2Walk [229] Mujoco [170] None 1 GPT-4 80% Mean Success 10 Hz\\nTable 7: Multi-Tasks\\nTitle Datasets & Simulation Real RobotNumber\\nof TasksBase Model SR SR Descriptions Frequency\\nGato [30]Meta-World [308],\\nSokoban [313],\\nBabyAI [314],\\nProcgen\\nBenchmark [315],\\nArcade Learning\\nEnvironment [316],\\nDM Control\\nSuite [317]Sawyer arm 604 GPT-497%\\n68%\\n80%\\n68%\\nsuper-human\\n64%Meta-World,\\nSokoban,\\nBabyAI,\\nProcgen,\\nALE Atari,\\nDM Control Suite20 Hz\\nGrounded\\nDecoding [318]2D Maze [319],\\nRavens [306]None 124GPT-3.5\\nPaLM71%\\n46%\\n95%\\n51%Ravens Seen Tasks,\\nRavens Seen Tasks,\\n2D Maze,\\nMobile Manipulationopen loop\\nEureka [182]Issac Gym [171],\\nBidexterous\\nManipulation [320]None 29 GPT-455%\\n3.7\\n1Bi-Dextrous,\\nIsaac Tasks –\\n(Human Score)open loop\\nLang2Reward [226] MuJoCo MPC [321] Google robot 17 GPT-495%\\n80%Quadruped,\\nDextrous manipulatoropen loop\\nVC-1 [264] CortexBench [264] None 17 VC-159%\\n89%\\n67%\\n72%\\n60%\\n70%\\n63%Adroit,\\nMeta-World,\\nDMControl,\\nTrifinger,\\nObjectNav,\\nImageNav,\\nMobile Pickopen loop\\n25', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c8851531-9ccf-4b5e-8bcf-39e474c0116c', embedding=None, metadata={'page_label': '26', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='7dd811edee3d412b7118d51d25fd6a618daf95d3925bd51e2073d6592501e705', text='GPT-4 GPT-3CLIP ViLD PaLM\\nT5-related modelsPaLM-E GPT-3.5 Owl-ViTRT-1\\nLLaMA-7BDetic\\nRoboCatLIVGNM SaMCodexSAM VC-1\\nStable Diffusion\\nBase Model02468# Works Using the Base ModelFigure 11: The histogram showing the number of times different base foundation models are used in developing robotics systems, among\\nthe papers we included in this survey. In the plot we can see GPT-4, GPT-3 are among top choices due to their few-shot promptable nature,\\nas well as accessibility through APIs. CLIP and ViLD are frequently used to bridge image and text representations. Apart from CLIP,\\nT5 family models are frequently used to encode text to get text features. PaLM and PaLM-E are used for robot planning. RT-1, which is\\noriginally developed for manipulation, emerges as a new base model which other manipulation models are built upon.\\n6 Discussions and Future Directions\\n6.1 Remaining Challenges and Open Discussions\\nGrounding for Robot Embodiment Although numerous strategies have been explored to address the problem of grounding,\\nas discussed in Section 9, there are many open challenges in this area. First, grounding needs an effective medium or interface\\nthat bridges concepts and robot actions. Existing interfaces, such as those employing natural language [24, 31] and code\\n[216, 221], are limited. While concepts can be articulated through language and code, they are not universally applicable\\nto nuances such as dexterous body movements. Furthermore, these interfaces often depend on predefined skill libraries\\nthat are not only time-intensive to develop but also lack generalization to new environments. Using reward as an interface\\n[182, 226, 232] may alleviate some of the generalization issues in simulations by acquiring skills dynamically. However, the\\ntime-consuming and potentially unsafe nature of training RL algorithms in the real world raises questions about the feasibility\\nof this method, with real-world validations of its effectiveness yet to be demonstrated.\\nSecond, we need to move from a unimodal notion of grounding, like mapping the word to meaning to a more holistic grounding\\nof multiple sensory modalities. Approaches that rely solely on visual data [280] may capture certain physical properties such\\nas material, transparency, and deformability. Yet, they fall short in grasping concepts like friction, which requires interactive\\ndata with proprioceptive feedback, or the scent of an object, which cannot be acquired without additional modalities such as\\nolfaction.\\nLastly, we should consider grounding from an embodiment perspective. The same task may necessitate distinct actions based\\non the robot’s embodiment; for example, opening a door would require drastically different maneuvers from a humanoid robot\\ncompared to a quadruped. Current research on grounding often emphasizes environmental adaptation while affording less\\nconsideration to how embodiment shapes interaction strategies.\\nSafety and Uncertainty As we pursue deployments of real robots to work alongside humans in factories, to provide elderly\\ncare, or to assist in homes and offices, these autonomous systems (and the foundation models that power them) will require\\nmore effective measures of safety. While formal hardware and software safety checks still apply, the use of foundation models\\nto support provable safety analysis will become an increasingly necessary direction. With the goal of deploying robots to\\nsafety-critical scenarios, prior works have considered leveraging Lyapunov-style safety index functions [122, 323, 324], in\\n26', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='581b386b-5821-4982-b76b-790a65c48d25', embedding=None, metadata={'page_label': '27', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='f9af906e9af2e4594d8b92eb8c11387a9e53b5f3a19401a37c7fc315ad27238e', text='attempts to provide hard safety guarantees for nonlinear systems with complex dynamics and external disturbances (see also\\nSection 3.5). Traditionally, the systems under consideration by the provable safety literature are of low dimension, often\\nrequire careful specification of a world/dynamics model, require specifying an initial safe set and/or set-boundary distance\\nfunctions, require some heuristics and training “tricks” to obtain useful safety value functions that balance conservativeness\\nversus performance, do not naturally support multi-agent settings, and present challenges in safely updating the safety value\\nfunction and growing the safe set online. Herbert et al. [324] synthesized several techniques into a framework—thereby easing\\ncomputation, streamlining updates to the safe sets by one or more orders of magnitude compared to the prior art, and extending\\nHamilton-Jacobi Reachability analysis to 10-dimensional systems that govern quadcopter control. Chen et al. [122] combine\\nRL with HJ Reachability analysis to learn safety value functions from high-dimensional inputs (RGB images, plus vehicle\\nstate), to trade-off a performance-oriented policy and a safety-oriented policy, within a jointly-optimized dual actor-critic\\nframework, for simulated autonomous racing. Tian et al. [325] integrate HJ Reachability analysis in the context of multi-agent\\ninteractions in urban autonomous driving, by formulating the problem as a general-sum Stackelberg game.\\nHowever, in all of these works, open questions remain on integrating socially-acceptable safety constraints and formal\\nguarantees for systems with robotic foundational models. One of the directions is to formulate safety as an affordance [326].\\nThe definition of safety changes based on the capability of the robot and social context. Another focus for safety is to ensure\\nrobust alignment of the robot’s inferred task specification to a human user’s communicative intent. Foundation models offer\\na way to encode the enormous world knowledge, which can serve as commonsense priors to decode the underlying intent.\\nRecent works improve the use of LLMs for robotics with conformal prediction [187] and explicit constraint checking [327].\\nDespite these advances, foundation models currently lack native capacity to reason about the uncertainty associated with their\\noutputs. If properly calibrated, uncertainty quantification in foundation models can be used to trigger fall-back safety measures\\nlike early termination, pre-defined safe maneuvers, or human-in-the-loop interventions.\\nIs there a Dichotomy between End-to-End and Modular Approaches? The human brain serves as an example of a\\nfunctional approach to learning and generalization. While neuroscientists have identified specific regions of the brain, such\\nas the visual cortex, somatosensory cortex, and motor cortex, the brain demonstrates remarkable plasticity and the ability to\\nreorganize its functions to adapt to changes or brain lesions. This flexibility suggests that the brain may have evolved to be\\nmodular as a consequence of unified training, combining specific functionalities while maintaining the capacity for general\\nlearning [328, 329]. Similarly, in “Bertology”, NLP researchers show how local parts of trained networks can specialize in\\none area over others. This indicates that certain modules of large-scale models may become highly specialized for specific\\nfunctions, which can be adapted for downstream tasks without re-training the entire network. This transfer learning approach\\ncan lead to more efficient use of computational resources and faster adaptation to new tasks.\\nIn the context of robotics, taking a premature stand for either modular or end-to-end policy architectures may limit the potential\\nof foundation models for robotics. Modular solutions can provide specific biases and enable effective task-specific performance,\\nbut they may not fully leverage the potential of general learning and transferability. On the other hand, end-to-end solutions\\nhave a history of working well on certain tasks on CV and NLP, but they might not offer the desired flexibility for adaptation to\\nnew situations. As [45] pointed out, there appears to be a misconception about the modular versus end-to-end dichotomy. This\\nis because the former pertains to architecture while the latter relates to optimization – they are not mutually exclusive.\\nRegarding the architecture and optimization design for foudnation models used in robotics, we can focus on a functional\\napproach rather than categorizing it as either modular or end-to-end differentiable. One of the goals of a robotic foundational\\nmodel is to allow flexible modular components, each responsible for specific functionalities, with unified learning that leverages\\nshared representations and general learning capabilities.\\nAdaptability to Physical Changes in Embodiment From employing a pen to flip a light switch to maneuvering down a\\nstaircase with a broken leg encased in a cast, the human brain demonstrates versatile and adaptable reasoning. It is a single\\nunit that controls perceptual understanding, motion control, and dialogue capabilities. For motion control, it adapts to the\\nchanges in the embodiment, due to tool use or injury. This adaptability extends to more profound transformations, such as\\nindividuals learning to paint with their feet or mastering musical instruments with specialized prosthetics. We want to build\\nsuch interactive and adaptable intelligence in Robotics.\\nIn the previous discussions, we saw existing works successfully deploying navigation foundation models for various robot\\n27', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='bd1807c8-db84-45b4-be5f-ff2ed528a001', embedding=None, metadata={'page_label': '28', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='e19094d821513253ba5fc671e31474b1e42aa430ff4580c4e30701efcde56300', text='platforms [29], such as different wheeled robots and quadrupedal robots. We also witnessed the manipulation foundation\\nmodel used in different manipulators [28, 330] which can be used across different robotic platforms, ranging from tabletop\\nrobot arms to mobile manipulators.\\none of the key open research question is how robotics foundational models should enable motion control across different\\nphysical embodiments. Robot policies deployed in homes and offices must be robust to mechanical motion failures, such\\nas sensor malfunctions or actuator breakdowns, ensuring continued functionality in challenging environments. Furthermore,\\nrobotic systems must be designed to adapt to a variety of tools and peripherals, mirroring the human capability to interact\\nwith different instruments for specific tasks and physical tool uses. While some works [331–333] have explored learning\\nrepresentations for diverse tool use, these approaches are yet to be scaled up with foundation models.\\nWorld Model, or Model-agnostic? In classical robotics, especially in planning and control problems, it was common to\\nattempt to model as much as possible about the world that would be needed for robotics tasks. This was often carried out by\\nleveraging structural priors about the tasks, or by relying on heuristics or simplifying assumptions. Certainly, if it was possible\\nto perfectly model the world, solving robotics problems would become a lot simpler. Unfortunately, due to the complexity of\\nthe real world, world modeling in Robotics remains extremely difficult and sometimes even intractable. As a consequence,\\nobtaining policies that generalize across tasks and environments remains a core problem.\\nThe foundation models surveyed in this paper mostly take a model-agnostic (model-free) approach, leveraging the strength\\nof expansive datasets and large-scale deep learning architectures. Some exceptions have attempted to emulate model-based\\napproaches by directly employing LLMs as dynamic models. However, these attempts are still constrained by the inherent\\nlimitations of text-only descriptions and are prone to encountering issues with hallucinations, as discussed in [225, 271].\\nMany researchers would argue [138] that the data-scaled learning paradigm of these foundation models is still quite different\\nfrom how humanity and animals learn, which is in an extreme data- and energy-efficient manner. Achieving even close to\\nthe joint performance and efficiency of human learning ability remains intriguing. In [138], LeCun argues that one possible\\nanswer to resolving that puzzle may lie in the learning of world models, a model that predicts how the state of the world\\ngoing to change as consequences of the actions taken. If we were to develop world models that can emulate the precision\\nof the world’s representation through rigorous mathematical and physical modeling, it would bring us significantly closer\\nto addressing and generalizing complex issues in robotics. These sophisticated and reliable world models would enable\\nthe application of established model-based methodologies, including search-based and sample-based planning, as well as\\ntrajectory optimization techniques. This approach would not only facilitate the resolution of planning and control challenges\\nin robotics but also augment the explainability of these processes. It is posited that the pursuit of a ’foundation world\\nmodel’, characterized by remarkable generalization abilities and zero-shot learning capabilities, holds the potential to be a\\nparadigm-shifting development in the field.\\nNovel Robotics Platforms and Multi-sensory Information As demonstrated in Figure 4c and the Meta-analysis in Tables\\n2-7, existing real robot platforms utilized for deploying foundation models are predominantly limited to gripper-based, single-\\narm robot manipulators. The range of concepts learnable from tasks executed by these hardware systems is restricted, primarily\\nbecause the simple opening and closing actions of a gripper are easily describable by language. To enable robots to achieve\\na level of dexterity and motor skills comparable to those of animals and humans, or to perform complex domestic tasks,\\nit is essential for foundation models to acquire a deeper understanding of physical and household concepts. This learning\\nnecessitates a broader spectrum of information sources, such as diverse sensors (including smell, tactile, and thermal sensors),\\nand more intricate data such as proprioception data from robot platforms with high degrees of freedom.\\nCurrent dexterous manipulators, e.g., Shadow Hand [334], are prohibitively expensive and prone to frequent breakdowns,\\nhence they are predominantly experimented with in simulation. Moreover, tactile sensors are still limited in their application,\\noften confined to the fingertips, as in [335], or offer only low resolution, as observed in the robot-sweater [336]. Furthermore,\\nsince the bulk of data-collection is still conducted through human demonstrations, platforms that facilitate more accurate\\nand efficient data acquisition, such as ALOHA [337] and Leap Hands [284], are gaining popularity. Therefore, we posit\\nthat significant contributions are yet to be made—not only in terms of software innovations, but also in hardware. These\\nadvancements are crucial for providing richer data-collection and, thus, expanding the conceptual space of robotics foundation\\nmodels.\\n28', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='70a099ab-23b5-48a3-91ab-247a3ef984ee', embedding=None, metadata={'page_label': '29', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='841393caae0e4caf8b0704a08faf8a31cea48800ab4959f465ad7c29847ef48b', text='Continual Learning Continual learning broadly refers to the ability to learn and adapt to dynamic and changing environ-\\nments. Specifically, it refers to learning algorithms that can learn and adapt to the underlying training data distribution and\\nchanging learning objective, as they evolve through time.\\nContinual learning is challenging, as neural network models often suffer from catastrophic forgetting, leading to a significant\\ndecrease in overall model performance on prior tasks. One naive solution to mitigate performance degradation due to\\ncatastrophic forgetting involves periodically re-training models with the entire dataset collected, which generally allows\\nmodels to avoid forgetting issues, since the process encompasses both old and new data. However, this method demands\\nsignificant computational and memory resources. In contrast, training or fine-tuning only on new tasks or current data, without\\nrevisiting previous data, is less resource-intensive but incurs catastrophic forgetting due to the model’s tendency to overwrite\\npreviously learned information. This forgetting can be attributed to task interference between old and new data, concept drifts\\nas data distributions evolve over time, and limitations in model expressivity based on their size.\\nAdditionally, with the increasing capacities of models, continuously re-training them on expanding data corpora becomes less\\nfeasible. Recent works in vision and language continual learning [338–341] have proposed various solutions, yet achieving\\neffective continual learning, that can be applied to robotics, still remains a challenging objective. For continual learning,\\nlarge pre-trained foundational models currently face the above challenges and more, primarily because their extensive size\\nmakes retraining more difficult. In Robotics applications, specifically, continual learning is imperative to the deployability\\nof robot learning policies in diverse environments, yet it is still a largely-unexplored domain. Whereas some recent works\\nhave studied various sub-topics of continual learning [342]—e.g., incremental learning [343], rapid motor adaptation [344],\\nhuman-in-the-loop learning [345, 346]—these solutions are often designed for a single task/platform and do not yet consider\\nfoundation models.\\nWe need continual learning algorithms that are designed with machine learning fundamentals in mind and practical real-\\ntime systems considerations. Some open research problems and viable approaches are: (1) mixing different proportions of\\nthe prior data distribution when fine-tuning on latest data to alleviate catastrophic forgetting [347], (2) developing efficient\\nprototypes from prior distributions or curriculum to learn new tasks [348] for task inference, (3) improving training stability\\nand sample-efficiency of online learning algorithms [349, 350], and (4) identifying principled ways to seamlessly incorporate\\nlarge-capacity models into control frameworks (perhaps with hierarchical learning [351–353] / slow-fast control [354]) for\\nreal-time inference.\\nStandardization and Reproducibility The robotics community needs to encourage standardized and reproducible research\\npractices to ensure that published findings can be validated and compared by others. To enable reproducibility at scale, we\\nneed to bridge the gap between simulated environments and real-world hardware and improve the transferability of ML models.\\nHomerobot [205] is a promising step towards enabling both simulation and hardware platforms for open vocabulary pick-\\nand-place tasks. We need to establish standardized task definitions and affordances to handle different robot morphologies,\\nenabling more efficient model development.\\n6.2 Summary\\nIn this survey paper, we analyzed the current research works on foundation models for robotics based on two major categories:\\n(1) works which apply foundation models to robotic tasks, and (2) works attempting to develop robotics foundation models\\nfor robotics tasks using robotics data. We went through the methodologies and experiments of these papers, and provided\\nanalysis and insights based on these research works. Furthermore, we specially covered how these foundation models help\\nresolve the common challenges in robotics. Finally, we discussed remaining challenges in robotics that have not been solved\\nby foundation models, as well as other promising research directions.\\n29', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='4a6a09fe-c726-4836-9c5e-fb87c7df2522', embedding=None, metadata={'page_label': '30', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='ddeafd4efac1e6c61dff8408ffff05b6e630db5a4fbf02044be47208dff87931', text='Disclaimer\\nDue to the rapidly changing nature of the field, we checkpointed this version of literature review on Dec 13th 2023, and might\\nhave missed some relevant work. In addition, due to the rich body of literature and the extensiveness of this survey, there\\nmay be inaccuracies or mistakes in the paper. We welcome readers to send pull requests to our GitHub repository (inside\\nhttps://robotics-fm-survey.github.io/ ) so we may continue to update our references, correct the mistakes and inaccuracies,\\nas well as updating the entries of the meta studies in the paper. Please refer to the contribution guide in the GitHub\\nrepository.\\nAcknowledgments\\nWe would like to thank Vincent Vanhoucke for feedbacks on a draft of this survey paper. In addition, we would like to thank\\nKedi Xu for insightful discussions about the papers list.\\n30', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='9cf78b9b-c868-4eb2-8ecb-e996264324c5', embedding=None, metadata={'page_label': '31', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='28221eb57d67cd8a13d22ca83f55731452590eb19337f6601e471e4d1b5aa6f9', text='References\\n[1] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. International Journal\\nof Robotics Research (IJRR) , 2013. 3, 9\\n[2] Daniel Maturana, Po-Wei Chou, Masashi Uenoyama, and Sebastian Scherer. Real-time semantic mapping for autonomous off-road\\nnavigation. In Field and Service Robotics , pages 335–350. Springer, 2018. 9\\n[3] Berk Calli, Arjun Singh, James Bruce, Aaron Walsman, Kurt Konolige, Siddhartha Srinivasa, Pieter Abbeel, and Aaron M Dollar.\\nYale-cmu-berkeley dataset for robotic manipulation research. In International Journal of Robotics Research , page 261 – 268, 2017. 3\\n[4] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional\\nactivation feature for generic visual recognition. In ICML , 2014. 3\\n[5] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In CVPR , 2017. 3\\n[6] William Shen, Felipe Trevizan, and Sylvie Thi ´ebaux. Learning domain-independent planning heuristics with hypergraph networks.\\nInProceedings of the International Conference on Automated Planning and Scheduling , volume 30, pages 574–584, 2020. 3\\n[7] Beomjoon Kim and Luke Shimanuki. Learning value functions with relational state representations for guiding task-and-motion\\nplanning. In Conference on Robot Learning , pages 955–968. PMLR, 2020.\\n[8] Grady Williams, Paul Drews, Brian Goldfain, James M. Rehg, and Evangelos A. Theodorou. Aggressive driving with model predictive\\npath integral control. In ICRA , 2016. 3, 10\\n[9] Ahmed H Qureshi, Yinglong Miao, Anthony Simeonov, and Michael C Yip. Motion planning networks: Bridging the gap between\\nlearning-based and classical motion planners. IEEE Transactions on Robotics , pages 1–9, 2020. 3, 6\\n[10] Adam Fishman, Adithyavairavan Murali, Clemens Eppner, Bryan Peele, Byron Boots, and Dieter Fox. Motion policy networks. In\\nProceedings of the 6th Conference on Robot Learning (CoRL) , 2022. 3, 6\\n[11] Xue Bin Peng, Erwin Coumans, Tingnan Zhang, Tsang-Wei Lee, Jie Tan, and Sergey Levine. Learning agile robotic locomotion skills\\nby imitating animals. In RSS, 2020. 3, 6\\n[12] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. In Journal of\\nMachine Learning Research , 2016. 5, 6\\n[13] Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco Hutter. Learning\\nagile and dynamic motor skills for legged robots. In Science Robotics , 30 Jan 2019. 6\\n[14] Anusha Nagabandi, Kurt Konoglie, Sergey Levine, and Vikash Kumar. Deep dynamics models for learning dexterous manipulation.\\nInCoRL , 2019. 3, 6\\n[15] Dmitry Kalashnkov and Jake Varley and Yevgen Chebotar and Ben Swanson and Rico Jonschkowski and Chelsea Finn and Sergey\\nLevine and Karol Hausman. Mt-opt: Continuous multi-task robotic reinforcement learning at scale. arXiv:2104.08212 , 2021. 3\\n[16] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. BC-Z:\\nZero-Shot Task Generalization with Robotic Imitation Learning. In 5th Annual Conference on Robot Learning , 2021. 3\\n[17] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav\\nShyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language\\nmodels are few-shot learners, 2020. 3, 8\\n[18] Aditya Ramesh, Prafulla Dhariwal Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip\\nlatents. arXiv preprint arXiv:2204.06125 , 2022. 3, 7\\n[19] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar, Seyed Ghasemipour,\\nBurcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi.\\nPhotorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487 , 2022. 3, 7\\n[20] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties\\nin self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV) , 2021. 3, 7\\n[21] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead,\\nAlexander C. Berg, Wan-Yen Lo, Piotr Doll ´ar, and Ross Girshick. Segment anything. arXiv:2304.02643 , 2023. 7\\n31', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d702f408-7233-4652-bf2d-4d3b3554f432', embedding=None, metadata={'page_label': '32', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='0fdb4fc32c1e535ef6fdf54b7074ec0f166cbb42eab58be6674eae295553ef02', text='[22] Maxime Oquab, Timoth ´ee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza,\\nFrancisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike\\nRabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin,\\nand Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. 3, 5, 7\\n[23] Rishi Bommasani et. al. from the Center for Research on Foundation Models (CRFM) at the Stanford Institute for Human-Centered\\nArtificial Intelligence (HAI). On the opportunities and risks of foundation models. In arXiv:2108.07258 , 2021. 3, 7\\n[24] Ahn et. al. Do as i can, not as i say: Grounding language in robotic affordances. In CoRL , 2022. 3, 11, 14, 16, 20, 24, 26\\n[25] Boyuan Chen, Fei Xia, Brian Ichter, Kanishka Rao, Keerthana Gopalakrishnan, Michael S. Ryoo, Austin Stone, and Daniel Kappler.\\nOpen-vocabulary queryable scene representations for real world planning. In arXiv:2209.09874 , 2022. 12, 13, 16, 20\\n[26] Yujin Tang, Wenhao Yu, Jie Tan, Heiga Zen, Aleksandra Faust, and Tatsuya Harada. Saytap: Language to quadrupedal locomotion,\\n2023. 3, 11, 15, 16, 20, 25\\n[27] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan,\\nKarol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint\\narXiv:2212.06817 , 2022. 3, 9, 11, 12, 18, 20, 21, 22, 23\\n[28] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess,\\nAvinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol\\nHausman, Alex Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel\\nLeal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao,\\nKrista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong\\nTran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun\\nXu, Tianhe Yu, and Brianna Zitkovich. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In arXiv\\npreprint arXiv:2307.15818 , 2023. 3, 11, 16, 17, 18, 19, 20, 22, 28\\n[29] Dhruv Shah, Ajay Sridhar, Nitish Dashora, Kyle Stachowicz, Kevin Black, Noriaki Hirose, and Sergey Levine. Vint: A foundation\\nmodel for visual navigation. In arxiv preprint arXiv:2306.14846 , 2023. 3, 18, 19, 20, 28\\n[30] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez,\\nYury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian\\nChen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. In Transactions on Machine Learning\\nResearch (TMLR) , November 10, 2022. 3, 12, 16, 18, 19, 22, 25\\n[31] Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson,\\nQuan Ho Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent\\nVanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Peter R. Florence. PaLM-E: An embodied\\nmultimodal language model. ArXiv , abs/2303.03378, 2023. 3, 12, 17, 18, 19, 24, 26\\n[32] Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. Challenges and applications\\nof large language models. arXiv:2307.10169 , 2023. 3, 4\\n[33] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, and In So Kweon. Text-to-image diffusion models in generative ai: A\\nsurvey. arXiv:2303.07909 , 2023.\\n[34] Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. Harnessing the\\npower of llms in practice: A survey on chatgpt and beyond. arXiv:2304.13712 , 2023. 4\\n[35] Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale Schuurmans. Foundation models for decision making:\\nProblems, methods, and opportunities. arXiv:2303.04129 , 2023. 3, 4\\n[36] Chaoning Zhang, Fachrina Dewi Puspitasari, Sheng Zheng, Chenghao Li, Yu Qiao, Taegoo Kang, Xinru Shan, Chenshuang Zhang,\\nCaiyan Qin, Francois Rameau, Lik-Hang Lee, Sung-Ho Bae, and Choong Seon Hong. A survey on segment anything model (sam):\\nVision foundation model meets prompt engineering, 2023. 4\\n[37] Muhammad Awais, Muzammal Naseer, Salman Khan, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan\\nYang, and Fahad Shahbaz Khan. Foundational models defining a new era in vision: A survey and outlook, 2023. 4\\n[38] Yifan Du, Zikang Liu, Junyi Li, and Wayne Xin Zhao. A survey of vision-language pre-trained models. IJCAI-2022 survey track ,\\n2022. 4\\n32', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='117d2e3f-df8e-4dfb-b312-269a685d7448', embedding=None, metadata={'page_label': '33', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='cbeb9949f876cf8963adb5307a4a396f549215a2dfb4b7eeac881f9ee4e7a892', text='[39] Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He, Ruotong Liao Gengyuan Zhang, Yao Qin, Volker Tresp, and Philip\\nTorr. A systematic survey of prompt engineering on vision-language foundation models. arXiv:2307.12980 , 2023. 4\\n[40] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin,\\nWayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A survey on large language model based autonomous agents. arXiv:2308.11432 ,\\n2023. 4\\n[41] Jinzhou Lin, Han Gao, Rongtao Xu, Changwei Wang, Man Zhang, Li Guo, and Shibiao Xu. The development of llms for embodied\\nnavigation. In IEEE/ASME TRANSACTIONS ON MECHATRONICS , volume 1, Sept. 2023. 4\\n[42] Anirudha Majumdar. Robotics: An idiosyncratic snapshot in the age of llms, 8 2023. 4\\n[43] Xuan Xiao, Jiahang Liu, Zhipeng Wang, Yanmin Zhou, Yong Qi, Qian Cheng, Bin He, and Shuo Jiang. Robot learning in the era of\\nfoundation models: A survey, 2023. 4\\n[44] Roya Firoozi, Johnathan Tucker, Stephen Tian, Anirudha Majumdar, Jiankai Sun, Weiyu Liu, Yuke Zhu, Shuran Song, Ashish Kapoor,\\nKarol Hausman, Brian Ichter, Danny Driess, Jiajun Wu, Cewu Lu, and Mac Schwager. Foundation models in robotics: Applications,\\nchallenges, and the future, 2023. 4\\n[45] Vincent Vanhoucke. The end-to-end false dichotomy: Roboticists arguing lego vs. playmo. Medium , October 28 2018. 5, 27\\n[46] Yuke Zhu. Cs391r: Robot learning, 2021. 5\\n[47] Jonathan Francis, Nariaki Kitamura, Felix Labelle, Xiaopeng Lu, Ingrid Navarro, and Jean Oh. Core challenges in embodied\\nvision-language planning. Journal of Artificial Intelligence Research , 74:459–515, 2022. 5, 9\\n[48] Shaoqing Ren, Kaiming He, Ross Girshick, , and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal\\nnetworks. In NIPS , 2015. 5\\n[49] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767 , 2018.\\n[50] Nikhil Varma Keetha, Chen Wang, Yuheng Qiu, Kuan Xu, and Sebastian Scherer. Airobject: A temporally evolving graph embedding\\nfor object identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8407–8416,\\n2022.\\n[51] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a\\nmetric space. In NIPS , 2017.\\n[52] Antoni Rosinol, John J Leonard, and Luca Carlone. Nerf-slam: Real-time dense monocular slam with neural radiance fields. arXiv\\npreprint arXiv:2210.13641 , 2022. 5\\n[53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell,\\nPamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language\\nsupervision. In ICML , 2021. 5, 7, 8, 13\\n[54] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D Tardos. Orb-slam: a versatile and accurate monocular slam system. IEEE\\ntransactions on robotics , 31(5):1147–1163, 2015. 5\\n[55] Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry. IEEE transactions on pattern analysis and machine\\nintelligence , 40(3):611–625, 2017.\\n[56] Xiang Gao, Rui Wang, Nikolaus Demmel, and Daniel Cremers. Ldso: Direct sparse odometry with loop closure. In 2018 IEEE/RSJ\\nInternational Conference on Intelligent Robots and Systems (IROS) , pages 2198–2204. IEEE, 2018.\\n[57] Jakob Engel, Thomas Sch ¨ops, and Daniel Cremers. Lsd-slam: Large-scale direct monocular slam. In European conference on\\ncomputer vision , pages 834–849. Springer, 2014.\\n[58] Jon Zubizarreta, Iker Aguinaga, and J. M. M. Montiel. Direct sparse mapping. IEEE Transactions on Robotics , 2020.\\n[59] Shibo Zhao, Peng Wang, Hengrui Zhang, Zheng Fang, and Sebastian Scherer. Tp-tio: A robust thermal-inertial odometry with deep\\nthermalpoint. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , pages 4505–4512. IEEE, 2020.\\n5\\n[60] Wolfgang Hess, Damon Kohler, Holger Rapp, and Daniel Andor. Real-time loop closure in 2d lidar slam. In 2016 IEEE international\\nconference on robotics and automation (ICRA) , pages 1271–1278. IEEE, 2016. 5\\n33', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='7651cb9a-fff5-433a-9036-0af628aa1a3a', embedding=None, metadata={'page_label': '34', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='6ed0cad64db07ecfb266b5845819e19a4ca9aa4b8d9369bbefdebb3cec23a2eb', text='[61] S. Kohlbrecher, J. Meyer, O. von Stryk, and U. Klingauf. A flexible and scalable slam system with full 3d motion estimation. In Proc.\\nIEEE International Symposium on Safety, Security and Rescue Robotics (SSRR) . IEEE, November 2011.\\n[62] Ji Zhang and Sanjiv Singh. Loam: Lidar odometry and mapping in real-time. In Robotics: Science and systems , volume 2, pages\\n1–9. Berkeley, CA, 2014. 5\\n[63] Shuo Yang, Zixin Zhang, Zhengyu Fu, and Zachary Manchester. Cerberus: Low-drift visual-inertial-leg odometry for agile locomotion.\\nICRA , 2023. 5\\n[64] Christian Forster, Luca Carlone, Frank Dellaert, and Davide Scaramuzza. Imu preintegration on manifold for efficient visual-inertial\\nmaximum-a-posteriori estimation. Technical report, EPFL, 2015. 5\\n[65] Ji Zhang and Sanjiv Singh. Visual-lidar odometry and mapping: Low-drift, robust, and fast. In ICRA , 2015. 5\\n[66] Johannes Graeter, Alexander Wilczynski, and Martin Lauer. Limo: Lidar-monocular visual odometry. In 2018 IEEE/RSJ international\\nconference on intelligent robots and systems (IROS) , pages 7872–7879. IEEE, 2018.\\n[67] Thien-Minh Nguyen, Shenghai Yuan, Muqing Cao, Thien Hoang Nguyen, and Lihua Xie. Viral slam: Tightly coupled camera-imu-\\nuwb-lidar slam. arXiv preprint arXiv:2105.03296 , 2021.\\n[68] Tixiao Shan, Brendan Englot, Drew Meyers, Wei Wang, Carlo Ratti, and Daniela Rus. Lio-sam: Tightly-coupled lidar inertial\\nodometry via smoothing and mapping. In 2020 IEEE/RSJ international conference on intelligent robots and systems (IROS) , pages\\n5135–5142. IEEE, 2020.\\n[69] Shibo Zhao, Hengrui Zhang, Peng Wang, Lucas Nogueira, and Sebastian Scherer. Super odometry: Imu-centric lidar-visual-inertial\\nestimator for challenging environments. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , pages\\n8729–8736. IEEE, 2021. 5\\n[70] Sen Wang, Ronald Clark, Hongkai Wen, and Niki Trigoni. Deepvo: Towards end-to-end visual odometry with deep recurrent\\nconvolutional neural networks. In 2017 IEEE international conference on robotics and automation (ICRA) , pages 2043–2050. IEEE,\\n2017. 5\\n[71] Wenshan Wang, Yaoyu Hu, and Sebastian Scherer. Tartanvo: A generalizable learning-based vo. In CoRL , 2020. 5\\n[72] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe. Unsupervised learning of depth and ego-motion from video. In CVPR , 2017. 5\\n[73] Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. In NeurIPS , 2021.\\n[74] Zihan Zhu, Songyou Peng, Viktor Larsson, Zhaopeng Cui, Martin R Oswald, Andreas Geiger, and Marc Pollefeys. Nicer-slam:\\nNeural implicit scene encoding for rgb slam. arXiv preprint arXiv:2302.03594 , 2023.\\n[75] Nikhil Keetha, Jay Karhade, Krishna Murthy Jatavallabhula, Gengshan Yang, Sebastian Scherer, Deva Ramanan, and Jonathon Luiten.\\nSplatam: Splat, track & map 3d gaussians for dense rgb-d slam. arXiv preprint arXiv:2312.02126 , 2023. 5\\n[76] Lerrel Pinto, Dhiraj Gandhi, Yuanfeng Han, Yong-Lae Park, and Abhinav Gupta. The curious robot: Learning visual representations\\nvia physical interactions. In ECCV , 2016. 5\\n[77] Dinesh Jayaraman and Kristen Grauman. Learning to look around: Intelligently exploring unseen environments for unknown tasks.\\nInCVPR , 2018. 5\\n[78] Liren Jin, Xieyuanli Chen, Julius R¨ uckin, and Marija Popovi ´c. Neu-nbv: Next best view planning using uncertainty estimation in\\nimage-based neural rendering. arXiv preprint arXiv:2303.01284 , 2023.\\n[79] Yafei Hu, Junyi Geng, Chen Wang, John Keller, and Sebastian Scherer. Off-Policy Evaluation with Online Adaptation for Robot\\nExploration in Challenging Environments. In IEEE Robotics and Automation Letters (RA-L) , 2023. 5\\n[80] Peter E. Hart, Nils J. Nilsson, and Bertram Raphael. A formal basis for the heuristic determination of minimum cost paths. IEEE\\nTransactions on Systems Science and Cybernetics , 4(2):100–107, 1968. 5\\n[81] Venkatraman Narayanan, Mike Phillips, and Maxim Likhachev. Anytime safe interval path planning for dynamic environments. In\\n2012 IEEE/RSJ International Conference on Intelligent Robots and Systems , pages 4708–4715, 2012.\\n[82] Sandip Aine, Siddharth Swaminathan, Venkatraman Narayanan, Victor Hwang, and Maxim Likhachev. Multi-heuristic A. In Dieter\\nFox, Lydia E. Kavraki, and Hanna Kurniawati, editors, Robotics: Science and Systems X, University of California, Berkeley, USA,\\nJuly 12-16, 2014 , 2014.\\n34', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8146af7c-1914-4441-93a5-de505536b1f5', embedding=None, metadata={'page_label': '35', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='8ec1ff60abdd90ee05c15547af666b84f8ef5ba48a3cfd749b86b9e2f946671f', text='[83] Brian MacAllister, Jonathan Butzke, Alex Kushleyev, Harsh Pandey, and Maxim Likhachev. Path planning for non-circular micro\\naerial vehicles in constrained environments. In 2013 IEEE International Conference on Robotics and Automation , pages 3933–3940,\\n2013.\\n[84] Benjamin J. Cohen, Sachin Chitta, and Maxim Likhachev. Single- and dual-arm motion planning with heuristic search. Int. J. Robotics\\nRes., 33(2):305–320, 2014. 5\\n[85] Steven M LaValle et al. Rapidly-exploring random trees: A new tool for path planning. Technical report, Iowa State University, 1998.\\n6\\n[86] J.J. Kuffner and S.M. LaValle. Rrt-connect: An efficient approach to single-query path planning. In Proceedings 2000 ICRA.\\nMillennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065) ,\\nvolume 2, pages 995–1001 vol.2, 2000.\\n[87] L.E. Kavraki, P. Svestka, J.-C. Latombe, and M.H. Overmars. Probabilistic roadmaps for path planning in high-dimensional\\nconfiguration spaces. IEEE Transactions on Robotics and Automation , 12(4):566–580, 1996.\\n[88] Sertac Karaman and Emilio Frazzoli. Sampling-based algorithms for optimal motion planning. The international journal of robotics\\nresearch , 30(7):846–894, 2011.\\n[89] Jonathan D Gammell, Siddhartha S Srinivasa, and Timothy D Barfoot. Batch informed trees (bit): Sampling-based optimal planning\\nvia the heuristically guided search of implicit random geometric graphs. In 2015 IEEE international conference on robotics and\\nautomation (ICRA) , pages 3067–3074. IEEE, 2015.\\n[90] Sanjiban Choudhury, Jonathan D Gammell, Timothy D Barfoot, Siddhartha S Srinivasa, and Sebastian Scherer. Regionally accelerated\\nbatch informed trees (rabit): A framework to integrate local information into optimal path planning. In 2016 IEEE International\\nConference on Robotics and Automation (ICRA) , pages 4207–4214. IEEE, 2016. 6\\n[91] Malik Ghallab, Dana Nau, and Paolo Traverso. Automated Planning and Acting . Cambridge University Press, 2016. 6\\n[92] Caelan Reed Garrett, Rohan Chitnis, Rachel Holladay, Beomjoon Kim, Tom Silver, Leslie Pack Kaelbling, and Tomas Lozano-P ´erez.\\nIntegrated Task and Motion Planning. In arXiv:2010.01083 , 2010. 6, 10\\n[93] Dhruv Shah, Arjun Bhorkar, Hrish Leen, Ilya Kostrikov, Nick Rhinehart, and Sergey Levine. Offline reinforcement learning for visual\\nnavigation. In CoRL , 2022. 6\\n[94] Kyle Stachowicz, Arjun Bhorkar, Dhruv Shah, Ilya Kostrikov, and Sergey Levine. Fastrlap: A system for learning high-speed driving\\nvia deep rl and autonomous practicing. arXiv pre-print , 2023. 6\\n[95] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on\\nopen problems. In NeurIPS 2020 Tutorial , 2020. 6\\n[96] Fangkai Yang, Daoming Lyu, Bo Liu, and Steven Gustafson. Peorl: Integrating symbolic planning and hierarchical reinforcement\\nlearning for robust decision-making. arXiv preprint arXiv:1804.07779 , 2018. 6\\n[97] Yuqian Jiang, Fangkai Yang, Shiqi Zhang, and Peter Stone. Task-motion planning with reinforcement learning for adaptable mobile\\nservice robots. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , pages 7529–7534. IEEE, 2019.\\n6\\n[98] Garrett Andersen and George Konidaris. Active exploration for learning symbolic representations. Advances in Neural Information\\nProcessing Systems , 30, 2017. 6\\n[99] George Konidaris, Leslie Pack Kaelbling, and Tomas Lozano-Perez. From skills to symbols: Learning symbolic representations for\\nabstract high-level planning. Journal of Artificial Intelligence Research , 61:215–289, 2018. 6\\n[100] Brian Paden, Michal ˇC´ap, Sze Zheng Yong, Dmitry Yershov, and Emilio Frazzoli. A survey of motion planning and control techniques\\nfor self-driving urban vehicles. IEEE Transactions on intelligent vehicles , 1(1):33–55, 2016. 6\\n[101] Jonathan Francis, Bingqing Chen, Weiran Yao, Eric Nyberg, and Jean Oh. Distribution-aware goal prediction and conformant\\nmodel-based planning for safe autonomous driving. ICML Workshop on Safe Learning for Autonomous Driving , 2022. 6\\n[102] James Herman, Jonathan Francis, Siddha Ganju, Bingqing Chen, Anirudh Koul, Abhinav Gupta, Alexey Skabelkin, Ivan Zhukov,\\nMax Kumskoy, and Eric Nyberg. Learn-to-race: A multimodal control environment for autonomous racing. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision , pages 9793–9802, 2021. 9, 22\\n35', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8e63de2e-a712-42ce-8124-2deec1f9bae3', embedding=None, metadata={'page_label': '36', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='ed4bfd4019ac073c1d75f61effeb54d87d6d14c97a0499ec9cf4a796ee38f06e', text='[103] Jonathan Francis, Bingqing Chen, Siddha Ganju, Sidharth Kathpal, Jyotish Poonganam, Ayush Shivani, Vrushank Vyas, Sahika Genc,\\nIvan Zhukov, Max Kumskoy, et al. Learn-to-race challenge 2022: Benchmarking safe learning and cross-domain generalisation in\\nautonomous racing. ICML Workshop on Safe Learning for Autonomous Driving , 2022. 6, 9\\n[104] Grady Williams, Andrew Aldrich, and Evangelos A Theodorou. Model predictive path integral control: From theory to parallel\\ncomputation. Journal of Guidance, Control, and Dynamics , 40(2):344–357, 2017. 6\\n[105] Christopher G Atkeson and Stefan Schaal. Robot learning from demonstration. In ICML , 1997. 6\\n[106] Jens Kober, J. Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The International Journal of Robotics\\nResearch , 2013. 6\\n[107] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS ,\\n2012. 6\\n[108] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller.\\nPlaying atari with deep reinforcement learning. In NIPS Deep Learning Workshop , 2013. 6\\n[109] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis\\nAntonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever,\\nTimothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep\\nneural networks and tree search. In Nature , 2016. 6\\n[110] Elia Kaufmann, Antonio Loquercio, Ren ´e Ranftl, Matthias M¨ uller, Vladlen Koltun, and Davide Scaramuzza. Deep drone acrobatics.\\nInProceedings of Robotics: Science and Systems , Corvalis, Oregon, USA, July 2020. 6\\n[111] Sergey Levine, Peter Pastor, Alex Krizhevsky, and Deirdre Quillen. Learning hand-eye coordination for robotic grasping with deep\\nlearning and large-scale data collection. In arXiv:1603.02199 , 2016. 6, 9\\n[112] Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal\\nKalakrishnan, Vincent Vanhoucke, and Sergey Levine. Qt-opt: Scalable deep reinforcement learning for vision-based robotic\\nmanipulation. In CoRL , 2018. 9, 10, 18\\n[113] Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning quadrupedal locomotion over\\nchallenging terrain. In Science Robotics , 21 Oct 2020. 6\\n[114] Stephane Ross, Geoffrey J. Gordon, and J. Andrew Bagnell. A reduction of imitation learning and structured prediction to no-regret\\nonline learning. In AISTATS , 2011. 6\\n[115] Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In ICML , 2004. 6\\n[116] Brian D. Ziebart, Andrew Maas, J.Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse reinforcement learning. In AAAI ,\\n2008. 6\\n[117] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua\\nBengio. Generative adversarial networks. In NIPS , 2014. 6\\n[118] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In NIPS , 2016. 6\\n[119] Yunpeng Pan, Ching-An Cheng, Kamil Saigol, Keuntaek Lee, Xinyan Yan, Evangelos Theodorou, and Byron Boots. Agile autonomous\\ndriving using end-to-end deep imitation learning. In RSS, 2018. 6\\n[120] Chenhao Li, Marin Vlastelica, Sebastian Blaes, Jonas Frey, Felix Grimminger, and Georg Martius. Learning agile skills via adversarial\\nimitation of rough partial demonstrations. In CoRL , 2022. 6\\n[121] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction , second edition . The MIT Press, 2018. 6\\n[122] Bingqing Chen, Jonathan Francis, Jean Oh, Eric Nyberg, and Sylvia L Herbert. Safe autonomous racing via approximate reachability\\non ego-vision. arXiv preprint arXiv:2110.07699 , 2021. 6, 11, 26, 27\\n[123] Deepak Pathak Zipeng Fu, Xuxin Cheng. Deep whole-body control: Learning a unified policy for manipulation and locomotion. In\\nCoRL , 2022. 6\\n[124] Xuxin Cheng, Kexin Shi, Ananye Agarwal, and Deepak Pathak. Extreme parkour with legged robots. In arXiv:2309.14341 , 2023. 6\\n[125] Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using\\nprobabilistic dynamics models. In Neural Information Processing Systems , 2018. 6\\n36', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='df56e95a-8673-4582-b9f6-0e8e00f7ae75', embedding=None, metadata={'page_label': '37', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='cce93dff0a4dc84539518e6a8a2b2480099c7e9dc83e893073571bb9347e4a27', text='[126] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through video prediction. In NIPS ,\\n2016. 6\\n[127] Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In ICRA , 2017. 6\\n[128] S. Levine I. Kostrikov, A. Nair. Offline reinforcement learning with implicit q-learning. In ICLR , 2022. 6\\n[129] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y. Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo:\\nModel-based offline policy optimization. In NeurIPS , 2020. 6\\n[130] Wenxuan Zhou, Sujay Bajracharya, and David Held. Plas: Latent action space for offline reinforcement learning. In Conference on\\nRobot Learning (CoRL) , 2020. 6\\n[131] Rafael Rafailov, Tianhe Yu, Aravind Rajeswaran, and Chelsea Finn. Offline reinforcement learning from images with latent space\\nmodels. In Proceedings of Machine Learning Research , volume 144:1–15, 2021. 6\\n[132] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision\\nlearners. In CVPR , 2022. 7\\n[133] Namuk Park, Wonjae Kim, Byeongho Heo, Taekyung Kim, and Sangdoo Yun. What do self-supervised vision transformers learn?\\nInICLR , 2023. 7\\n[134] Shashank Shekhar, Florian Bordes, Pascal Vincent, and Ari Morcos. Objectives matter: Understanding the impact of self-supervised\\nobjectives on vision transformer representations. arXiv:2304.13089 , 2023. 7\\n[135] Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao Gu, et al. Conceptfusion: Open-set multimodal 3d mapping. RSS,\\n2023.\\n[136] Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel. Deep vit features as dense visual descriptors. arXiv:2112.05814 , 2021.\\n[137] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-vocabulary panoptic segmentation\\nwith text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\\npages 2955–2966, 2023. 7\\n[138] Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review , 62, 2022. 7, 28\\n[139] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind:\\nOne embedding space to bind them all. arXiv preprint arXiv:2211.05778 , 2022. 7, 8\\n[140] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch,\\nBernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R ´emi Munos, and\\nMichal Valko. Bootstrap your own latent: A new approach to self-supervised learning. In NeurIPS , 2020. 7\\n[141] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas.\\nSelf-supervised learning from images with a joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition , pages 15619–15629, 2023. 7\\n[142] Adrien Bardes, Jean Ponce, and Yann LeCun. Mc-jepa: A joint-embedding predictive architecture for self-supervised learning of\\nmotion and content features. arXiv preprint arXiv:2307.12698 , 2023. 7\\n[143] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint arXiv:2006.11239 , 2006. 7\\n[144] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew Ilya Sutskever, and Mark Chen.\\nGlide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741 ,\\n2021. 7\\n[145] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training.\\nTechnical report, OpenAI, 2018. 7\\n[146] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask\\nlearners. Technical report, OpenAI, 2019. 8\\n[147] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for\\nlanguage understanding, 2019. 8\\n[148] Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. Harnessing the\\npower of llms in practice: A survey on chatgpt and beyond, 2023. 8\\n37', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='e93254f0-39e6-4475-b691-8d0024149bf7', embedding=None, metadata={'page_label': '38', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='9a69d918fc0c0a5d3fc53a6cbd7b2d69d2dcc2bd7d3a2e12d26323b53016d913', text='[149] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human\\npreferences. Advances in neural information processing systems , 30, 2017. 8\\n[150] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and language knowledge\\ndistillation. In ICLR , 2022. 8\\n[151] Boyi Li, Kilian Q. Weinberger, Serge Belongie, Vladlen Koltun, and Ren ´e Ranftl. Language-driven semantic segmentation. In ICLR ,\\n2022. 8, 13\\n[152] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-\\nlanguage tasks. In arXiv:1908.02265 , 2019. 8\\n[153] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.\\nScaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine\\nLearning , 2021. 8\\n[154] Hangbo Bao, Wenhui Wang, Li Dong, and Furu Wei. Vl-beit: Generative vision-language pretraining, 2022. 8\\n[155] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie\\nMillican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne\\nMonteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo\\nBarreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. ArXiv ,\\nabs/2204.14198, 2022. 8\\n[156] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. GIT: A\\ngenerative image-to-text transformer for vision and language. Transactions on Machine Learning Research , 2022. 8\\n[157] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping language-image pre-training for unified vision-\\nlanguage understanding and generation. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore,\\nMaryland, USA , 2022. 8\\n[158] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image\\nencoders and large language models. CoRR , abs/2301.12597, 2023. 8\\n[159] OpenAI. Gpt-4 technical report. ArXiv , abs/2303.08774, 2023. 8\\n[160] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm, 2023. 8\\n[161] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang,\\nJinglin Liu, Yi Ren, Zhou Zhao, and Shinji Watanabe. Audiogpt: Understanding and generating speech, music, sound, and talking\\nhead, 2023. 8\\n[162] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. Speechgpt: Empowering large language\\nmodels with intrinsic cross-modal conversational abilities, 2023. 8\\n[163] Le Xue, Mingfei Gao, Chen Xing, Roberto Mart ´ın-Mart ´ın, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, and Silvio\\nSavarese. Ulip: Learning a unified representation of language, images, and point clouds for 3d understanding, 2023. 8\\n[164] Peide Huang, Xilun Zhang, Ziang Cao, Shiqi Liu, Mengdi Xu, Wenhao Ding, Jonathan Francis, Bingqing Chen, and Ding Zhao.\\nWhat went wrong? closing the sim-to-real gap via differentiable causal discovery. In 7th Annual Conference on Robot Learning ,\\n2023. 9, 22\\n[165] Jonathan Francis. Knowledge-enhanced Representation Learning for Multiview Context Understanding . PhD thesis, Carnegie Mellon\\nUniversity, 2022. 9\\n[166] Gyan Tatiya, Jonathan Francis, and Jivko Sinapov. Transferring implicit knowledge of non-visual object properties across heteroge-\\nneous robot morphologies. In 2023 IEEE International Conference on Robotics and Automation (ICRA) , pages 11315–11321. IEEE,\\n2023. 9, 14\\n[167] Gyan Tatiya, Jonathan Francis, and Jivko Sinapov. Cross-tool and cross-behavior perceptual knowledge transfer for grounded object\\nrecognition. arXiv preprint arXiv:2303.04023 , 2023. 9, 14\\n[168] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai,\\nBenjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy\\nGao, Aditya Joshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous\\n38', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='cabfbbe6-2008-4575-a512-d0166ae6d6dc', embedding=None, metadata={'page_label': '39', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='e98f149ac3ff623c2b05e8f61501729751d897e2462ef1a5f4374d2a549374b5', text='driving: Waymo open dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) ,\\nJune 2020. 9\\n[169] Alexander Herzog*, Kanishka Rao*, Karol Hausman*, Yao Lu*, Paul Wohlhart*, Mengyuan Yan, Jessica Lin, Montserrat Gonzalez\\nArenas, Ted Xiao, Daniel Kappler, Daniel Ho, Jarek Rettinghouse, Yevgen Chebotar, Kuang-Huei Lee, Keerthana Gopalakrishnan,\\nRyan Julian, Adrian Li, Chuyuan Kelly Fu, Bob Wei, Sangeetha Ramesh, Khem Holden, Kim Kleiven, David Rendleman, Sean\\nKirmani, Jeff Bingham, Jon Weisz, Ying Xu, Wenlong Lu, Matthew Bennice, Cody Fong, David Do, Jessica Lam, Yunfei Bai, Benjie\\nHolson, Michael Quinlan, Noah Brown, Mrinal Kalakrishnan, Julian Ibarz, Peter Pastor, and Sergey Levine. Deep rl at scale: Sorting\\nwaste in office buildings with a fleet of mobile manipulators. In Robotics: Science and Systems (RSS) , 2023. 9\\n[170] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ international\\nconference on intelligent robots and systems , pages 5026–5033. IEEE, 2012. 9, 22, 25\\n[171] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin,\\nArthur Allshire, Ankur Handa, and Gavriel State. Isaac gym: High performance gpu-based physics simulation for robot learning,\\n2021. 22, 25\\n[172] Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu, Nikita Rudin, David Hoeller, Jia Lin Yuan, Pooria Poorsarvi Tehrani, Ritvik\\nSingh, Yunrong Guo, Hammad Mazhar, Ajay Mandlekar, Buck Babich, Gavriel State, Marco Hutter, and Animesh Garg. Orbit: A\\nunified simulation framework for interactive robot learning environments, 2023.\\n[173] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen\\nKoltun, Jitendra Malik, et al. Habitat: A platform for embodied ai research. In Proceedings of the IEEE/CVF international conference\\non computer vision , pages 9339–9347, 2019.\\n[174] Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra\\nChaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel\\nChang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. Habitat 2.0: Training home assistants to\\nrearrange their habitat, 2022. 21\\n[175] Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Tsung-Yen Yang, Ruslan Partsey, Ruta Desai, Alexander William\\nClegg, Michal Hlavac, So Yeon Min, Vladim ´ır Vondru ˇs, Theophile Gervet, Vincent-Pierre Berges, John M. Turner, Oleksandr\\nMaksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra Malik, Devendra Singh Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai, and\\nRoozbeh Mottaghi. Habitat 3.0: A co-habitat for humans, avatars and robots, 2023. 9, 21\\n[176] Embodiment Collaboration. Open x-embodiment: Robotic learning datasets and rt-x models, 2023. 9, 18, 20, 21, 22\\n[177] Elena Arcari, Maria Vittoria Minniti, Anna Scampicchio, Andrea Carron, Farbod Farshidian, Marco Hutter, and Melanie N. Zeilinger.\\nBayesian multi-task learning mpc for robotic mobile manipulation, 2023. 10\\n[178] Chao Cao, Hongbiao Zhu, Howie Choset, and Ji Zhang. TARE: A Hierarchical Framework for Efficiently Exploring Complex 3D\\nEnvironments. In ICRA , 2023. 10\\n[179] Fahad Islam, Oren Salzman, Aditya Agarwal, and Maxim Likhachev. Provably constant-time planning and replanning for real-time\\ngrasping objects off a conveyor belt. In RSS, 2020. 10\\n[180] Dhruv Mauria Saxena, Muhammad Suhail Saleem, and Maxim Likhachev. Manipulation planning among movable obstacles using\\nphysics-based adaptive motion primitives. In 2021 IEEE International Conference on Robotics and Automation (ICRA) . IEEE, May\\n2021. 10\\n[181] Yuchen Cui, Scott Niekum, Abhinav Gupta, Vikash Kumar, and Aravind Rajeswaran. Can foundation models perform zero-shot task\\nspecification for robot manipulation? In Learning for Dynamics and Control Conference , pages 893–905. PMLR, 2022. 11, 20\\n[182] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and\\nAnima Anandkumar. Eureka: Human-level reward design via coding large language models. In 2nd Workshop on Language and\\nRobot Learning: Language as Grounding , 2023. 11, 15, 20, 25, 26\\n[183] Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe,\\nRudolph Triebel, Peter Jung, Ribana Roscher, et al. A survey of uncertainty in deep neural networks. Artificial Intelligence Review ,\\n56(Suppl 1):1513–1589, 2023. 11\\n[184] Xuhong Li, Haoyi Xiong, Xingjian Li, Xuanyu Wu, Xiao Zhang, Ji Liu, Jiang Bian, and Dejing Dou. Interpretable deep learning:\\nInterpretation, interpretability, trustworthiness, and beyond. Knowledge and Information Systems , 64(12):3197–3234, 2022. 11\\n39', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='5c5bd423-3837-488c-af28-b0c6daf1c459', embedding=None, metadata={'page_label': '40', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='24f9d11830f0a181a56fe1bf9c95a20b553cf833d8bfc0a875b3ae4ba60185bd', text='[185] Ta-Chung Chi, Minmin Shen, Mihail Eric, Seokhwan Kim, and Dilek Hakkani-tur. Just ask: An interactive learning framework for\\nvision and language navigation. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 34, pages 2459–2466,\\n2020. 11\\n[186] Anastasios N Angelopoulos and Stephen Bates. A gentle introduction to conformal prediction and distribution-free uncertainty\\nquantification. arXiv preprint arXiv:2107.07511 , 2021. 11\\n[187] Allen Z. Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake\\nVarley, Zhenjia Xu, Dorsa Sadigh, Andy Zeng, and Anirudha Majumdar. Robots that ask for help: Uncertainty alignment for large\\nlanguage model planners. In 7th Annual Conference on Robot Learning , 2023. 11, 20, 27\\n[188] Kim P Wabersich, Andrew J Taylor, Jason J Choi, Koushil Sreenath, Claire J Tomlin, Aaron D Ames, and Melanie N Zeilinger.\\nData-driven safety filters: Hamilton-jacobi reachability, control barrier functions, and predictive methods for uncertain systems. IEEE\\nControl Systems Magazine , 43(5):137–177, 2023. 11\\n[189] Kai-Chieh Hsu, Haimin Hu, and Jaime Fern ´andez Fisac. The safety filter: A unified view of safety-critical control in autonomous\\nsystems. arXiv preprint arXiv:2309.05837 , 2023. 11\\n[190] Aaron D Ames, Samuel Coogan, Magnus Egerstedt, Gennaro Notomista, Koushil Sreenath, and Paulo Tabuada. Control barrier\\nfunctions: Theory and applications. In 2019 18th European control conference (ECC) , pages 3420–3431. IEEE, 2019. 11\\n[191] Somil Bansal, Mo Chen, Sylvia Herbert, and Claire J Tomlin. Hamilton-jacobi reachability: A brief overview and recent advances.\\nIn2017 IEEE 56th Annual Conference on Decision and Control (CDC) , pages 2242–2253. IEEE, 2017. 11\\n[192] Karen Leung, Nikos Ar ´echiga, and Marco Pavone. Backpropagation through signal temporal logic specifications: Infusing logical\\nstructure into gradient-based methods. The International Journal of Robotics Research , 42(6):356–370, 2023. 11\\n[193] Shangding Gu, Long Yang, Yali Du, Guang Chen, Florian Walter, Jun Wang, Yaodong Yang, and Alois Knoll. A review of safe\\nreinforcement learning: Methods, theory and applications. arXiv preprint arXiv:2205.10330 , 2022. 11\\n[194] Charles Dawson, Sicun Gao, and Chuchu Fan. Safe control with learned certificates: A survey of neural lyapunov, barrier, and\\ncontraction methods for robotics and control. IEEE Transactions on Robotics , 2023. 11\\n[195] Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Coline Devin, Alex X. Lee, Maria Bauza, Todor Davchev, Yuxiang Zhou,\\nAgrim Gupta, Akhil Raju, Antoine Laurens, Claudio Fantacci, Valentin Dalibard, Martina Zambelli, Murilo Martins, Rugile Pevcevi-\\nciute, Michiel Blokzijl, Misha Denil, Nathan Batchelor, Thomas Lampe, Emilio Parisotto, Konrad ˙Zo lna, Scott Reed, Sergio G ´omez\\nColmenarejo, Jon Scholz, Abbas Abdolmaleki, Oliver Groth, Jean-Baptiste Regli, Oleg Sushkov, Tom Roth ¨orl, Jos ´e Enrique Chen,\\nYusuf Aytar, Dave Barker, Joy Ortiz, Martin Riedmiller, Jost Tobias Springenberg, Raia Hadsell, Francesco Nori, and Nicolas Heess.\\nRobocat: A self-improving foundation agent for robotic manipulation, 2023. 12, 18, 20, 24\\n[196] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In CoRL , 2021. 12, 13\\n[197] William Shen, Ge Yang, Alan Yu, Jansen Wong, Leslie Pack Kaelbling, and Phillip Isola. Distilled feature fields enable few-shot\\nlanguage-guided manipulation. CoRL , 2023. 13, 23\\n[198] Yanjie Ze, Ge Yan, Yueh-Hua Wu, Annabella Macaluso, Yuying Ge, Jianglong Ye, Nicklas Hansen, Li Erran Li, and Xiaolong Wang.\\nMulti-task real robot learning with generalizable neural feature fields. CoRL , 2023. 12, 13, 23\\n[199] Reihaneh Mirjalili, Michael Krawez, and Wolfram Burgard. Fm-loc: Using foundation models for improved vision-based localization.\\narXiv:2304.07058 , 2023. 13\\n[200] Gyan Tatiya, Jonathan Francis, Ho-Hsiang Wu, Yonatan Bisk, and Jivko Sinapov. Mosaic: Learning unified multi-sensory object\\nproperty representations for robot perception. arXiv preprint arXiv:2309.08508 , 2023. 13, 14, 21\\n[201] Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram Burgard. Visual language maps for robot navigation. In Proceedings of the\\nIEEE International Conference on Robotics and Automation (ICRA) , London, UK, 2023. 12, 13, 16, 22\\n[202] Nur Muhammad (Mahi) Shafiullah, Chris Paxton, Lerrel Pinto1 Soumith Chintala, and Arthur Szlam. Clip-fields: Weakly supervised\\nsemantic fields for robotic memory. In RSS, 2023. 13, 16, 25\\n[203] Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, Shuang Li, Ganesh Iyer, Soroush\\nSaryazd, Nikhil Keetha, Ayush Tewari, Joshua B. Tenenbaum, Celso Miguel de Melo, Madhava Krishna, Liam Paull, Florian Shkurti,\\nand Antonio Torralba. Conceptfusion: Open-set multimodal 3d mapping. In arXiv:2302.07241 , 2023. 13\\n[204] Dhruv Shah, Blazej Osinski, Brian Ichter, and Sergey Levine. Lm-nav: Robotic navigation with large pre-trained models of language,\\nvision, and action. In CoRL , 2022. 12, 13, 25\\n40', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='ebab9f07-35fa-4ae8-ae7c-5b36857ec067', embedding=None, metadata={'page_label': '41', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='7e9b50a600e20ebe2faa686bf5a00d65feb05df98b9087c4d95468be1c9d6493', text='[205] Sriram Yenamandra, Arun Ramachandran, Mukul Khanna, Karmesh Yadav, Devendra Singh Chaplot, Gunjan Chhablani, Alexander\\nClegg, Theophile Gervet, Vidhi Jain, Ruslan Partsey, Ram Ramrakhya, Andrew Szot, Tsung-Yen Yang, Aaron Edsinger, Charlie\\nKemp, Binit Shah, Zsolt Kira, Dhruv Batra, Roozbeh Mottaghi, Yonatan Bisk, and Chris Paxton. The homerobot open vocab mobile\\nmanipulation challenge. In Thirty-seventh Conference on Neural Information Processing Systems: Competition Track , 2023. 12, 24,\\n29\\n[206] Theophile Gervet, Zhou Xian, Nikolaos Gkanatsios, and Katerina Fragkiadaki. Act3d: 3d feature field transformers for multi-task\\nrobotic manipulation, 2023. 13\\n[207] Christina Kassab, Matias Mattamala, Lintong Zhang, and Maurice Fallon. Language-extended indoor slam (lexis): A versatile system\\nfor real-time visual scene understanding. arXiv preprint arXiv:2309.15065 , 2023. 13\\n[208] Nikhil Keetha, Avneesh Mishra, Jay Karhade, Krishna Murthy Jatavallabhula, Sebastian Scherer, Madhava Krishna, and Sourav Garg.\\nAnyloc: Towards universal visual place recognition. RA-L , 2023. 13\\n[209] Yao He, Ivan Cisneros, Nikhil Keetha, Jay Patrikar, Zelin Ye, Ian Higgins, Yaoyu Hu, Parv Kapoor, and Sebastian Scherer. Foundloc:\\nVision-based onboard aerial localization in the wild, 2023. 13\\n[210] Jivko Sinapov, Connor Schenck, Kerrick Staley, Vladimir Sukhoy, and Alexander Stoytchev. Grounding semantic categories in\\nbehavioral interactions: Experiments with 100 objects. Robotics and Autonomous Systems , 62(5):632–645, may 2014. 14\\n[211] Jivko Sinapov, Connor Schenck, and Alexander Stoytchev. Learning relational object categories using behavioral exploration and\\nmultimodal perception. In International Conference on Robotics and Automation (ICRA) , pages 5691–5698, Hong Kong, China, may\\n2014. IEEE.\\n[212] Mevlana C. Gemici and Ashutosh Saxena. Learning haptic representation for manipulating deformable food objects. In Intelligent\\nRobots and Systems (IROS) , pages 638–645, Chicago, IL, USA, Sep 2014. IEEE.\\n[213] Gyan Tatiya, Ramtin Hosseini, Michael C. Hughes, and Jivko Sinapov. Sensorimotor cross-behavior knowledge transfer for grounded\\ncategory recognition. In International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob) . IEEE,\\n2019.\\n[214] Gyan Tatiya, Ramtin Hosseini, Michael Hughes, and Jivko Sinapov. A framework for sensorimotor cross-perception and cross-behavior\\nknowledge transfer for object categorization. Frontiers in Robotics and AI , 7:137, 2020.\\n[215] Gyan Tatiya, Yash Shukla, Michael Edegware, and Jivko Sinapov. Haptic knowledge transfer between heterogeneous robots using\\nkernel manifold alignment. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) . IEEE, 2020. 14\\n[216] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies:\\nLanguage model programs for embodied control. ArXiv , abs/2209.07753, 2023. 14, 24, 26\\n[217] Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua B\\nTenenbaum, et al. Video language planning. arXiv preprint arXiv:2310.10625 , 2023. 14\\n[218] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S¨ underhauf, Ian Reid, Stephen Gould, and Anton Van\\nDen Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In\\nProceedings of the IEEE conference on computer vision and pattern recognition , pages 3674–3683, 2018. 14\\n[219] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable\\nknowledge for embodied agents, 2022. 14, 16\\n[220] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette Bohg. Text2motion: From natural language instructions\\nto feasible plans. arXiv preprint arXiv:2303.12153 , 2023. 14, 23\\n[221] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and\\nAnimesh Garg. ProgPrompt: Generating situated robot task plans using large language models, 2022. 14, 16, 26\\n[222] Lirui Wang, Yiyang Ling, Zhecheng Yuan, Mohit Shridhar, Chen Bao, Yuzhe Qin, Bailin Wang, Huazhe Xu, and Xiaolong Wang.\\nGensim: Generating robotic simulation tasks via large language models. In CoRL , 2023. 14, 17, 23\\n[223] J. Seipp, ´A. Torralba, and J. Hoffmann. Pddl generators, 2022. 15, 24\\n[224] Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, and Niko Suenderhauf. Sayplan: Grounding large language\\nmodels using 3d scene graphs for scalable task planning. In 7th Annual Conference on Robot Learning , 2023. 15\\n41', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d72f03db-e7c7-466a-8a14-0c2a7623f16a', embedding=None, metadata={'page_label': '42', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='2fe229ebc683f5e165709abc769e41576900505bb5dc561ef743cc57acc72c42', text='[225] Quanting Xie, Tianyi Zhang, Kedi Xu, Matthew Johnson-Roberson, and Yonatan Bisk. Reasoning about the unseen for efficient\\noutdoor object navigation, 2023. 15, 17, 22, 28\\n[226] Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Hao-Tien Lewis Chiang, Tom\\nErez, Leonard Hasenclever, Jan Humplik, Brian Ichter, Ted Xiao, Peng Xu, Andy Zeng, Tingnan Zhang, Nicolas Heess, Dorsa Sadigh,\\nJie Tan, Yuval Tassa, and Fei Xia. Language to rewards for robotic skill synthesis. Arxiv preprint arXiv:2306.08647 , 2023. 15, 20,\\n25, 26\\n[227] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer: Composable 3d value maps for\\nrobotic manipulation with language models. arXiv preprint arXiv:2307.05973 , 2023. 15, 16, 22, 23\\n[228] Junting Chen, Guohao Li, Suryansh Kumar, Bernard Ghanem, and Fisher Yu. How to not train your dragon: Training-free embodied\\nobject goal navigation with semantic frontiers. arXiv preprint arXiv:2305.16925 , 2023. 15\\n[229] Yen-Jen Wang, Bike Zhang, Jianyu Chen, and Koushil Sreenath. Prompt a robot to walk with large language models, 2023. 15, 25\\n[230] Yuqing Du, Olivia Watkins, Zihan Wang, C ´edric Colas, Trevor Darrell, Pieter Abbeel, Abhishek Gupta, and Jacob Andreas. Guiding\\npretraining in reinforcement learning with large language models, 2023. 15\\n[231] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. Reward design with language models, 2023.\\n[232] Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, and Tao Yu. Text2reward: Automated\\ndense reward function generation for reinforcement learning, 2023. 15, 26\\n[233] Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete Florence, Igor Mordatch, Sergey Levine, Karol\\nHausman, and Brian Ichter. Grounded decoding: Guiding text generation with grounded models for robot control, 2023. 16\\n[234] Thomas Carta, Cl ´ement Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer. Grounding large language\\nmodels in interactive environments with online reinforcement learning, 2023. 16, 17\\n[235] Yen-Jen Wang, Bike Zhang, Jianyu Chen, and Koushil Sreenath. Prompt a robot to walk with large language models, 2023. 16\\n[236] Huy Ha, Pete Florence, and Shuran Song. Scaling up and distilling down: Language-guided robot skill acquisition. In Proceedings\\nof the 2023 Conference on Robot Learning , 2023. 17, 20, 23\\n[237] Yufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang, Yian Wang, Katerina Fragkiadaki, Zackory Erickson, David Held, and\\nChuang Gan. Robogen: Towards unleashing infinite data for automated robot learning via generative simulation, 2023. 17, 20\\n[238] Tianhe Yu, Ted Xiao, Austin Stone, Jonathan Tompson, Anthony Brohan, Su Wang, Jaspiar Singh, Clayton Tan, Jodilyn Peralta Dee M,\\nBrian Ichter, Karol Hausman, and Fei Xia. Scaling robot learning with semantically imagined experience. In arXiv:2302.11550 ,\\n2023. 17, 20\\n[239] Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, Montserrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu, Chuyuan Fu, Keerthana\\nGopalakrishnan, Zhuo Xu, Priya Sundaresan, Peng Xu, Hao Su, Karol Hausman, Chelsea Finn, Quan Vuong, and Ted Xiao.\\nRt-trajectory: Robotic task generalization via hindsight trajectory sketches, 2023. 17, 20\\n[240] Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Walke, Chelsea Finn, Aviral Kumar, and Sergey Levine. Zero-shot robotic\\nmanipulation with pretrained image-editing diffusion models, 2023. 17\\n[241] Yilun Du, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Joshua B. Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning\\nuniversal policies via text-guided video generation. In NeurIPS , 2023. 17\\n[242] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-\\nthought prompting elicits reasoning in large language models, 2023. 17\\n[243] Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le\\nBras, Jena D. Hwang, Soumya Sanyal, Sean Welleck, Xiang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi. Faith and fate:\\nLimits of transformers on compositionality, 2023. 17\\n[244] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Pod-\\nstawski, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph of thoughts: Solving elaborate problems with large language\\nmodels, 2023. 17\\n[245] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts:\\nDeliberate problem solving with large language models, 2023. 17\\n42', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='c653f581-e7c0-4688-87f7-983e26d6e4d0', embedding=None, metadata={'page_label': '43', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='b4d087be22ad9e57bd361e92898f341672a3b231b0125cefc7b37cba0dbd30f5', text='[246] Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, and Chuang Gan. Planning with large language\\nmodels for code generation. In The Eleventh International Conference on Learning Representations , 2023. 17\\n[247] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. Llm+p: Empowering large language\\nmodels with optimal planning proficiency, 2023. 17, 24\\n[248] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch,\\nYevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner\\nmonologue: Embodied reasoning through planning with language models, 2022. 17, 24\\n[249] Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart, Brianna Zitkovich, Fei\\nXia, Chelsea Finn, et al. Open-world object manipulation using pre-trained vision-language models. arXiv preprint arXiv:2303.00905 ,\\n2023. 18, 19, 23\\n[250] Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Aky¨ urek, Anima\\nAnandkumar, et al. Pre-trained language models for interactive decision-making. Advances in Neural Information Processing\\nSystems , 35:31199–31212, 2022. 18\\n[251] Siddharth Karamcheti, Megha Srivastava, Percy Liang, and Dorsa Sadigh. Lila: Language-informed latent actions. In Conference on\\nRobot Learning , pages 1379–1390. PMLR, 2022. 18, 20\\n[252] Hengyuan Hu and Dorsa Sadigh. Language instructed reinforcement learning for human-ai coordination. arXiv preprint\\narXiv:2304.07297 , 2023. 18\\n[253] Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, and Pete Florence.\\nInteractive language: Talking to robots in real time. arXiv preprint arXiv:2210.06407 , 2022. 18, 21, 23, 24\\n[254] Kuang-Huei Lee, Ted Xiao, Adrian Li, Paul Wohlhart, Ian Fischer, and Yao Lu. Pi-qt-opt: Predictive information improves multi-task\\nrobotic reinforcement learning at scale. CoRL , 2022. 18\\n[255] Alexander Herzog, Kanishka Rao, Karol Hausman, Yao Lu, Paul Wohlhart, Mengyuan Yan, Jessica Lin, Montserrat Gonzalez\\nArenas, Ted Xiao, Daniel Kappler, Daniel Ho, Jarek Rettinghouse, Yevgen Chebotar, Kuang-Huei Lee, Keerthana Gopalakrishnan,\\nRyan Julian, Adrian Li, Chuyuan Kelly Fu, Bob Wei, Sangeetha Ramesh, Khem Holden, Kim Kleiven, David Rendleman, Sean\\nKirmani, Jeff Bingham, Jon Weisz, Ying Xu, Wenlong Lu, Matthew Bennice, Cody Fong, David Do, Jessica Lam, Yunfei Bai, Benjie\\nHolson, Michael Quinlan, Noah Brown, Mrinal Kalakrishnan, Julian Ibarz, Peter Pastor, and Sergey Levine. Deep rl at scale: Sorting\\nwaste in office buildings with a fleet of mobile manipulators. In RSS, 2023. 18\\n[256] Yevgen Chebotar, Quan Vuong, Alex Irpan, Karol Hausman, Fei Xia, Yao Lu, Aviral Kumar, Tianhe Yu, Alexander Herzog, Karl\\nPertsch, Keerthana Gopalakrishnan, Julian Ibarz, Ofir Nachum, Sumedh Sontakke, Grecia Salazar, Huong T Tran, Jodilyn Peralta,\\nClayton Tan, Deeksha Manjunath, Jaspiar Singht, Brianna Zitkovich, Tomas Jackson, Kanishka Rao, Chelsea Finn, and Sergey Levine.\\nQ-transformer: Scalable offline reinforcement learning via autoregressive q-functions. In CoRL , 2023. 19, 20\\n[257] Aviral Kumar, Anikait Singh, Frederik Ebert, Mitsuhiko Nakamoto, Yanlai Yang, Chelsea Finn, and Sergey Levine. Pre-training for\\nrobots: Offline rl enables learning new tasks from a handful of trials. In RSS, 2023. 19\\n[258] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. In\\nNeurIPS , 2020. 19\\n[259] Simone Parisi, Aravind Rajeswaran, Senthil Purushwalkam, and Abhinav Gupta. The unsurprising effectiveness of pre-trained vision\\nmodels for control, 2022. 19\\n[260] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: A universal visual representation for robot\\nmanipulation, 2022. 24\\n[261] Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell. Real-world robot learning with\\nmasked visual pre-training. In Conference on Robot Learning , pages 416–426. PMLR, 2023. 19\\n[262] Ilija Radosavovic, Baifeng Shi, Letian Fu, Ken Goldberg, Trevor Darrell, and Jitendra Malik. Robot learning with sensorimotor\\npre-training, 2023. 19\\n[263] Nicklas Hansen, Zhecheng Yuan, Yanjie Ze, Tongzhou Mu, Aravind Rajeswaran, Hao Su, Huazhe Xu, and Xiaolong Wang. On\\npre-training for visuo-motor control: Revisiting a learning-from-scratch baseline. In ICML , 2023. 19\\n43', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='41db17b2-95ea-42a1-874f-6898f8bad552', embedding=None, metadata={'page_label': '44', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='ac6e84660b08160b401307e67add25df14cd39a4377c74351e7351263b2d9a98', text='[264] Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, Yecheng Jason Ma, Claire Chen, Sneha Silwal, Aryan Jain, Vincent-Pierre Berges,\\nPieter Abbeel, Jitendra Malik, Dhruv Batra, Yixin Lin, Oleksandr Maksymets, Aravind Rajeswaran, and Franziska Meier. Where are\\nwe in the search for an artificial visual cortex for embodied intelligence?, 2023. 19, 25\\n[265] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman,\\nXiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu,\\nMandar Joshi, Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic,\\nAustin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner, Yang Li, Daniel\\nKeysers, Anurag Arnab, Yuanzhong Xu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua\\nZhai, Neil Houlsby, and Radu Soricut. Pali-x: On scaling up a multilingual vision and language model, 2023. 19\\n[266] Shikhar Bahl, Russell Mendonca, Lili Chen, Unnat Jain, and Deepak Pathak. Affordances from human videos as a versatile\\nrepresentation for robotics. CVPR , 2023. 19\\n[267] Dhruv Shah, Ajay Sridhar, Arjun Bhorkar, Noriaki Hirose, and Sergey Levine. Gnm: A general navigation model to drive any robot.\\nInICRA , 2023. 19, 21, 25\\n[268] Joanne Truong, April Zitkovich, Sonia Chernova, Dhruv Batra, Tingnan Zhang, Jie Tan, and Wenhao Yu. Indoorsim-to-outdoorreal:\\nLearning to navigate outdoors without any outdoor experience. In arXiv:2305.01098 , 2023. 19\\n[269] Rogerio Bonatti, Sai Vemprala, Shuang Ma, Felipe Frujeri, Shuhang Chen, and Ashish Kapoor. Pact: Perception-action causal\\ntransformer for autoregressive robotics pre-training. In arXiv:2209.11133 , 2022. 19\\n[270] Qiao Gu, Alihusein Kuwajerwala, Sacha Morin, Krishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera,\\nWilliam Paul, Kirsty Ellis, Rama Chellappa, et al. Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning.\\narXiv preprint arXiv:2309.16650 , 2023. 20\\n[271] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model\\nis planning with world model. arXiv preprint arXiv:2305.14992 , 2023. 20, 28\\n[272] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke\\nZhu, and Linxi Fan. VIMA: general robot manipulation with multimodal prompts. ArXiv , abs/2210.03094, 2022. 20, 23\\n[273] Shikhar Bahl, Abhinav Gupta, and Deepak Pathak. Human-to-robot imitation in the wild. In RSS, 2022. 20\\n[274] Vidhi Jain, Yixin Lin, Eric Undersander, Yonatan Bisk, and Akshara Rai. Transformers are adaptable task planners. In 6th Annual\\nConference on Robot Learning , 2022. 20\\n[275] Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy sketches. In International\\nconference on machine learning , pages 166–175. PMLR, 2017. 20\\n[276] Marjorie Skubic, Derek Anderson, Samuel Blisard, Dennis Perzanowski, and Alan Schultz. Using a hand-drawn sketch to control a\\nteam of robots. Autonomous Robots , 22:399–410, 2007. 20\\n[277] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought\\nprompting elicits reasoning in large language models. Advances in Neural Information Processing Systems , 35:24824–24837, 2022.\\n20\\n[278] Jianzong Wu, Xiangtai Li, Shilin Xu Haobo Yuan, Henghui Ding, Yibo Yang, Xia Li, Jiangning Zhang, Yunhai Tong, Xudong Jiang,\\nBernard Ghanem, et al. Towards open vocabulary learning: A survey. arXiv preprint arXiv:2306.15880 , 2023. 20\\n[279] Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. Holistic analysis of hallucination\\nin gpt-4v (ision): Bias and interference challenges. arXiv preprint arXiv:2311.03287 , 2023. 20\\n[280] Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, and Dorsa Sadigh. Physically grounded\\nvision-language models for robotic manipulation. In arxiv , 2023. 21, 23, 26\\n[281] Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, Sergey Levine,\\nand Chelsea Finn. Robonet: Large-scale multi-robot learning, 2020. 21\\n[282] Frederik Ebert, Yanlai Yang, Karl Schmeckpeper, Bernadette Bucher, Georgios Georgakis, Kostas Daniilidis, Chelsea Finn, and\\nSergey Levine. Bridge data: Boosting generalization of robotic skills with cross-domain datasets, 2021. 21\\n[283] Homer Walke, Kevin Black, Abraham Lee, Moo Jin Kim, Max Du, Chongyi Zheng, Tony Zhao, Philippe Hansen-Estruch, Quan\\nVuong, Andre He, et al. Bridgedata v2: A dataset for robot learning at scale. arXiv preprint arXiv:2308.12952 , 2023. 21, 23\\n44', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='d5651157-5b5a-45a8-afc5-a8612ce855d4', embedding=None, metadata={'page_label': '45', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='940c41c10742d90df1a6953454e7bfdaf62b04d62212a649a86249d772376232', text='[284] Kenneth Shaw, Ananye Agarwal, and Deepak Pathak. Leap hand: Low-cost, efficient, and anthropomorphic hand for robot learning.\\narXiv preprint arXiv:2309.06440 , 2023. 21, 28\\n[285] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and\\nYinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158 , 2017. 21\\n[286] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world perception for\\nembodied agents. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 9068–9079, 2018. 21\\n[287] Manolis Savva*, Abhishek Kadian*, Oleksandr Maksymets*, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen\\nKoltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: A Platform for Embodied AI Research. In Proceedings of the\\nIEEE/CVF International Conference on Computer Vision (ICCV) , 2019. 21\\n[288] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon,\\nYuke Zhu, et al. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474 , 2017. 22\\n[289] So Yeon Min, Devendra Singh Chaplot, Pradeep Ravikumar, Yonatan Bisk, and Ruslan Salakhutdinov. Film: Following instructions\\nin language with modular methods, 2022. 22\\n[290] Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor. Airsim: High-fidelity visual and physical simulation for autonomous\\nvehicles, 2017. 22\\n[291] Google DeepMind. Mujoco 3.0. https://github.com/google-deepmind/mujoco/releases/tag/3.0.0 , 2023.\\nAccessed: [Insert date of access]. 22\\n[292] S. Dass, J. Yapeter, J. Zhang, J. Zhang, K. Pertsch, S. Nikolaidis, and J. J. Lim. Clvr jaco play dataset. https://github.com/\\nclvrai/clvr-jaco-play-dataset , 2023. 23\\n[293] Jianlan Luo, Charles Xu, Xinyang Geng, Gilbert Feng, Kuan Fang, Liam Tan, Stefan Schaal, and Sergey Levine. Multi-stage cable\\nrouting through hierarchical imitation learning, 2023. 23\\n[294] Jyothish Pari, Nur Muhammad Shafiullah, Sridhar Pandian Arunachalam, and Lerrel Pinto. The surprising effectiveness of represen-\\ntation learning for visual imitation. arXiv preprint arXiv:2112.01511 , 2021. 23\\n[295] L. Y. Chen, S. Adebola, and K. Goldberg. Berkeley ur5 demonstration dataset. https://sites.google.com/view/\\nberkeley-ur5/home . Accessed: [Insert Date Here]. 23\\n[296] Erick Rosete-Beas, Oier Mees, Gabriel Kalweit, Joschka Boedecker, and Wolfram Burgard. Latent plans for task agnostic offline\\nreinforcement learning. In Proceedings of the 6th Conference on Robot Learning (CoRL) , 2022. 23\\n[297] Xufeng Zhao, Mengdi Li, Cornelius Weber, Muhammad Burhan Hafez, and Stefan Wermter. Chat with the environment: Interactive\\nmultimodal perception using large language models, 2023. 23\\n[298] Coppelia Robotics. Coppeliasim. https://www.coppeliarobotics.com/ . Accessed: [Insert Date Here]. 23\\n[299] E. Coumans and Y. Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. 23\\n[300] Siyuan Huang, Zhengkai Jiang, Hao Dong, Yu Qiao, Peng Gao, and Hongsheng Li. Instruct2act: Mapping multi-modality instructions\\nto robotic actions with large language model, 2023. 23\\n[301] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang,\\net al. Sapien: A simulated part-based interactive environment. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition , pages 11097–11107, 2020. 23\\n[302] Kaichun Mo, Leonidas Guibas, Mustafa Mukadam, Abhinav Gupta, and Shubham Tulsiani. Where2act: From pixels to actions for\\narticulated 3d objects, 2021. 23\\n[303] Alex X. Lee, Coline Devin, Yuxiang Zhou, Thomas Lampe, Konstantinos Bousmalis, Jost Tobias Springenberg, Arunkumar Byravan,\\nAbbas Abdolmaleki, Nimrod Gileadi, David Khosid, Claudio Fantacci, Jose Enrique Chen, Akhil Raju, Rae Jeong, Michael Neunert,\\nAntoine Laurens, Stefano Saliceti, Federico Casarini, Martin Riedmiller, Raia Hadsell, and Francesco Nori. Beyond pick-and-place:\\nTackling robotic stacking of diverse shapes, 2021. 24\\n[304] Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Walter Talbott, Katherine Metcalf, Natalie Mackraz, Devon Hjelm,\\nand Alexander Toshev. Large language models as generalizable policies for embodied tasks, 2023. 24\\n45', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='8fdf7e2d-ed0c-4ccc-9a9a-3793fa0737a2', embedding=None, metadata={'page_label': '46', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='8de44b1867e9be5332857d08dd2f83a4289bb9972a897019f204cd116f3e51c6', text='[305] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda,\\nNicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 , 2021.\\n24\\n[306] Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan\\nDuong, Vikas Sindhwani, and Johnny Lee. Transporter networks: Rearranging the visual world for robotic manipulation. Conference\\non Robot Learning (CoRL) , 2020. 24, 25\\n[307] Yecheng Jason Ma, William Liang, Vaidehi Som, Vikash Kumar, Amy Zhang, Osbert Bastani, and Dinesh Jayaraman. Liv: Language-\\nimage representations and rewards for robotic control, 2023. 24\\n[308] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Avnish Narayan, Hayden Shively, Adithya Bellathur, Karol Hausman, Chelsea\\nFinn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning, 2021. 24, 25\\n[309] Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and\\nThomas Funkhouser. Tidybot: Personalized robot assistance with large language models. arXiv preprint arXiv:2305.05658 , 2023. 24\\n[310] Yan Ding, Xiaohan Zhang, Chris Paxton, and Shiqi Zhang. Task and motion planning with large language models for object\\nrearrangement. arXiv preprint arXiv:2303.06247 , 2023. 24\\n[311] N. Koenig and A. Howard. Design and use paradigms for gazebo, an open-source multi-robot simulator. In 2004 IEEE/RSJ\\nInternational Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566) , volume 3, pages 2149–2154 vol.3,\\n2004. 24\\n[312] Karmesh Yadav, Ram Ramrakhya, Santhosh Kumar Ramakrishnan, Theo Gervet, John Turner, Aaron Gokaslan, Noah Maestre,\\nAngel Xuan Chang, Dhruv Batra, Manolis Savva, Alexander William Clegg, and Devendra Singh Chaplot. Habitat-matterport 3d\\nsemantics dataset, 2023. 24, 25\\n[313] Th ´eophane Weber, S ´ebastien Racaniere, David P Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adria Puigdomenech\\nBadia, Oriol Vinyals, Nicolas Heess, Yujia Li, et al. Imagination-augmented agents for deep reinforcement learning. arXiv preprint\\narXiv:1707.06203 , 2017. 25\\n[314] Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua\\nBengio. Babyai: A platform to study the sample efficiency of grounded language learning. arXiv preprint arXiv:1810.08272 , 2018.\\n25\\n[315] Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation to benchmark reinforcement learning.\\nInInternational conference on machine learning , pages 2048–2056. PMLR, 2020. 25\\n[316] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for\\ngeneral agents. Journal of Artificial Intelligence Research , 47:253–279, 2013. 25\\n[317] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh\\nMerel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690 , 2018. 25\\n[318] Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete Florence, Igor Mordatch, Sergey Levine, Karol Haus-\\nman, and Brian Ichter. Grounded decoding: Guiding text generation with grounded models for robot control. ArXiv , abs/2303.00855,\\n2023. 25\\n[319] M. Chevalier-Boisvert, L. Willems, and S. Pal. Minimalistic gridworld environment for gymnasium. https://github.com/\\npierg/environments-rl , 2018. 25\\n[320] Yuanpei Chen, Tianhao Wu, Shengjie Wang, Xidong Feng, Jiechuan Jiang, Zongqing Lu, Stephen McAleer, Hao Dong, Song-Chun\\nZhu, and Yaodong Yang. Towards human-level bimanual dexterous manipulation with reinforcement learning. Advances in Neural\\nInformation Processing Systems , 35:5150–5163, 2022. 25\\n[321] Taylor Howell, Nimrod Gileadi, Saran Tunyasuvunakool, Kevin Zakka, Tom Erez, and Yuval Tassa. Predictive sampling: Real-time\\nbehaviour synthesis with mujoco, 2022. 25\\n[322] Matthew Chignoli, Donghyun Kim, Elijah Stanger-Jones, and Sangbae Kim. The mit humanoid robot: Design, motion planning, and\\ncontrol for acrobatic behaviors. In 2020 IEEE-RAS 20th International Conference on Humanoid Robots (Humanoids) , pages 1–8.\\nIEEE, 2021. 22\\n[323] Weiye Zhao, Tairan He, and Changliu Liu. Model-free safe control for zero-violation reinforcement learning. In 5th Annual Conference\\non Robot Learning , 2021. 26\\n46', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='05101af0-da66-4460-9b79-50beb0ad3e82', embedding=None, metadata={'page_label': '47', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='e27e6850d7668c104ca5fc35bf8a66c53909010b6cf7c19b492f35d44a73da4d', text='[324] Sylvia Herbert, Jason J. Choi, Suvansh Sanjeev, Marsalis Gibson, Koushil Sreenath, and Claire J. Tomlin. Scalable learning of safety\\nguarantees for autonomous systems using hamilton-jacobi reachability, 2021. 26, 27\\n[325] Ran Tian, Liting Sun, Andrea Bajcsy, Masayoshi Tomizuka, and Anca D Dragan. Safety assurances for human-robot interaction\\nvia confidence-aware game-theoretic human models. In 2022 International Conference on Robotics and Automation (ICRA) , pages\\n11229–11235. IEEE, 2022. 27\\n[326] S.H. Cheong, J.H. Lee, and C.H. Kim. A new concept of safety affordance map for robots object manipulation. In 2018 27th IEEE\\nInternational Symposium on Robot and Human Interactive Communication (RO-MAN) , pages 565–570, 2018. 27\\n[327] Ziyi Yang, Shreyas Sundara Raman, Ankit Shah, and Stefanie Tellex. Plug in the safety chip: Enforcing constraints for LLM-driven\\nrobot agents. In 2nd Workshop on Language and Robot Learning: Language as Grounding , 2023. 27\\n[328] Olaf Sporns and Richard F Betzel. Modular brain networks. Annual review of psychology , 67:613–640, 2016. 27\\n[329] David Meunier, Renaud Lambiotte, and Edward T Bullmore. Modular and hierarchically modular organization of brain networks.\\nFrontiers in neuroscience , 4:200, 2010. 27\\n[330] Montserrat Gonzalez Arenas, Ted Xiao, Sumeet Singh, Vidhi Jain, Allen Z. Ren, Quan Vuong, Jake Varley, Alexander Herzog, Isabel\\nLeal, Sean Kirmani, Dorsa Sadigh, Vikas Sindhwani, Kanishka Rao, Jacky Liang, and Andy Zeng. How to prompt your robot:\\nA promptbook for manipulation skills with code as policies. In 2nd Workshop on Language and Robot Learning: Language as\\nGrounding , 2023. 28\\n[331] Zengyi Qin, Kuan Fang, Yuke Zhu, Li Fei-Fei, and Silvio Savarese. Keto: Learning keypoint representations for tool manipulation,\\n2019. 28\\n[332] Dylan Turpin, Liquang Wang, Stavros Tsogkas, Sven J. Dickinson, and Animesh Garg. Gift: Generalizable interaction-aware\\nfunctional tool affordances without labels. ArXiv , abs/2106.14973, 2021.\\n[333] Carl Qi, Sarthak Shetty, Xingyu Lin, and David Held. Learning generalizable tool-use skills through trajectory generation. ArXiv ,\\nabs/2310.00156, 2023. 28\\n[334] Shadow Robot Company. Dexterous hand series. https://www.shadowrobot.com/dexterous-hand-series/ , 2023.\\nAccessed: 2023-12-10. 28\\n[335] Siyuan Dong, Wenzhen Yuan, and Edward H. Adelson. Improved gelsight tactile sensor for measuring geometry and slip. In 2017\\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) . IEEE, September 2017. 28\\n[336] Zilin Si, Tianhong Catherine Yu, Katrene Morozov, James McCann, and Wenzhen Yuan. Robotsweater: Scalable, generalizable, and\\ncustomizable machine-knitted tactile skins for robots. arXiv preprint arXiv:2303.02858 , 2023. 28\\n[337] Tony Z. Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware,\\n2023. 28\\n[338] Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah A. Smith, Yejin\\nChoi, and Kentaro Inui. Realtime qa: What’s the answer right now?, 2022. 29\\n[339] Sanket Vaibhav Mehta, Jai Gupta, Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Jinfeng Rao, Marc Najork, Emma Strubell, and Donald\\nMetzler. Dsi++: Updating transformer memory with new documents. ArXiv , abs/2212.09744, 2022.\\n[340] Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, and Emma Strubell. An empirical investigation of the role of pre-training in\\nlifelong learning. Journal of Machine Learning Research , 24(214):1–50, 2023.\\n[341] James Seale Smith, Paola Cascante-Bonilla, Assaf Arbelle, Donghyun Kim, Rameswar Panda, David Cox, Diyi Yang, Zsolt Kira,\\nRogerio Feris, and Leonid Karlinsky. Construct-vl: Data-free continual structured vl concepts learning. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 14994–15004, 2023. 29\\n[342] Timoth ´ee Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David Filliat, and Natalia D ´ıaz-Rodr ´ıguez. Continual learning\\nfor robotics: Definition, framework, learning strategies, opportunities and challenges. Information Fusion , 58:52–68, June 2020. 29\\n[343] Guilherme Maeda, Marco Ewerton, Takayuki Osa, Baptiste Busch, and Jan Peters. Active incremental learning of robot movement\\nprimitives. In Sergey Levine, Vincent Vanhoucke, and Ken Goldberg, editors, Proceedings of the 1st Annual Conference on Robot\\nLearning , volume 78 of Proceedings of Machine Learning Research , pages 37–46. PMLR, 13–15 Nov 2017. 29\\n[344] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik. Rma: Rapid motor adaptation for legged robots. In Robotics: Science\\nand Systems , 2021. 29\\n47', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='1a0e45b7-186c-421b-aa8b-5b46a445a22f', embedding=None, metadata={'page_label': '48', 'file_name': '.papers/2312.08782v2.Toward_General_Purpose_Robots_via_Foundation_Models__A_Survey_and_Meta_Analysis.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='235913816107f41235fc4169c28ae114b86011c0625eb821ca4726628047def7', text='[345] Joey Hejna and Dorsa Sadigh. Inverse preference learning: Preference-based rl without a reward function, 2023. 29\\n[346] Zihao Li, Zhuoran Yang, and Mengdi Wang. Reinforcement learning with human feedback: Learning dynamic choices via pessimism,\\n2023. 29\\n[347] Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features\\nand underperform out-of-distribution. In International Conference on Learning Representations , 2022. 29\\n[348] OpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias\\nPlappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan,\\nWojciech Zaremba, and Lei Zhang. Solving rubik’s cube with a robot hand, 2019. 29\\n[349] Viraj Mehta, Vikramjeet Das, Ojash Neopane, Yijia Dai, Ilija Bogunovic, Jeff Schneider, and Willie Neiswanger. Sample efficient\\nreinforcement learning from human feedback via active exploration, 2023. 29\\n[350] Gaon An, Junhyeok Lee, Xingdong Zuo, Norio Kosaka, Kyung-Min Kim, and Hyun Oh Song. Direct preference-based policy\\noptimization without reward modeling, 2023. 29\\n[351] Anurag Ajay, Seungwook Han, Yilun Du, Shuang Li, Abhi Gupta, Tommi Jaakkola, Josh Tenenbaum, Leslie Kaelbling, Akash\\nSrivastava, and Pulkit Agrawal. Compositional foundation models for hierarchical planning, 2023. 29\\n[352] Kevin Frans, Jonathan Ho, Xi Chen, Pieter Abbeel, and John Schulman. Meta learning shared hierarchies, 2017.\\n[353] Suraj Nair and Chelsea Finn. Hierarchical foresight: Self-supervised learning of long-horizon tasks via visual subgoal generation. In\\nInternational Conference on Learning Representations , 2020. 29\\n[354] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition, 2019. 29\\n48', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n"
     ]
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.readers import PDFReader\n",
    "\n",
    "# from llama_hub.file.pymu_pdf.base import PyMuPDFReader\n",
    "\n",
    "loader = PDFReader()\n",
    "\n",
    "documents = []\n",
    "for i in files:\n",
    "    documents.append(loader.load_data(f\".papers/{i}\"))\n",
    "\n",
    "documents = [c for d in documents for c in d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.readers import PDFReader\n",
    "import logging\n",
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")\n",
    "\n",
    "from llama_index.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    "    KeywordExtractor,\n",
    "    EntityExtractor,\n",
    "    BaseExtractor,\n",
    ")\n",
    "\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.ingestion import IngestionPipeline\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "\n",
    "DOCUMENT_PATH = \".papers\"\n",
    "QUANT_VERSION = \"mistral-7b-instruct-v0.2.Q3_K_S.gguf\"\n",
    "MODEL_PATH = f\"./models/{QUANT_VERSION}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 documents\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(DOCUMENT_PATH)\n",
    "loader = PDFReader()\n",
    "\n",
    "documents = []\n",
    "for i in files:\n",
    "    documents.append(loader.load_data(f\".papers/{i}\"))\n",
    "\n",
    "documents = [c for d in documents for c in d][:5]\n",
    "# documents = random.sample(documents, 5)\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='f4582705-ae42-4e1e-9641-ccda6c2408b0', embedding=None, metadata={'page_label': '1', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='2f6c3dd94d8e6a1f97f07f2ce1df2512ca573ac0385a0ea64be4bf13b33a33e8', text='Are Emergent Abilities of Large Language Models a\\nMirage?\\nRylan Schaeffer, Brando Miranda, and Sanmi Koyejo\\nComputer Science, Stanford University\\nAbstract\\nRecent work claims that large language models display emergent abilities , abil-\\nities not present in smaller-scale models that are present in larger-scale models.\\nWhat makes emergent abilities intriguing is two-fold: their sharpness , transition-\\ning seemingly instantaneously from not present to present, and their unpredictabil-\\nity, appearing at seemingly unforeseeable model scales. Here, we present an al-\\nternative explanation for emergent abilities: that for a particular task and model\\nfamily, when analyzing ﬁxed model outputs, emergent abilities appear due the\\nresearcher’s choice of metric rather than due to fundamental changes in model\\nbehavior with scale. Speciﬁcally, nonlinear or discontinuous metrics produce ap-\\nparent emergent abilities, whereas linear or continuous metrics produce smooth,\\ncontinuous, predictable changes in model performance. We present our alternative\\nexplanation in a simple mathematical model, then test it in three complementary\\nways: we (1) make, test and conﬁrm three predictions on the effect of metric\\nchoice using the InstructGPT/GPT-3 family on tasks with claimed emergent abil-\\nities, (2) make, test and conﬁrm two predictions about metric choices in a meta-\\nanalysis of emergent abilities on BIG-Bench; and (3) show how to choose metrics\\nto produce never-before-seen seemingly emergent abilities in multiple vision tasks\\nacross diverse deep networks. Via all three analyses, we provide evidence that al-\\nleged emergent abilities evaporate with different metrics or with better statistics,\\nand may not be a fundamental property of scaling AI models.\\n1 Introduction\\nEmergent properties of complex systems have long been studied across disciplines, from physics to\\nbiology to mathematics. The idea of emergence was popularized by Nobel Prize-winning physicist\\nP.W. Anderson’s “More Is Different” [1], which argues that as the complexity of a system increases,\\nnew properties may materialize that cannot be predicted even from a precise quantitative understand-\\ning of the system’s microscopic details. Recently, the idea of emergence gained signiﬁcant attention\\nin machine learning due to observations that large language models (LLMs) such as GPT [3], PaLM\\n[6] and LaMDA [30] exhibit so-called “emergent abilities” [33, 8, 28, 3] (Fig. 1).\\nThe term “emergent abilities of LLMs” was recently and crisply deﬁned as “abilities that are not\\npresent in smaller-scale models but are present in large-scale models; thus they cannot be predicted\\nby simply extrapolating the performance improvements on smaller-scale models” [33]. Such emer-\\ngent abilities were ﬁrst discovered in the GPT-3 family [3]. Subsequent work emphasized the discov-\\nery, writing that “[although model] performance is predictable at a general level, performance on a\\nspeciﬁc task can sometimes emerge quite unpredictably and abruptly at scale” [8]. These quotations\\ncollectively identify the two deﬁning properties of emergent abilities in LLMs:\\n1.Sharpness , transitioning seemingly instantaneously from not present to present\\nPreprint. Under review.arXiv:2304.15004v2  [cs.AI]  22 May 2023', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='1affc42a-596d-4609-a7b2-681677d59b8c', embedding=None, metadata={'page_label': '2', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='d74b72a185048718e5a7cf97d0bda92a2a76375c8145ef77665959b2e4f91db7', text='Figure 1: Emergent abilities of large language models . Model families display sharp andunpre-\\ndictable increases in performance at speciﬁc tasks as scale increases. Source: Fig. 2 from [33].\\n2.Unpredictability , transitioning at seemingly unforeseeable model scales\\nThese emergent abilities have garnered signiﬁcant interest, raising questions such as: What controls\\nwhich abilities will emerge? What controls when abilities will emerge? How can we make desirable\\nabilities emerge faster, and ensure undesirable abilities never emerge? These questions are especially\\npertinent to AI safety and alignment, as emergent abilities forewarn that larger models might one\\nday, without warning, acquire undesired mastery over dangerous capabilities [29, 10, 17, 18].\\nIn this paper, we call into question the claim that LLMs possess emergent abilities, by which we\\nspeciﬁcally mean sharp andunpredictable changes in model outputs as a function of model scale\\non speciﬁc tasks. Our doubt stems from the observation that emergent abilities seem to appear\\nonly under metrics that nonlinearly or discontinuously scale any model’s per-token error rate. For\\ninstance, as we later show, >92% of emergent abilities on BIG-Bench tasks [28] (hand-annotated\\nby [32]) appear under either of these two metrics:\\nMultiple Choice Gradedef={1if highest probability mass on correct option\\n0otherwise\\nExact String Matchdef={1if output string exactly matches target string\\n0otherwise\\nThis raises the possibility of an alternative explanation for the origin of LLMs’ emergent abilities:\\nsharp and unpredictable changes might be induced by the researcher’s choice of measurement, even\\nthough the model family’s per-token error rate changes smoothly, continuously and predictably with\\nincreasing scale. Speciﬁcally, our alternative posits that emergent abilities are a mirage caused\\nprimarily by the researcher choosing a metric that nonlinearly or discontinuously deforms per-token\\nerror rates, and secondarily by possessing too few test data to accurately estimate the performance\\nof smaller models, thereby causing smaller models to appear wholly unable to perform the task.\\nTo communicate our alternative explanation, we present it as a simple mathematical model and\\ndemonstrate how it quantitatively reproduces the evidence offered in support of emergent abilities\\nof LLMs. We then test our alternative explanation in three complementary ways:\\n1. We make, test and conﬁrm three predictions based on our alternative hypotheses using the\\nInstructGPT [24] / GPT-3 [3] model family.\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e27d66f0-15b1-4240-ae03-5874dba8095a', embedding=None, metadata={'page_label': '3', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='64aefe95c6f85237c360fd9cac7f03b7e48850420fa4bff16d3cf661a7b95ad5', text='109\\n1010\\n1011\\nModel Parameters\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0Multiple Choice Grade\\nNum. Choices\\n2D\\n109\\n1010\\n1011\\nModel Parameters\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7Multiple Choice Grade\\nPublished Emergent Ability\\n109\\n1010\\n1011\\nModel Parameters\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0Accuracy\\nT arget Str Len\\n5\\n109\\n1010\\n1011\\nModel Parameters\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0Accuracy\\nPublished Emergent Ability C\\n109\\n1010\\n1011\\nModel Parameters\\n101\\n100CE(Per-Token)\\nA\\n109\\n1010\\n1011\\nModel Parameters\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0Prob Correct (Per-T oken)\\nB\\nEmergent Abilities No Emergent Abilities\\nDiscontinuously\\nscore\\nLLM outputs\\n109\\n1010\\n1011\\nModel Parameters\\n4\\n3\\n2\\n1\\n0- Number of Incorrect T okens\\nT arget Str Len\\n5E\\nNonlinearly\\nscore\\nLLM outputs\\nContinuously\\nscore\\nLLM outputsLinearly\\nscore\\nLLM outputs\\n109\\n1010\\n1011\\nModel Parameters\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0- Brier Score\\nNum. Choices\\n2FFigure 2: Emergent abilities of large language models are created by the researcher’s chosen\\nmetrics, not unpredictable changes in model behavior with scale. (A) Suppose the per-token\\ncross-entropy loss decreases monotonically with model scale, e.g., LCEscales as a power law. (B)\\nThe per-token probability of selecting the correct token asymptotes towards 1. (C) If the researcher\\nscores models’ outputs using a nonlinear metric such as Accuracy (which requires a sequence of\\ntokens to allbe correct), the metric choice nonlinearly scales performance, causing performance\\nto change sharply and unpredictably in a manner that qualitatively matches published emergent\\nabilities (inset). (D) If the researcher instead scores models’ outputs using a discontinuous metric\\nsuch as Multiple Choice Grade (akin to a step function), the metric choice discontinuously scales\\nperformance, again causing performance to change sharply and unpredictably. (E) Changing from a\\nnonlinear metric to a linear metric such as Token Edit Distance, scaling shows smooth, continuous\\nand predictable improvements, ablating the emergent ability. (F) Changing from a discontinuous\\nmetric to a continuous metric such as Brier Score again reveals smooth, continuous and predictable\\nimprovements in task performance. Consequently, emergent abilities are created by the researcher’s\\nchoice of metrics, not fundamental changes in model family behavior on speciﬁc tasks with scale.\\n2. We meta-analyze published benchmarks [28, 33] to reveal that emergent abilities only ap-\\npear for speciﬁc metrics, not for model families on particular tasks, and that changing the\\nmetric causes the emergence phenomenon to evaporate.\\n3. We induce never-before-seen, seemingly emergent abilities in multiple architectures across\\nvarious vision tasks by intentionally changing the metrics used for evaluation.\\n2 Alternative Explanation for Emergent Abilities\\nHow might smooth, continuous, predictable changes in model family performance appear sharp and\\nunpredictable? The answer is that the researcher’s choice of a nonlinear or discontinuous metric can\\ndistort the model family’s performance to appear sharp and unpredictable.\\nTo expound, suppose that within a model family, the test loss falls smoothly, continuously and\\npredictably with the number of model parameters. One reason to believe this is the phenomenon\\nknown as neural scaling laws: empirical observations that deep networks exhibit power law scaling\\nin the test loss as a function of training dataset size, number of parameters or compute [13, 27, 11,\\n16, 9, 12, 15, 34, 14, 7, 26]. For concreteness, suppose we have a model family of different numbers\\nof parameters N > 0and assume that each model’s per-token cross entropy falls as a power law\\nwith the number of parameters Nfor constants c>0,α< 0(Fig. 2A):\\nLCE(N) =(N\\nc)α\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='60e5d3f3-b774-499f-ac5d-73f165e4fb9b', embedding=None, metadata={'page_label': '4', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='7454a1f7746355b3416d24820439c6ae72e51c6465e552436e63a1c61b537f93', text='To be clear, we do not require this particular functional form to hold; rather, we use it for illustra-\\ntive purposes. Let Vdenote the set of possible tokens, p∈∆|V|−1denote the true but unknown\\nprobability distribution, and ˆpN∈∆|V|−1denote theN-parameter model’s predicted probability\\ndistribution. The per-token cross entropy as a function of number of parameters Nis:\\nLCE(N)def=−∑\\nv∈Vp(v) log ˆpN(v)\\nIn practice,pis unknown, so we substitute a one-hot distribution of the observed token v∗:\\nLCE(N) =−log ˆpN(v∗)\\nA model with Nparameters then has a per-token probability of selecting the correct token (Fig. 2B):\\np(single token correct ) = exp(\\n−LCE(N))\\n= exp(\\n−(N/c)α)\\nSuppose the researcher then chooses a metric that requires selecting Ltokens correctly. For example,\\nour task might be L-digit integer addition, and a model’s output is scored 1if allLoutput digits\\nexactly match all target digits with no additions, deletions or substitutions, 0otherwise. If the\\nprobability each token is correct is independent1, the probability of scoring 1is:\\nAccuracy (N)≈pN(single token correct )num. of tokens= exp(\\n−(N/c)α)L\\nThis choice of metric nonlinearly scales performance with increasing token sequence length. When\\nplotting performance on a linear-log plot, one sees a sharp, unpredictable emergent ability on longer\\nsequences (Fig. 2C) that closely matches claimed emergent abilities (inset). What happens if the\\nresearcher switches from a nonlinear metric like Accuracy, under which the per-token error rate\\nscales geometrically in target length (App. A.3), to an approximately linear metric like Token Edit\\nDistance, under which the per-token error rate scales quasi-linearly in target length (App. A.2)?\\nToken Edit Distance (N)≈L(\\n1−pN(single token correct ))\\n=L(\\n1−exp(\\n−(N/c)α))\\nThe linear metric reveals smooth, continuous, predictable changes in model performance (Fig. 2E).\\nSimilarly, if the researcher uses a discontinuous metric like Multiple Choice Grade, the researcher\\ncan ﬁnd emergent abilities (Fig. 2D), but switching to a continuous metric like Brier Score removes\\nthe emergent ability (Fig. 2F). In summary, sharp and unpredictable changes with increasing scale\\ncan be fully explained by three interpretable factors: (1) the researcher choosing a metric that nonlin-\\nearly or discontinuously scales the per-token error rate, (2) having insufﬁcient resolution to estimate\\nmodel performance in the smaller parameter regime, with resolution2set by 1/test dataset size, and\\n(3) insufﬁciently sampling the larger parameter regime.\\n3 Analyzing InstructGPT/GPT-3’s Emergent Arithmetic Abilities\\nPrevious papers prominently claimed the GPT [3, 24] family3displays emergent abilities at integer\\narithmetic tasks [8, 28, 33] (Fig. 2E). We chose these tasks as they were prominently presented\\n[3, 8, 28, 33], and we focused on the GPT family due to it being publicly queryable. As explained\\nmathematically and visually in Sec. 2, our alternative explanation makes three predictions:\\n1. Changing the metric from a nonlinear or discontinuous metric (Fig. 2CD) to a linear or\\ncontinuous metric (Fig. 2 EF) should reveal smooth, continuous, predictable performance\\nimprovement with model scale.\\n1While the independence assumption is not true, the approximation yields results qualitatively matching the\\nobserved emergence claims.\\n2Resolution is deﬁned as “The smallest interval measurable by a scientiﬁc instrument; the resolving power.”\\n3As of 2023-03-15, 4 models with 350M, 1.3B, 6.7B, 175B parameters are available via the OpenAI API.\\n4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ad81faaa-9c10-4078-a9c5-563d2cd1c0b9', embedding=None, metadata={'page_label': '5', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='bb2783b4a3dcc00f798ac88ff7d51d624b23f3627f08aa2aae9c9dd9f4669b54', text='10910101011\\nModel Parameters0.00.20.40.60.81.0Accuracy\\nT arget Str Len\\n1\\n2\\n3\\n4\\n5\\n10910101011\\nGPT-3 Model Parameters0.00.20.40.60.81.0AccuracyT arget Str Len\\n1\\n2\\n3\\n4\\n5\\nT emp\\n0.0\\n1.0\\n10910101011\\nGPT-3 Model Parameters0.00.20.40.60.81.0AccuracyT arget Str Len\\n1\\n2\\n3\\n4\\n5\\nT emp\\n0.0\\n1.0\\n10910101011\\nModel Parameters4\\n3\\n2\\n1\\n0- T oken Edit Distance\\nT arget Str Len\\n1\\n2\\n3\\n4\\n5\\n10910101011\\nGPT-3 Model Parameters6\\n5\\n4\\n3\\n2\\n1\\n0- T oken Edit DistanceT arget Str Len\\n1\\n2\\n3\\n4\\nT emp\\n0.0\\n1.0\\n10910101011\\nGPT-3 Model Parameters6\\n5\\n4\\n3\\n2\\n1\\n0- T oken Edit DistanceT arget Str Len\\n1\\n2\\n3\\n4\\n5\\nT emp\\n0.0\\n1.0Figure 3: Claimed emergent abilities evaporate upon changing the metric. Left to Right: Math-\\nematical Model, 2-Integer 2-Digit Multiplication Task, 2-Integer 4-Digit Addition Task. Top: When\\nperformance is measured by a nonlinear metric (e.g., Accuracy), the InstructGPT/GPT-3 [3, 24]\\nfamily’s performance appears sharp and unpredictable on longer target lengths. Bottom: When per-\\nformance is instead measured by a linear metric (e.g., Token Edit Distance), the family exhibits\\nsmooth, predictable performance improvements.\\n10910101011\\nModel Parameters106\\n105\\n104\\n103\\n102\\n101\\n100Accuracy\\nT arget Str Len\\n1\\n2\\n3\\n4\\n5\\nFigure 4: Claimed emergent abilities evaporate upon using better statistics. Left to Right: Math-\\nematical Model, 2-Integer 2-Digit Multiplication Task, 2-Integer 4-Digit Addition Task. Based on\\nthe predictable effect Accuracy has on performance, measuring performance requires high resolu-\\ntion. Generating additional test data increases the resolution and reveals that even on Accuracy,\\nthe InstructGPT/GPT-3 family’s [3, 24] performance is above chance and improves in a smooth,\\ncontinuous, predictable manner that qualitatively matches the mathematical model.\\n2. For nonlinear metrics, increasing the resolution of measured model performance by in-\\ncreasing the test dataset size should reveal smooth, continuous, predictable model improve-\\nments commensurate with the predictable nonlinear effect of the chosen metric .\\n3. Regardless of metric, increasing the target string length should predictably affect the\\nmodel’s performance as a function of the length-1 target performance: approximately geo-\\nmetrically for accuracy and approximately quasilinearly for token edit distance.\\nTo test these predictions, we collected outputs from the InstructGPT/GPT-3 family on two tasks:\\n2-shot multiplication between two 2-digit integers and 2-shot addition between two 4-digit integers.\\nPrediction: Emergent Abilities Disappear With Different Metrics On both arithmetic tasks, the\\nGPT family displays emergent abilities if the target has 4 or 5 digits and if the metric is Accuracy\\n(Fig. 3, top) [3, 8, 33]. However, if one changes from nonlinear Accuracy to linear Token Edit\\nDistance while keeping the models’ outputs ﬁxed , the family’s performance smoothly, continuously\\n5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LlamaCPP(\n",
    "    model_path=MODEL_PATH,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    context_window=3000,\n",
    "    generate_kwargs={},\n",
    "    model_kwargs={\"n_gpu_layers\": 30},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = SentenceSplitter(chunk_size=256)\n",
    "_ = st.split_text(documents[0].text)\n",
    "len(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \"Explaining Emergent Abilities in Large Language Models: Metric Choice, Scale, and Resolution\"'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "### [INST] Context: {context_str}. Give a highly concise title that summarizes all of \\\n",
    "the unique themes found in the context, in no more than 20 words. \\\n",
    "Dont include descriptions of what you are doing or how you did it. Be as concise as possible. \\\n",
    "\n",
    "Title: [/INST]\"\"\"\n",
    "\n",
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "llm.predict(PromptTemplate(template=prompt), context_str=documents[-2].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llm_predictor.base import LLMPredictorType\n",
    "from llama_index.bridge.pydantic import Field\n",
    "\n",
    "\n",
    "class CustomLLMExtractor(BaseExtractor):\n",
    "    llm: LLMPredictorType = Field(description=\"The LLM to use for generation.\")\n",
    "    prompt: PromptTemplate = Field(\n",
    "        default=\"\"\"[INST] [/INST]\"\"\",\n",
    "        description=\"The prompt to extract titles with.\",\n",
    "    )\n",
    "\n",
    "    def __init__(self, llm=None, prompt=None):\n",
    "        super().__init__(llm=llm, prompt=PromptTemplate(template=prompt))\n",
    "\n",
    "    # self.llm = kwargs.get(\"llm\")\n",
    "    # self.prompt = kwargs.get(\"prompt\")  # PromptTemplate(template=prompt)\n",
    "\n",
    "    async def aextract(self, nodes):\n",
    "        metadata_list = [\n",
    "            {\"custom_title\": self.llm.predict(self.prompt, context_str=node.text)}\n",
    "            for node in nodes\n",
    "        ]\n",
    "        return metadata_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1809982/1913198160.py:13: RuntimeWarning: coroutine 'LLM.apredict' was never awaited\n",
      "  nodes = pipeline.run(documents=documents)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback <TaskWakeupMethWrapper object at 0x7f9f282c51f0>(<Future finis...f60>, ...],))>)\n",
      "handle: <Handle <TaskWakeupMethWrapper object at 0x7f9f282c51f0>(<Future finis...f60>, ...],))>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/starscream/anaconda3/lib/python3.8/asyncio/events.py\", line 81, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "RuntimeError: Leaving task <Task pending name='Task-2' coro=<Kernel.dispatch_queue() running at /media/starscream/wheeljack1/projects/casper/src/language/.venv/lib/python3.8/site-packages/ipykernel/kernelbase.py:529> wait_for=<Future pending cb=[<TaskWakeupMethWrapper object at 0x7f9f37104310>()]> cb=[_wrap_awaitable.<locals>.<lambda>() at /media/starscream/wheeljack1/projects/casper/src/language/.venv/lib/python3.8/site-packages/tornado/gen.py:852, IOLoop.add_future.<locals>.<lambda>() at /media/starscream/wheeljack1/projects/casper/src/language/.venv/lib/python3.8/site-packages/tornado/ioloop.py:699]> does not match the current task None.\n"
     ]
    }
   ],
   "source": [
    "extractors = [\n",
    "    # TitleExtractor(nodes=1, llm=llm, node_template=prompt),\n",
    "    # QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
    "    # EntityExtractor(prediction_threshold=0.5),\n",
    "    # # SummaryExtractor(summaries=[\"prev\", \"self\"], llm=llm),\n",
    "    # KeywordExtractor(keywords=10, llm=llm),\n",
    "    CustomLLMExtractor(llm=llm, prompt=prompt),\n",
    "]\n",
    "\n",
    "transformations = [st] + extractors\n",
    "\n",
    "pipeline = IngestionPipeline(transformations=transformations)\n",
    "nodes = pipeline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'page_label': '1', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Emergent Abilities in Large Language Models: A Metric Illusion\"'}\n",
      "1\n",
      "{'page_label': '1', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Explaining Emergent Abilities in Large Language Models: Metric Choice vs. Model Scaling\"'}\n",
      "2\n",
      "{'page_label': '1', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Investigating the Illusion of Emergent Abilities in AI: Impact of Metric Choice on Performance\"'}\n",
      "3\n",
      "{'page_label': '1', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Emergent Abilities in AI: Debunking Myths through Metric Choices in Vision Tasks\"'}\n",
      "4\n",
      "{'page_label': '1', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Emergence of Complex Properties in Large Language Models: A Challenge to Fundamental AI Capabilities\"'}\n",
      "5\n",
      "{'page_label': '1', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Emergence of Unique Abilities in Large-Scale Language Models\"'}\n",
      "6\n",
      "{'page_label': '1', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Emergence of Unique Abilities in Large-Scale Language Models\"'}\n",
      "7\n",
      "{'page_label': '1', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Emergence of Unpredictable Abilities in Large-Scale Language Models\"'}\n",
      "8\n",
      "{'page_label': '1', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Emergent Abilities in Large-Scale Language Models: Sudden and Unpredictable Appearance\"'}\n",
      "9\n",
      "{'page_label': '2', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Emergent Abilities in Large Language Models: Unpredictable Increases and AI Safety Implications\"'}\n",
      "10\n",
      "{'page_label': '2', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Emergent Abilities in Large Language Models: Unpredictability and Control\"'}\n",
      "11\n",
      "{'page_label': '2', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Emergent Abilities in Large Language Models: Unpredictability and Control\"'}\n",
      "12\n",
      "{'page_label': '2', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Challenging the Concept of Emergent Abilities in LLMs: A Critique on Metric-Induced Changes\"'}\n",
      "13\n",
      "{'page_label': '2', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Emergent Abilities in LLMs: A Possible Illusion Induced by Nonlinear Metric Choices and Insufficient Test Data\"'}\n",
      "14\n",
      "{'page_label': '2', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Alternative Explanation for Emergent Abilities in LLMs: A Mathematical Model and Predictions\"'}\n",
      "15\n",
      "{'page_label': '3', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Multiple Choice Grades and Model Parameters in Three Sets for Accuracy and Emergent Ability Evaluation'}\n",
      "16\n",
      "{'page_label': '3', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Multiple-Choice Model Parameters and Accuracy Rates for Emergent Abilities'}\n",
      "17\n",
      "{'page_label': '3', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Multiple-Choice Model Performance: Accuracy, Emergent Abilities, and Scoring Methods'}\n",
      "18\n",
      "{'page_label': '3', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Model Performance Metrics: Accuracy, Probability Scores, Brier Score (Emergent Abilities, Token Count, Smoothness)'}\n",
      "19\n",
      "{'page_label': '3', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Emergent Abilities in Large Language Models: Metric-Driven Development and Score Comparison\"'}\n",
      "20\n",
      "{'page_label': '3', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Model Performance and Emergent Abilities: Brier Score, Choices, and Scaling of Large Language Models\"'}\n",
      "21\n",
      "{'page_label': '3', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Model Scaling and Emergent Abilities: Per-Token Loss and Brier Score in Large Language Models\"'}\n",
      "22\n",
      "{'page_label': '3', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Impact of Metric Choice and Model Scale on Unpredictable Emergent Abilities in Language Models\"'}\n",
      "23\n",
      "{'page_label': '3', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Metric Choices and Model Performance: Nonlinear and Discontinuous Scaling of Emergent Abilities in Large Language Models.'}\n",
      "24\n",
      "{'page_label': '3', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Metric Choices and Model Performance: Nonlinear Scaling of Emergent Abilities in Language Models'}\n",
      "25\n",
      "{'page_label': '3', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': \" Metric Choice's Impact on Model Performance: Nonlinear vs. Linear Scaling and Its Effect on Emergent Abilities.\"}\n",
      "26\n",
      "{'page_label': '3', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Metric Choice and Model Performance: Nonlinear vs. Discontinuous vs. Linear Scaling and Emergent Abilities.'}\n",
      "27\n",
      "{'page_label': '3', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': \" Metric Choice's Impact on Model Performance: Discontinuous vs. Continuous Metrics and Emergent Abilities\"}\n",
      "28\n",
      "{'page_label': '3', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Metric Choice and Emergent Abilities: Revealing Specific Impact on Model Performance.'}\n",
      "29\n",
      "{'page_label': '3', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Metric Choice and Emergent Abilities: Smooth Changes with Unpredictable Consequences in Model Performance.'}\n",
      "30\n",
      "{'page_label': '3', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Emergent Abilities and Metric Choices: Smooth Changes with Discontinuous Impact\"'}\n",
      "31\n",
      "{'page_label': '3', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Emergent Abilities in AI: Metric-Dependent Appearance and Nonlinear Performance Distortion\"'}\n",
      "32\n",
      "{'page_label': '3', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Influence of Metric Choice on Unpredictable Emergent Abilities in Model Families\"'}\n",
      "33\n",
      "{'page_label': '3', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Nonlinear Metrics and Emergent Abilities: Distorting Continuous Performance\"\\nor\\n\"Power Law Scaling in Neural Networks: Impact on Predictability of Model Family Performance\"'}\n",
      "34\n",
      "{'page_label': '3', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Impact of Model Complexity on Test Loss: Neural Scaling Laws and Power-Law Relationships\\n\\n[1. Model family performance distortion, 2. Smooth test loss scaling, 3. Neural scaling laws, 4. Power law relationship between test loss and model size]'}\n",
      "35\n",
      "{'page_label': '4', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Cross-Entropy Loss for N-Parameter Model: Per-Token Probability Error with Unknown True Distribution'}\n",
      "36\n",
      "{'page_label': '4', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Cross-Entropy Loss for N-Parameter Models: Per-Token and Multi-Token Accuracy Relationship'}\n",
      "37\n",
      "{'page_label': '4', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Cross-Entropy Loss for Token Selection: Per-Token and Multi-Token Accuracy with N-Parameter Models'}\n",
      "38\n",
      "{'page_label': '4', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Per-Token Cross-Entropy and Model Accuracy: Relating Number of Parameters to Probability of Correct Tokens\"'}\n",
      "39\n",
      "{'page_label': '4', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Nonlinear Scaling of Performance in Token Sequence Recognition: Sharp Emergence of Longer Sequence Accuracy on Log-Log Plots (Independent Token Probabilities)'}\n",
      "40\n",
      "{'page_label': '4', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': \" Nonlinear Metric's Unpredictable Impact on Emergent Abilities in Long Token Sequences\"}\n",
      "41\n",
      "{'page_label': '4', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Nonlinear vs. Linear Metrics for Long Sequence Learning: Emergent Abilities and Scalability\"'}\n",
      "42\n",
      "{'page_label': '4', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Impact of Metric Choice on Long-Sequence Performance: From Nonlinear Accuracy to Linear Token Edit Distance.'}\n",
      "43\n",
      "{'page_label': '4', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Comparing Model Performance with Nonlinear and Linear Metrics: From Accuracy to Token Edit Distance.'}\n",
      "44\n",
      "{'page_label': '4', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Comparing Nonlinear and Linear Model Performance: Sharp vs. Smooth Changes with Accuracy and Token Edit Distance Metrics'}\n",
      "45\n",
      "{'page_label': '4', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Comparing Model Performance with Different Metrics: Nonlinear vs. Linear and Discontinuous. Emergent Abilities and Scalability in Token Sequences.'}\n",
      "46\n",
      "{'page_label': '4', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Impact of Metric Choice on Model Performance: Smooth vs. Emergent Abilities in Linear-Log Plots\"'}\n",
      "47\n",
      "{'page_label': '4', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Impact of Metric Choice on Model Performance: Smooth vs. Discontinuous Changes and Emergent Abilities'}\n",
      "48\n",
      "{'page_label': '4', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Factors Influencing Model Performance: Metric Choice, Resolution, and Sampling in Longer Texts\"'}\n",
      "49\n",
      "{'page_label': '4', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Understanding Emergent Abilities in GPT-3: Metric Sensitivity and Arithmetic Capabilities\"'}\n",
      "50\n",
      "{'page_label': '4', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Understanding GPT-3\\'s Arithmetic Abilities: Factors Influencing Sharp Changes in Performance\"'}\n",
      "51\n",
      "{'page_label': '4', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Understanding GPT\\'s Emergent Arithmetic Abilities: Factors Affecting Sharp Changes in Performance\"'}\n",
      "52\n",
      "{'page_label': '4', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Explaining Sharp Changes in GPT Performance: Metric Choice, Insufficient Resolution, and Inadequate Sampling in Integer Arithmetic Tasks\"'}\n",
      "53\n",
      "{'page_label': '4', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Investigating GPT-3\\'s Arithmetic Abilities: Linear Metrics vs. Emergent Capabilities\"'}\n",
      "54\n",
      "{'page_label': '4', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Emergent Arithmetic in GPT-3: Predictable Performance Improvement with Linear Metrics and Large Model Sizes\"'}\n",
      "55\n",
      "{'page_label': '4', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Nonlinear to linear metric shift: Smooth performance improvement and resolution analysis with large-scale models (350M, 1.3B, 6.7B, 175B params)\"'}\n",
      "56\n",
      "{'page_label': '5', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' GPT-3 Model Parameters and Accuracy Levels from Given Inputs'}\n",
      "57\n",
      "{'page_label': '5', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' GPT-3 Model Parameters and Token Edit Distance for Different Lengths'}\n",
      "58\n",
      "{'page_label': '5', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' GTP-3 Model Parameters: Edit Distance and Length Configurations'}\n",
      "59\n",
      "{'page_label': '5', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' GPT-3 Model Parameters: Token Edit Distance and Target String Length'}\n",
      "60\n",
      "{'page_label': '5', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' GPT-3 Model Parameters with Token Edit Distance and Target String Length Variations'}\n",
      "61\n",
      "{'page_label': '5', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Comparison of Model Parameters and Token Edit Distance in Different Contexts'}\n",
      "62\n",
      "{'page_label': '5', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Comparison of Model Performance with Different Metrics: 2-Digit Multiplication and 4-Digit Addition Tasks'}\n",
      "63\n",
      "{'page_label': '5', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Performance of InstructGPT/GPT-3 family on different metrics: Sharp vs. smooth improvement in accuracy and token edit distance for mathematical tasks.'}\n",
      "64\n",
      "{'page_label': '5', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Performance of GPT-3 Model with Different Metrics: Sharp vs. Smooth Improvements and Disappearing Abilities'}\n",
      "65\n",
      "{'page_label': '5', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Performance Metrics and Emergent Abilities: Nonlinear vs. Linear Measures in Math Tasks\"'}\n",
      "66\n",
      "{'page_label': '5', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Effect of Metric Choice on Predictability of Mathematical Model Performance in Multi-Digit Addition and Multiplication Tasks'}\n",
      "67\n",
      "{'page_label': '5', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Nonlinear vs. Linear Performance Metrics: Smooth Improvements in InstructGPT/GPT-3 family\\'s abilities, despite Evaporating Emergent Capacities\"'}\n",
      "68\n",
      "{'page_label': '5', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Predictable Improvements in Model Performance with Linear Metrics\"'}\n",
      "69\n",
      "{'page_label': '5', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Predictable Model Improvements with Increased Test Data Resolution for Linear and Nonlinear Metrics\"'}\n",
      "70\n",
      "{'page_label': '5', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Analyzing Model Performance: Smooth Improvements in Accuracy and Nonlinear Metrics with Increased Dataset Size and Target Length\"'}\n",
      "71\n",
      "{'page_label': '5', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Predictable Model Improvements: Smooth Performance Changes in Accuracy and Nonlinear Metrics with InstructGPT/GPT-3\"'}\n",
      "72\n",
      "{'page_label': '5', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' \"Predictable Model Improvements and Emergent Abilities in GPT-3: Accuracy vs. Nonlinear Metrics\"'}\n",
      "73\n",
      "{'page_label': '5', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': \" Nonlinear Model Performance: Predictable Effects of Metric and Target Length on GPT-3's Arithmetic Tasks: Emergence of Accuracy-based Abilities with Longer Targets.\"}\n",
      "74\n",
      "{'page_label': '5', 'file_name': '.papers/2304.15004v2.Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage_.pdf', 'custom_title': ' Nonlinear Metrics and Emergent Abilities: Smooth Model Improvements and Length Effects in GPT-3 for Arithmetic Tasks'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1809982/3769702779.py:1: RuntimeWarning: coroutine 'LLM.apredict' was never awaited\n",
      "  for i, n in enumerate(nodes):\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "for i, n in enumerate(nodes):\n",
    "    print(i)\n",
    "    print(n.metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral_u",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
